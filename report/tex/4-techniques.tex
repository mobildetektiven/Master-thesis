\chapter{Techniques}\label{chap:techniques}

Reduction of the number of features in the data is desirable, firstly it simplifies the model, makin it computationally faster. Secondly, a simpler model is easier to interpret. A good understadning of how and why an algorithm behaves as it does is important! There are two main techniques for reducing the feature size, feature selection, and dimensionality reduction. The former, removes uninformative and redundant features from the feature set. The latter creates new features either as linear or non-linear combinations of the original feature set.

\section{Feature selection}\label{sec:feature_selec}

    \subsection{}

\section{Dimensionality reduction}\label{sec:dim_red}
The dimensions of the new set is then often decied by how much the new features explain of the variance in the old dataset. 
    \subsection{PCA}\label{subsec:PCA}
    
    \subsection{Kernel PCA}\label{subsec:kernelPCA}


\section{Anomaly detction}\label{sec:Anomaly_detection}
    
    \subsection{One class SVM}\label{subsec:OCSVM}
    
    \subsection{Neural Networks}\label{subsec:NN}
    
    \subsection{K-nearest neigbours}\label{subsec:k_neig}
    
    \subsection{K-means clustering}\label{subsec:k_means}
    