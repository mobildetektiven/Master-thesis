%!TEX root = ../Thesis.tex
\chapter{Discussion}\label{cha:discussion}

\section{Analyzing the data and building a case}
    Before the data analysis could start, the data needed to be reconstructed. As mentioned the data was far from ready for analysis as it was received. A python library was created that automatically reads the files received by the energy company, and produces data with a structure that allows analysis. The library is created in a way that allows for easy use of new data in the same format. This shows one of several challenges when working with real-world data from the industry.    
    
    Several issues arose once starting working with the data. A problem which most likely is relevant to most industries today is that having data is far from a guarantee of having good data. At first glance, the 90Gb of available data seemed like more than enough find many interesting cases. However, as the work progressed, it became apparent that finding good cases for testing anomaly detection, was not straightforward. It became clear that very few variables were sampled simultaneously, at a higher rate than hourly. Due to this, much time was spent trying to find a case that did not need a higher sampling rate. Going through the plant logs, no good cases were found. It might be possible that the energy company is sampling data in a scheme similar to the one used in \cite{Selak2014}, but this is very hard to reverse engineer from the data. Even if one chooses to sample data in such a scheme, it would be beneficial to sample more data at a higher and constant frequency, as it could open up for using techniques such as time-series forecasting. 
     
    Going through the logs from the power plants, the Pelton needle case was quickly picked out as interesting. As \cite{Aasnes2017} focused on the guide vanes of Francis turbines, building a case around the Pelton needles would serve as a natural extension, where one of the methods used for the Francis case could be tested on a new case. The needles play a vital part in the power production, and increasing knowledge about their condition and the operational trend would be a great addition to the existing instrumentation. Having four different Pelton turbines from two different plants in the dataset would allow testing how much adaptation would be needed to make the different anomaly detection techniques work on new plants. Having turbines with no reported incidents also made it possible to verify if the techniques were prone to give false positives.  
     
    It would have been interesting to include more than just the needle variables in the analysis, but this was not possible due to the data. The methods used in the analysis can handle more variables. Hence the analysis can be extended if the data is available. The possibility of using interpolation to handle the missing data was also addressed, but it was quickly discarded as it would require a lot of plant and process insight to pick techniques that interpolated every  process variable in a reasonable way.     
     
     As there only was one reported incident with the needles during operation, it was decided to create an artificial error that replicates the needle pattern seen for the days building up to the incident. Having the start date of the artificial error would give an insight into how quickly the anomaly detection techniques can catch such a trend. It could also give an indication of when one could expect that the original error started.  
    
    
  

\section{Choosing the methods}
    Several different anomaly detection methods and techniques were considered. As the Pelton case only had one reported incident, it made making a labeled data set hard. This opted for using methods that did not need labeled data, as one does not have to collect data from all the known failure modes, only from normal operation. As mentioned in the introduction, this opens the possibility also to detect unknown failure modes. This means that methods for labeled and unlabeled data does not exclude one another, and for a complete condition monitoring system using both approaches might yield the best solution. Three methods were chosen for the anomaly analysis, that all have been used for anomaly detection. OC SVM was chosen because it showed promise in the case with the Francis turbines guide vanes. KDE was picked because it is a relatively simple method, which is fast to implement through scikit-learn. The LSTM RNN algorithm was picked because it has shown promise when working with time series, and would serve well for comparison for the less complex other methods. Where OC SVM is a classifier, KDE and LSTM RNN are regressors. Hence the two latter does not classify the data as normal or abnormal. They produce a score that mirrors how anomalous the data is evaluated to be.
    
\section{Hyperparameterization}
    The optimal set of hyperparameters were searched for through a grid search for the three methods. The parameters were found using data from plant 1, and the same parameters were used for all the different training sets. This was done to see how well the methods would transfer between plants. If one needs to search for the optimal hyperparameters for each plant, then it will not be possible to deploy a pre-trained anomaly detection system simply. KDE and LSTM RNN was found to adapt well to data from new plants. A more extensive search for the optimal set of hyperparameters could have been performed, especially for OC SVM and LSTM RNN, but as the goal of this thesis was to investigate possible anomaly detection methods, the parameterizations found yielded good enough results to give an indication on what the methods can be used for.
    
    The custom cost function designed for the one class SVM seemed to work as intended, but can most likely be improved. As one gets more knowledge about how the data is structured, using different kernels, then the Gaussian kernel can also help with the performance of the algorithm.

\section{Training sets and test cases}
    Many different approaches could have been taken with the training sets and test cases. The main motivation for the chosen setup was that it enabled to compare performance across plants and training sizes. Many more cases could have been created, but as mentioned earlier this is an initial study, that can be used as starting ground for further testing. It was clear that the sampling rate was very different between the plants, as 14 days of sampling on plant 2 gave as many data points as almost five months at plant 1. This shows the importance of investigating the data. It was also important to ensure that the 14 days from plant 2 covered all operational modes for the plant. The final setup was three different training sets and four different test cases. The three training sets made it possible to evaluate how well the methods adapted to data from different plants and of different sizes. As it was chosen to use the optimal hyperparameterization found for data from plant 1, it also gave insight into how general the parameters were. The artificial test case made it possible to evaluate the performance of the production data seen in test case 1. The two start failure cases made it possible to evaluate the performance of more than one reported incident. 


\section{Performance of the methods}
    All methods showed promise in detecting the reported incident from plant 1 found in test case 1. OC SVM performed best when trained on training set 1. This indicates that the method is more sensitive to the chosen hyperparameters and that the optimal hyperparameters change when the training set changes. It also became clear that the OC SVM did not catch the pattern building up to the reported incident in test case 1, and that it seems like a method more suited for shutdown alerts than for early warning. As the method is trained without labeled data, it is hard to find the optimal selection of hyperparameters. The custom cost function created tried to deal with this issue, but as can be seen from the performance, there could be a set of hyperparameters that would increase the performance of the OC SVM. When the method was trained on data from plant 2, it showed signs of giving false positives. This further strengthens the claim that the optimal hyperparameters change when the training set is changed. 
    
    KDE and LSTM RNN performed very similarly for all training sets on all test cases. This indicates that the two methods are more robust to changes in the training set. It is, however, important to notice that the LSTM RNN needed early stopping to yield a similar performance from training set 3. Complex NNs always run the risk of overfitting to the data. Both methods performed best when trained on training set 2. As mentioned in the analysis this can be because training set 2 is taken from plant 2 after years of operation, and it might then better represent normal needle deviation, not seen in the training set from plant 1 sampled right after maintenance. In addition set 2 is approximately of the same size as set 1 which the hyperparameterization is performed on, where as set 3 is 10 times larger. This shows that even if KDE and LSTM RNN seem more robust to changing the training sets without changing the hyperparameterization, this still is a concern one needs to keep in mind. Both methods clearly evaluated the data from the reported incident in test case 1 as anomalous, and one could identify a growing trend in the anomaly score towards the incident.   
    
    The artificial error in test case 2 showed that both KDE and LSTM RNN could detect an error as the one seen in plant 1 in its early stages. For training set 2, the LSTM RNN showed an increase in the anomaly score immediately after the artificial error was added to the data. The KDE also found this pattern but did not detect it as fast. This means that it is likely that the pattern seen for the anomaly scores for test case 1 towards the reported incident, can be used as an early warning system for system degradation. One can also claim that it can be detected by a pre-trained method, as long as the same transformation is used for both training and test data.  
    
    % One class SVM started to indicate anomalies one month before the peak when trained on training set 1. For training set 2 it only detected the sample that was evaluated as most anomalous by the two other methods. For training set 3 it also started gave false positives. This further strengthens the claim that the method is very sensitive to changes in training data and that the optimal hyperparameters for the method are strongly connected to the structure of the training data. KDE and LSTM RNN showed that pre-trained methods can be deployed to a new plant, and still perform on a high level. They also showed that even if they seem robust, the hyperparameters are connected to the training data. 
    
    Training case 3 and 4 showed that KDE and LSTM RNN clearly are able to separate anomalies from normal data. The two start failures only generated a few anomalous data samples, but the methods gave those samples a much higher anomaly score than the normal data surrounding the incidents. OC SVM was only able to detect one of the start failures. This was also given the highest score by the scorers. The two start failure cases show that the methods evaluate the data correctly, and strengthen the hypothesis that the pattern seen leading up to the incident in test case 1, is due to system degradation. 
    
    The LSTM RNN is the most complex method, and several different network structures could have been included. If data had been sampled at an even frequency, it could have benefited more from its capability to remember previous calculations. Dropout could have been included to reduce the risk of overfitting, and this could have given better performance for the more complex network structures. However, the structure used in this analysis shows that the method is capable of detecting minor changes in the normal operation pattern and that it can be used for early warning about system degradation.  
    
    It is important to remark that the same transformation was used for all data sets, regardless of plant origin. This is crucial to ensure that the data is transformed in the same way, if not one could not have trusted the results of the methods on data from different plants.  It could have been beneficial to have more classes in the SVM. This could either be solved by getting labeled data and train the regular SVM. Another possible approach is to train several different OC SVM classifiers, with increasing complex decision boundary created by the separating hyperplane. Then the one can compare how the different classifiers evaluate the data and make it easier to spot trends earlier.

    Overall KDE seems like the best choice for anomaly detection as presented in this thesis. As mentioned, LSTM RNN did show slightly better performance for some of the cases, but KDE is a more straightforward method with much fewer hyperparameters. This makes it faster to optimize, and from the results, it appears to be very robust to changes in both data size and source. The results clearly show that deploying a KDE anomaly scorer at a plant with a Pelton turbine will increase the knowledge about the condition of its needles. Merely providing a graph with the anomaly score history will give operators a good indication of the condition of the needles. The primary challenge is to define a threshold for what is an anomaly and what is normal. In the results, a line evaluating 0.1\% of the data as anomalous was used. To apply this at a plant one would either need to determine a reasonable threshold through testing in cooperation with system experts or present the score to the operator, making them responsible for the evaluation. How fast the anomaly score changes should also be evaluated, as this can be an indication of how long the plant can normally be operated. It has been shown that it is possible to detect system anomalies without having to create data from failure modes artificially.    

    % how well do they perform?
    % difference between the methods?

    
    % For the anomaly scorers, the extreme values seen for the day of the incident could mean that instead of having one threshold one could have stages of anomalous operation. 
    
    
% \begin{itemize}
%     \item Finding techniques and methods for condition monitoring and anomaly detection. Finding methods for feature extraction and dimensionality reduction to be used in addition with the anomaly detection techniques.
%     \item Data preparation and case extraction. The provided data is not ready for analysis, and one needs to extract datasets for each of the plants. Once the data is in a format that can be analyzed, the historical data from the plants and the datasets need to be analyzed to build a  case for the further analysis. 
%     \item Analyzing the case using the techniques found. Emphasize on how the different techniques perform, and what can/needs to be done to improve the performance. 
% \end{itemize}

    
    
%     There is always a risk that the correlations and variable dependencies found can be caused by randomness. This needs to be taken into consideration when working such cases
    
    
%     The main motivation is to see when an errorthat is increasing over time is detectable By doing so, one knowsexactly when the error starts occurring, this can then be used to evaluate how earlydifferent anomaly detection techniques detect anomalies.
    
%     Training and evaluating the methods on data from two plants enables one toevaluate cross plant performance