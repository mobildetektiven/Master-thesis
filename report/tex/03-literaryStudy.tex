\chapter{Theory}\label{cha:litterature}

This chapter explains potential anomaly detection techniques and methods for feature extraction and dimensionality reduction to be used in addition with the anomaly detection techniques. 

% \section{Novelty vs anomaly(outlier) detection}
%     \cite{Pimentel2014} explains that an outlier or anomaly is a data point that is inconsistent with the rest of the dataset, but it is also used as a term to describe normal data that is far from the normal data point. This means that an outlier is not necesarily an unwanted data point, just an extreme. Outliers have a big impact when construction models, and if a data point is an outlier it is either very important to include or critical to exclude.
    
%     Clarify the two expressions, what is the difference between them? what is not? 
%      "Two interchangeable synonyms of novelty detection
%         [21,1] often used in the literature are anomaly detection and outlier detection [22]. The different terms originate from different domains of application to which one-class classi- fication can be applied, and there is no universally accepted definition.Merriam-Webster [23] defines “novelty” tomean “new and not resembling something formerly known or used”. Anomalies and outliers are two terms used most commonly in the context of anomaly detection; sometimes interchangeably [24].Barnett andLewis [25] define an outlier as a data point that “appears to be inconsistent with the remainder of that set of [training] data”. However, it is also used to describe a small fraction of “normal” data which lie far way from the majority of “normal” data in the feature space [9]. Therefore, outlier detection aims to handle these “rogue” observations in a set of data, which can have a large effect on the analysis of the data. In other words, outliers are assumed to contaminate the dataset under consideration and the goal is to cope with their presence during the model-construction stage." quote from \cite{Pimentel2014}
\section{Anomaly detection}\label{sec:novelty_detection}
    \cite{Pimentel2014} defines anomaly detection as the task of classifying test data that in some way differs from data used for training. This is like a one sided test or one class classification. This means that one is not training on data that represents fault, only on data sampled from normal operation. As a model describing the normal operation of the system is learnt, it is important that one have samples of normal system behaviour for all the possible states of operation. Anomaly detection avoid the issue with finding data that represents all possible failure modes. Such techniques are very valuable for industry applications, where the need for failure detection is great, but identifying all possible failure modes and collecting data to represent them can be difficult and very costly. It is much easier to get measurements from a machine during normal operation, than during failure, and it is in many cases close to impossible to obtain as many samples of negative or faulty behaviour as of normal behaviour. \cite{Tarassenko2009} claims that modern high-integrity systems are so complex that they introduce many possible failure modes that are not very well captured by the instrumentation available. This can be verified by visiting a hydro electric power plant. There is a lot of instrumentation, and many alarms for different components, but all of these are linked to well known failure modes. Anomaly detection introduces the possibility to detect unknown abnormalities. It is however important to notice that unsupervised methods are prone to suffer from a higher rate of false positives than what one can expect from supervised methods \cite{Latecki}. The performance of the anomaly detection is dependent on the choice of parameters, and it is important to keep in mind whether it is false negatives or false positives that are of the highest concern 
    
    Since anomaly detection techniques are based on "normal" system behaviour, they should in theory be able to detect any failure mode that can occur, if provided with the correct process information. Adding process signals that is not known to be connected to any of the known failure modes can be beneficial for such methods. This also makes sense when  you don't know what you are looking for, you want all possible information available. This leads to an important remark, only getting a notification that an anomaly is observed, does not provide very much information. For these techniques to have any real value, one need to be able to trace which component the anomaly comes from, and the magnitude of the anomaly. Once the anomaly detection techniques are trained, new data are given a novelty score based on how well they compare to the normal description of the system. This score can then be compared to a threshold, and the data will be marked as normal or abnormal.

    Many of the techniques in the following sections are based on the suggestions from \cite{Pimentel2014} which provides an extensive review of novelty detection. 

\section{Reducing the input space}\label{sec:reduce_features}
    As the size of the input grows, the complexity and run time of the techniques grows with it. It can become necessary to reduce the input space of the anomaly detection techniques to ensure reasonable run time. Interpreting and visualizing data of high dimensions is difficult. The feature size also introduces issues regarding memory and algorithm runtime, \cite{Guyon2003} and \cite{Dy2004}. Reducing the complexity of the problem is heavily correlated with reducing the number of features. Feature selection techniques can also reveal unknown plant dynamics which can help to understand why some components fail.
    
    "The problem is that not all features are important. Some of the features may be redundant, some may be irrelevant, and some can even misguide clustering results" from \cite{Dy2004} sums up one of the issues with datasets with many features. A concrete example of this is shown in figure \ref{fig:feature_selection}, her one can clearly see that $X_2$ does not provide any information on how the datapoints are clustered. Including this feature in a learning algorithm does not provide any information about the two classes of data found in the dataset, hence in best case the performance will be the same as if only $X_1$ was used. This is supported by \cite{Liu2010} that states that some features can be removed without lowering the performance of a learning algorithm.
    
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{report/figures/techniques/feature_selection.pdf}
        \caption{Example of a two-dimensional dataset, with one relevant and one irrelevant feature}
        \label{fig:feature_selection}
    \end{figure}
    
    There are two main techniques for reducing the input space, feature selection and dimensionality reduction. The former, tries to remove the least informative and the redundant features from the feature set. The latter creates new features either as linear or non-linear combinations of the original feature set, hence one can remove the dimensions which brings little data variance to the table. Feature selection is again separated into supervised and non-supervised selection. In the supervised case, one have a set of features and a target variable, hence one want to keep the features that is related to the target variable. For unsupervised feature selection, there is no target variable to help remove uninformative or redundant features, there is no straight forward way to confirm that the best subset of features are selected.
    
    
    \subsection{Filter, wrapper and embedded methods}\label{subsec:filter_wrapper_embedded}
        The different classes or types of methods for feature reduction are split into three groups, filter, wrapper and embedded methods. This section will give a short introduction to each of them.
        According to \cite{Liu2010} filter methods are methods that perform feature selection separate from the learning algorithm, and hence can be used no matter which learning algorithm is applied later. The wrapper methods however, need a predetermined learning algorithm. The features are then selected based upon the performance of the chosen algorithm.  Finally the embedded methods incorporate the selection of features in the training of the learning algorithms model.
        \cite{Liu2010} also states that since the filter methods are independent from the learning algorithm they are not biased with regard of the algorithm. Since different feature sets selected by different methods are to be tested on the same learning algorithms, this is seen as beneficial, and hence only filter methods are used in this thesis. 

\section{Supervised feature selection}\label{sec:sup_feat_select}
    Supervised feature selection can be used when you have a target to track. This means that you need to identify a process signal or a function of a process signal of interest which you want to trace in the other process variables. An example is anomaly detection for a bearing. By using process variables from the bearing such as vibration and temperature, one can find other process variable that are in some way related to the targets. This section introduces two supervised feature selection methods. 
    % As seen in chapter \ref{cha:data} the control problems with the needles can be observed in the difference or the RMSE between the pairwise controlled needles. Using the RMSE between the needles as a target variable enables the use of supervised feature selection algorithms. 
    
    
    \subsection{K-best using correlation and  mutual information}\label{subsec:K-best_feat_select}
    
        Mutual information, \cite{Kraskov2004} and \cite{Peng2005} explains how dependent or inversely how independent two variables are. It gives an understanding on how much knowing something about one variable reduces the uncertainty about the other. If two variables are completely independent, their corresponding mutual information or MI will be zero. 
        
        The mutual information is defined as
        \begin{align}\label{eq:tech_MI}
                MI(X,Y) = \int \int p(x,y) \log \frac{p(x,y}{p_x(x),p_y(y)},
        \end{align}
        where $x$ and $y$ are the two variables to compare, and $p(x)$ and $py(y)$ is their corresponding probability density functions. One benefit from MI is that does not only find linear correlation between two variables. In other words, it finds dependencies between variables not necessarily shown in correlation, meaning that this serves as complementary technique to using co-variance or correlation for feature selection, \cite{Li}. The K features with highest MI will be chosen. 
        
        
        The K-best features can also be selected using the above mentioned correlation. The correlation between a feature and the target variable is found as  
        \begin{align}
            corr(X,Y) = \frac{cov(X,Y}{\sigma_x\sigma_y} = \frac{E[(X-\mu_x)(Y-\mu_y)]}{\sigma_x\sigma_y}.
        \end{align}
        Here the K features with highest correlation with the target variable will be chosen. 
        
\section{Unsupervised feature selection}\label{sec:unsup_feat_reduc}
    Unsupervised feature selection is harder than supervised feature selection. The lack of a target variable, removes the ability to easily interpret which variable dynamics that are important. Still the need to reduce the feature set is prominent. \cite{Dy2004} describes the problem as follows, "The goal of feature selection for unsupervised learning is to find the smallest feature subset that best uncovers “interesting natural” groupings (clusters) from data according to the chosen criterion". The chosen criterion defines what is thought to be interesting natural groupings. As they go on to explain, there is no one optimal golden rule for defining this criterion, and a subset that is good for one purpose might not be relevant for others. Two possible methods for unsupervised feature selection follows.
    
    
    \subsection{K-best using variance threshold}\label{subsec:var_thres}
        Variance threshold simply removes the features with lowest variance. According to \cite{He2005} it is one of the simplest evaluation methods for unsupervised selection. One of the drawbacks with using variance to select features, is that there might be a lot of features with large variances that is non-informative with what one is looking for. The algorithm is very simple, it returns the k features that has the highest variance. 
        
    
    \subsection{Laplacian score}\label{subsec:lapl_score}
        Laplacian score for feature selection was proposed as an alternative to unsupervised feature selection by \cite{He2005}. The method builds upon the assumption that features belonging to the same class or grouping can be found close together. The features are evaluated based on how well locality is preserved, which is found by the Laplacian score. The Laplacian score is calculated for each feature. A short introduction to the method is described below, more details can be found in \cite{He2005}. 
        
        Laplacian score is based on Laplacian Eigenmaps \cite{laplcian score for feature selection}, which is an algorithm for dimensionality reduction. The algorithm creates a nearest neighbors graph with a node for each sample of each feature. Hence the dimension becomes $m$ times $\#$features. Two nodes share an edge if they are among the k-nearest neighbors to each other, meaning that k is one of they tunable parameters for this algorithm. Each edge is then given a weight based on the Gaussian radial basis function;
        
        \begin{align}\label{eq:tech_LS}
            W_{i,j} = e^-{\frac{||{\bm{X_i}-\bm{X_j}}^2||}{t}},
        \end{align}
        
        where t is another tunable parameter. Then a graph Laplacian is computed which then again is used to calculate a Laplacian score. As mentioned the algorithm has two tunable parameters. The main reson for choosing Laplacian score as one of the unsupervised feature selection methods, is that it has the ability to compare and rate features against each other. This introduces a new dimension compared to the more naive variance threshold algorithm, that only looks at one feature at a time. 
    

\section{Dimensionality reduction}\label{sec:dim_red}
    One of the issues with feature reduction is that the features that are removed, may hold information that could help the learning algorithm. In dimensional reduction, the feature set is used to create new features which means that a new reduced feature space of dimension n can hold the same amount of information as the original feature space of size m, $m>n$.

    \subsection{PCA}\label{subsec:PCA}
        Principal component analysis, is one popular technique used for dimensional reduction. PCA is an orthogonal transformation that takes a set of possibly correlated variables, and transforms them into a set of non correlated components, effectivly reducing the dimensions of the data. These new dimensions are known as principal components. There are several different algorithms that can be used to calculate the principal components, one of them is singular value decomposition or SVD. SVD decomposed the data into eigenvalues and eigenvectors. The larger a eigenvalue is, the better its corresponding eigenvector capture the variance of the data. All eigenvectors are orthogonal, this means that each vector introduces an entirely new dimension. In a high dimensional dataset with many features, there will most likely be features that are fairly similar. Figure \ref{fig:tech:PCA} shows intuitively how the PCA algorithm works. Data samples from a dataset with three features are shown in the plot. As the principal components in red and green shows, most of the variance in the samples can be represented using only one dimension. Adding the second dimension, only small residuals might be left. This illustrates how the PCA algorighm operates. 
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{report/figures/techniques/PCA.pdf}
            \caption{Illustration on how principal component analysis work}
            \label{fig:tech:PCA}
        \end{figure}
        
        

    
    \subsection{Kernel PCA}\label{subsec:kernelPCA}
        Kernel PCA builds upon the PCA algorithm explained above, but it introduces a new trick. Where PCA is only able to create new features from linear combinations of the original feature set, kernel PCA uses the kernel trick to enable non-linear combinations as well. Say you have a two dimensional dataset containing samples from two different classes.  These classes are only linearly separable if the classes can be separated by a line. If one class surrounds the other class, there is no way to linearly separte the two clases in two dimensions. However, by extending the sample space into three dimensions, it might now be possible to separte the two classes with a hyperplane. Hence making a non-linear separation of the two classes possible, by still using linear methods. If the datasets are large, transforming all samples into a higher dimension can become computationally heavy. The kernel trick solves this problem without having to transform the samples. The kernel PCA extends the original PCA to a higher dimension using the kernel trick, enabling the extraction of nonlinear components.  
    
        By mapping the original data nonlinearly into a new feature space $F$ $\phi(\bm{x_1})$, and by performing PCA in this new feature space, one can get nonlinear principal components.


\section{Anomaly detection techniques}
    In this section three different techniques for anomaly detection is presented. SVM is a classifying algorithm that automatically classifies data as either normal or an anomaly. KDE and LSTM outputs an anomaly score, where the user need to classify anomalies either by a threshold or more complex evaluation methods. The evaluation is not a scope of this thesis, and will not be discussed further. 
    
    \subsection{One class SVM}\label{subsec:OCSVM}
    
            The following section is a based on my project thesis written the fall of 2017 and \cite{Hastie}. 
            
            Support vector machine or SVM, can be used for both supervised and unsupervised learning, where the unsupervised implementation is used for anomaly detection. This is also known as one class SVM. The SVM algorithms defines a hyperplane that that best separates two different classes. Hence data is labeled normal or abnormal based on which side of the hyperplane it lays. In anomaly detection only normal data is available, hence one has no negative data to help fitting the hyperplane. SVM is capable of separating data both linearly and non-linearly. A kernel function, or a similarity function, describes how similar two feature vectors are by taking the inner product of the two samples in a higher dimensional space. The kernel function does this without having to explicitly transform the data into this dimension. Two commonly used kernels are the Gaussian kernel
            \begin{align}
                K_g(\bm x^{1},\bm x^{2}) = e^{\frac{\norm{\bm x^{1}-\bm x^{2}}^2}{2\sigma^2}}, 
                \label{svm:gauss}
            \end{align}
            and the polynomial kernel
            \begin{align}
                K_p(\bm x^{1},\bm x^{2}) = (\bm x^{1T}\bm x^{2} + \alpha)^\beta.
                \label{svm:poly}
            \end{align}
           These are just two of many examples. The Gaussian or RBF kernell is a good first choice if you know that you have a nonlinear boundary, but don't know exactly what shape the boundary will take.
    
            \begin{align}
                f(\bm x) = \beta_0 + \bm \beta^T\bm x = 0,
                \label{svm:hyper}
            \end{align}
            defines a hyperplane. For any two feature vectors $\bm x^{[1]},\bm x^{[2]}$ that lie in or on the hyperplane where $\bm \hat x = \bm x^{[1]} - \bm x^{[2]}$, gives  $\bm \beta^T \bm \hat x = 0 $ and hence, $\bm \beta$ is a scalar multiplication of the normal vector to the hyperplane, 
            
            \begin{align}
                \bm \beta^* = \frac{\bm \beta}{\norm{\bm \beta}}.
                \label{svm:norm}
            \end{align}
            When inserting any feature point $\bm x_0$ that lies on or in the hyperplane defined by Equation \ref{svm:hyper}, one get $\bm \beta_0 + \bm \beta^T\bm x_0 = 0$ which yields $\bm \beta^T\bm x_0 = -\bm \beta_0$. 
            
            By using $\bm x_0$ as any feature vector from origo to the hyperplane and the normal vector $\bm \beta^*$, the distance for any given feature vector $\bm x$ to the hyperplane is given by
            \begin{align}
                \bm \beta^{*T}(\bm x - \bm x_0) = & \frac{\bm \beta^T}{\norm{\bm \beta}} (\bm x - \bm x_0) \nonumber \\
                % = & \frac{1}{\norm{\bm \beta}}(\bm \beta^T\bm x- \bm \beta^T\bm x_0) \nonumber \\
                % = & \frac{1}{\norm{\bm \beta}}(\bm \beta^T\bm x + \bm \beta_0) \nonumber \\
                = & \frac{1}{\norm{f'(\bm x)}}f(\bm x).
                \label{svm:dist}
            \end{align}
            
            A decision rule 
            \begin{align}
                y^i(\bm x^{iT} \bm \beta + \beta_0) \geq 1 
                \label{svm:decision}
            \end{align}
            can then be created using the hyperplane. Since there are two classes they are defined as $y_0 = 1$ and $y_1 = -1$. The decision rule tells which class a feature vector belongs to.
            
            Now that an expression for the distance from a hyperplane to any given feature vector in the feature space is defined, one can look at how to find the optimal hyperplane to separate the two classes. The margin or the width between the two closest point from both classes can be defined as $M = \frac{2}{\norm{\bm \beta}}$, and hence maximizing the distance can be formulated as minimizing
            \begin{align}
                J(\bm \beta) = & \frac{1}{2}\norm{\bm \beta}^2 \nonumber \\
                 s.t \quad y^i(\bm \beta^T\bm x^i + \beta_0) \geq & 1 \quad \forall \enspace i.
                \label{svm:cost}
            \end{align}
            The constraint here ensures that every point is classified correctly. Having to classify all samples correctly can lead to a very complex boundary or hyperplane, in addition there is always a risk that data is labled incorrectly. Therefore slack variables are added to enable some wrong classifications as seen in the updated minimization problem
            
            \begin{align}
                J(\bm \beta) = & \frac{1}{2}\norm{\bm \beta}^2  + C \sum_{i=1}^N \xi_i \nonumber \\
                 s.t \quad \xi_i \geq 0, y^i(\bm \beta^T\bm x^i + \beta_0) \geq & 1 - \xi_i \quad \forall \enspace i.
                \label{svm:cost}
            \end{align}
            % The solutution to the original SVM algorithm is found by a Lagrangian optimization problem which maximize distance between two classes as, 
            %  \begin{align}
            %     L = \sum_{i=1}^n  \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y^i y^j \bm x^{i}^T \bm x^j,
            %     \label{svm:dual}
            % \end{align}
            % or by 
            % \begin{align}
            %     L = \sum_{i=1}^n  \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y^i y^j K(\bm x^{i}, \bm x^j)
            % \end{align}
            % if the kernel trick is used. Here $\alpha$ are the parameters that defines the separating hyperplane, and it is constrained by having to correctly classify data to its original class. Both are a convex optimization problem which has the nice attribute of having a global maxima. This means that you will always find the best solution for the given parameters.
            
            In the case of one class SVM \cite{Shcolkopf} there is only one class available, so the problem is formulated slightly different,
            \begin{align}
                min \quad J(\bm \beta) = & \frac{1}{2}\norm{\bm \beta}^2  + \frac{1}{\nu N}\sum_{i=1}^N \xi_i - \rho \nonumber \\
                s.t \quad (\beta \cdot \phi(x_i) \geq & \rho - \xi_i \quad \forall \enspace i \nonumber \\
                \xi_i \geq & 0 \quad \forall \enspace i.
                \label{svm:cost}
            \end{align}
            The main difference is the parameter $\nu$ wich serves as a upper fractional bound for accepted number of outliers, and as a lower bound for the fraction of support vectors used by the algorithm. $\rho$ is simply an offset.   
            % If your classes are not linearly separable in the feature space, maximization of Equation \ref{svm:dual} has no global solution. However as can be seen in Equation \ref{svm:dual} one want to minimize $\bm x_i^T  \bm x_j$. This term can then be replaced by one of the kernel functions defined above, which now enables separation in a higher dimensional space.
            
            % The final thing to remark is that not all datasets are completely separable, to deal with this a slack variable is introduced into the optimization to allow miss-classification of some of the feature vectors. 
        
    
        % One issue with one class svm combined with non labeled data, is that hyperparameterization becomes hard. There is no out of the box scoring function that tell you how well the classifier is performing. As long as one is working in two or three dimensions it is possible to plot the decision boundary, and select parameterization based on visual observations. In higher dimensions this becomes a proble, therefor a new method is proposed used to reduce the number of hyperparameterizations to consider. A score is given each of the paramterization from
        
        % \begin{align}
        %     score = \abs\sigma + \mu + 100\frac{outliers}{inliers},
        % \end{align}
        % where the lower the score, the better the performance. $\sigma$ here represents the standard deviation of the distances from the decision boundary, $\mu$ is the unsigned distance to the decision boundary. These values were picked because the training data is said to be normal operation, and should not contain any outliers. However, this leads to the risk of a to general boundary where the classifier is no longer able to predict outliers. This means that the closer the boundary is to the samples in the training data, the higher the score. The final fraction of outliers and inliers is added to make sure that most of the data is classified as inliers. If not on runs the risk of fitting av very complex boundary which yields good distance measures for all samples, this will however lead to a large fraction of outliers which is not the case for the normal training data. 
        
        
    
    \subsection{Kernel density estimation}\label{subsec:kde}
        Kernel density estimation \cite{Latecki}, creates an estimate to the probability density function or pdf of the data. The data is then evaluated by how likely it is to belong to the estimated density function. This is a non-parametric technique meaning that the data is not assumed to take the shape of a given distribution. The probability density function is estimated using a set of kernels that are distributed across the data. The probability density is then estimated at each kernel location based on data that lie within a local neighborhood of the kernel. The density found at a point x within its neighboring n points is given by, 
        \begin{align}
            \hat{f}(x) = \frac{1}{n} \sum_{i=1}^n K_h(x-x_i)  = \frac{1}{nh} \sum_{i=1}^n K(\frac{x-x_i}{h}).
        \end{align}
        Here the $K()$ is the kernel function, and $h$ is a tunable hyperparameter for variance known as the bandwidth. This parameter smooths the distribution, the larger the bandwidth the smoother the density estimation becomes. Hence a too small bandwidth will lead to overfitting, and a too large bandwidth will lead to underfitting.   
        
        The kernel function can take on a number of forms, where the gaussian kernel,
        \begin{align}
            K(x-x_i;h) = e^-\frac{||x-x_i||^2}{2h^2}
        \end{align}
        is the most commonly used.
        
        
    % \subsection{K-nearest neigbours}\label{subsec:k_neig}
    %     outliers are datapoints that are far from the normal observations. 
        
    % \subsection{Timeseries forecasting using Neural Networks}\label{subsec:NN}
    %     present even if it will not be used due to the lack of consistency in the timeseries 
    %     Finne ein paper og citere på normal timeseries forecastin, denne vil eg då ikkje bruke sidan eg har aperiodisk sampling! 
    
    \subsection{Neural networks}
        The introduction to neural networks is based on my project thesis written fall 2017 \cite{myself} and \cite{Hastie}. 
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{report/figures/techniques/neural_network.pdf}
            \caption{A graph visualizing a N-level deep neural network}
            \label{fig:nn_fullnetwork}
        \end{figure}
        
        A NN is a n class classification or regression algorithm. For anomaly detection a NN can be trained to recreate the input of normal data. If the network is trained with enough normal data, it should be able to learn a good representation of patterns in this data. Abnormal data will in some way deviate form normal data, and hence its patterns are not familiar to the network. This will then result in recreated data that deviate more than normal from the input. This is also known as autoencoding. There are many types of NN specialized to solve specific tasks. Recurrent neural networks are designed to learn trends seen over a periode of time, and are therefore a good option when working with time series. Within the recurrent NN there is a type known as LSTM or long short term memory, which has shown to outperform the standard RNN when it comes to long horizon trends, \cite{Courville2016}. The following section introduces LSTM by first introducing NN and RNN.  
        
        A good visual representation is a graph with nodes and vertices as shown in Figure \ref{fig:nn_fullnetwork}. As seen, a NN consist of three parts. An input layer, a hidden layer and an output layer. The input layer is the feature vectors for the data samples, the hidden layer does all the computation and the output layer does the prediction. A NN with only one layer and a limited number of neurons, can approximate any function \cite{Hastie}.
        
        
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{report/figures/techniques/single_neuron.pdf}
            \caption{Visualization of a single neuron}
            \label{fig:nn_neuron}
        \end{figure}
        
        Each of the nodes marked  $z_{n,n}$ seen in Figure \ref{fig:nn_fullnetwork} is known as a neuron. A single neuron is visualized in Figure \ref{fig:nn_neuron}. Each neuron does two things. First it takes the dot product of the input vector and the weight vector, 
        
        \begin{equation}
            a = \bm \theta_n^T \bm Z_n,
            \label{eq:neuron_a}
        \end{equation}
        also shown in Figure \ref{fig:nn_neuron}. Then the output $Z_{n+1,n}$ is calculated 
        \begin{equation}
            Z_{n+1,n} = g(a_{n,n}).
            \label{eq:nn_activation}
        \end{equation}
        Here $g(a)$ is known as an activation function. There are many different options for activation functions, the most common are shown in Table \ref{tab:acitvations}. It is by using nonlinear activation functions, that a NN is enabled to approximate any function. If only linear functions are used, it can be verified that adding more layers does not but complicating a linear interpretation \cite{Hastie}. 
        
        
        %%%%%%%%% activation fucntions 
        % \begin{align}
        %     g_k(a) = a \textit{ identity function} \\
        %     g_k(a) = \frac{1}{1+e^{-a}} \textit{ sigmoid function} \\
        %     g_{k}(a) = \frac{e^{Tk}}{\sum_{i=1}^Ke^{Ti}} \textit{ softmax function} \\
        %     g_{k}(a) = \frac{e^a-e^{-a}}{e^a+e^{-a}} \textit{ tanh function} \\
        %     g_{k}(a) = max(a,0) \textit{ relu function}
        %     \label{eg:activations}
        % \end{align}
        
    \begin{table}[h]
        \centering
        \begin{tabular}{ | c | c |}
            \hline
            Function & Function\\ \hline
            Identity & $g_k(a) =a$ \\ \hline
            Sigmoid & $g_k(a) =\frac{1}{1+e^{-a}}$ \\ \hline
            Softmax & $g_k(a) =\frac{e^{Tk}}{\sum_{i=1}^Ke^{Ti}}$ \\ \hline
            Tanh & $g_k(a) =\frac{e^a-e^{-a}}{e^a+e^{-a}}$ \\ \hline
            Relu & $g_k(a) =max(a,0)$ \\ \hline
        \end{tabular}
        \caption{Different activation functions}
        \label{tab:acitvations}
    \end{table}
                
        % \begin{align}
        %     z_{m} =  \sigma(\theta_{0m} + \bm \theta_m^T \bm z_{m-1}), \textit{m = 1,..,M} \nonumber \\
        %     \bm \hat y = g(\bm z_m) \nonumber \\
        %     \label{eq:fwdprop} 
        %     % \textit{where $\sigma()$ and g() are different activation function}\nonumber 
        % \end{align}
        % In the first layer, $\bm z_1 = \bm x$. For all the other layers each $z_m$ is given by the activations function of a bias term pluss the dot product of the inputs $\bm z$ and the weight $\bm \theta$. This means that for each layer there exists $m$ $\bm \theta$ vectors, which can be written as $\bm \Theta_m$. For the regression classifiers one only had one vector $\bm \theta$, but for a NN you end up with $m$ $\bm \Theta$ matrices. This gives a good indication on why a NN can approximate any given function. 
        
        In the final layer, the output $\hat y$ is calculated. The number of elements in the final layer depends as mentioned if classification or regression is being performed. For regression, the final layer normally only has one element, but for classification, it holds as many elements as there are classes. For binary classification, the Sigmoid function is normally used. For k-class classification, the Softmax function is used, and for regression the identity function.
        
        The weights of a NN is updated through two steps, forward and backwards propagation. Forward propagation the input is fed through the mathematical operations in the network as seen in figure \ref{fig:nn_neuron} and \ref{fig:nn_fullnetwork} and a set of labels or values are predicted. The cost function 
        \begin{align}
            E(\bm\Theta) = &\sum_{i=1}^M\frac{1}{2}(\hat{y} - y)^2 \nonumber,
            \label{nn:cost}
        \end{align}
        expresses how well the NN is able to correctly predict its output. The NN weights or coefficients $\bm \Theta = [\bm \Theta_1 ... \bm \Theta_n]$ are then updated through a step called backpropagation. The cost function is minimized as a function of the weigths. Gradient descent or another optimization algorithm is used to calculate each of the different parameters in $\bm \Theta$'s contribution to the cost. $\bm \Theta$ is updated as 
        \begin{align}
            \bm \Theta := & \bm \Theta - \alpha \nabla E(\bm \Theta) \nonumber \\
            \Theta_{ij} = & \Theta_{ij} - \alpha \sum_{k=1}^M \frac{\partial E_k}{\partial \Theta_{ij}} 
            \label{bp:1}
        \end{align}
        Here $\alpha$ is the learning rate, a value between [0,1].
        
        The intuitive explanation to what the backpropagation does, is that the prediction errors, can be back-propagated to calculate the error in each neurons prediction. This prediction error is then used to update the corresponding weights for the neuron in matrix $\bm \Theta$. A full mathematical proof can be found in \cite{myself}
        
        % % Equation \ref{bp:1} to \ref{bp:9} shows how the partial derivative of the cost function is computed.   
        
        % \begin{align}
        %     \frac{\partial E}{\partial \Theta_{ji}} = \frac{\partial E}{\partial a_{ji}} \frac{\partial a_{ji}}{\partial \Theta_{ji}} \label{bp:2}.
        % \end{align}
        % Solving for the first term,
        % \begin{align}
        %     \delta_{ji} \coloneqq \frac{\partial E}{\partial a_{j,i}} = \sum_{k=1}^N \frac{\partial E}{\partial a_{j+1,k}} \frac{\partial a_{j+1,k}}{\partial a_{ji}} \label{bp:3}
        % \end{align}
        % where
        % \begin{align}
        %      \frac{\partial E}{\partial a_{j+1,k}} = \delta_{j+1,k}
        % \end{align}
        % and
        % \begin{align}
        %     \frac{\partial a_{j+1,k}}{\partial a_{ji}} = \frac{\partial \bm \theta_{j+1}^T \bm Z_{j+1}}{\partial a_{ji}}. \label{bp:4}
        % \end{align}
        % $\bm \theta_{j+1}^T \bm Z_{j+1}$ can be written as, 
        % \begin{align*}
        %     \bm \theta_{j+1}^T \bm Z_{j+1} = \Theta_{j+1,0}Z_{j+1,0} +\textit{ }..+ \textit{ }\Theta_{j+1,N}Z_{j+1,N} \nonumber \\
        %     = \Theta_{j+1,0}g(a_{j,0}) + \textit{ }.. +\textit{ }\Theta_{j+1,N}g(a_{j,N}). \label{bp:5}
        % \end{align*}
        % This gives, 
        % \begin{align}
        %     \frac{\partial a_{j+1,k}}{\partial a_{ji}} = \Theta_{j+1,i}\frac{\partial g(a_{j,i})}{\partial a_{ji}} =  \Theta_{j+1,i}g'(a_{ji}), \label{bp:6} 
        % \end{align}
        % which finally yields
        % \begin{align}
        %     \delta_{ji} = g'(a_{ji})\sum_{k=1}^N\delta_{j+1,k}\Theta_{j,k}. \label{bp:7}
        % \end{align}                    
        % Solving for the second term
        % \begin{align}
        %     a_{ji} = \bm \theta^T_i \bm Z_{i} \label{bp:10}
        % \end{align}
        % \begin{align}
        %     \frac{\partial a_{ji}}{\partial \Theta_{ij}} =  Z_{ji} \label{bp:8}
        % \end{align}
        % Gives the following equation
        % \begin{align}
        %     \frac{\partial E}{\partial \Theta_{ji}} = \delta_{ji}Z_{ji} \label{bp:9}
        % \end{align}
        
        The forward and backwardpropagation steps are then iterated over until a given number of iterations are reached. A complete pass forward and backward of all training examples, is known as one epoch. When the training set is large, it is often split into smaller batches to speed up calculations. The batch size tells you how many samples should be included in each forward and backward pass. Splitting the data into batches enables the algorithm to take advantage of parallelism, running the separate batches on separate CPU's or GPU's. 
        
        early stopping, mention the use of it. 
            
        When you design a NN, you are free to make it as wide and deep as you want. However, the deeper you make it, the more processing each iteration through it will take. Deep networks also increases the risk of overfitting. A good approach is to start with a shallow network, and increase the depth and width until you see that adding new layers, does not yield better performance. Other means to handle overfitting is regularization (making it costly to have large weights) and dropout. Dropout removes some edges between the neurons between each iteration, simplifying the network structure. 
        % To avoid overfitting the regularization term 
        % \begin{align}
        %     \frac{\lambda}{2} \sum_{km} \Theta_{km}^2  \nonumber \\
        %     = \frac{\lambda}{2}\bm \Theta^T \bm \Theta,
        %     \label{nn:overfit}
        % \end{align} 
        % is added to the cost function. The extra term in the cost function will penalize parameters $\neq 0$. This slightly modifies the update rule as 
        % \begin{align}
        %     \bm \Theta = & \bm \Theta - \alpha \nabla E(\bm \Theta) - \alpha \frac{\lambda}{2} \frac{\partial \bm \Theta^T \bm \Theta}{\partial \bm \Theta} \nonumber \\
        %     = &(1-\alpha \lambda)\bm \Theta - \alpha \frac{\partial E(\bm \Theta)}{\partial \bm \Theta}.
        %     \label{nn:updaterule2}
        % \end{align}
        % Since the regularization term is introduced to reduce the weights in the NN, it is a natural conclusion that the weights should be initialized to small values. Zero initialization is however not a good option, this will lead to zero derivatives, and hence the model will never be updated. Initializing all weights to a random number near zero, will give a roughly linear model \cite{Hastie}. This means that the model will start out more or less as a linear function and will grown in nonlinearity if necessary.
    
    
    \subsection{RNN}\label{subsubsec:RNN}
        Recurrent NN is a type of NN designed for sequential data. The RNN is specialized for processing values that belongs to a sequence, as often is the case with timeseries data. It is designed to scale to much longer sequences than what is practical for standard fully connected neural networks. The key is parameter sharing, which enables one to apply a model to different sequence lengths, and generalize accros them. The following quote from \cite{Courville2016} explains this well. "For example, consider the two sentences “I went to Nepal in 2009” and “In 2009, I went to Nepal.” If we ask a machine learning model to read each sentence and extract the year in which the narrator went to Nepal, we would like it to recognize the year 2009 as the relevant piece of information, whether it appears in the sixth word or in the second word of the sentence. Suppose that we trained a feedforward network that processes sentences of fixed length. A traditional fully connected feedforward network would have separate parameters for each input feature, so it would need to learn all the rules of the language separately at each position in the sentence. By comparison, a recurrent neural network shares the same weights across several time steps". Figure \ref{fig:RNN} shows a representation of a RNN. The RNN can be unfolded into a set of networks for a finite number of time steps as shown in the right of figure \ref{fig:RNN}, once the network is unfoleded in this way the backpropagation can be computed in a similar manner as explained in the above section. The internal state of the NN is updated as shown in equation \ref{eq:RNN}, here it becomes apparent that the current state is dependent on the previous state, which again is dependent on its predecessor, and so on. This clearly shows the recurrent pattern. 
        
        
        \begin{align}
            h^{(t)} = f(h^{(t-1)},x^t;\theta) 
            \label{eq:RNN}
        \end{align}
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{report/figures/techniques/RNN.pdf}
            \caption{A recurrent neural network, in compact and unrolled form.}
            \label{fig:RNN}
        \end{figure}
    
        One of the downfalls of the RNN is that as the depth of the network grows with the number of time steps. This introduces two problems known as vanishing and exploding gradient. This phenomena is explained in detail in \cite{Pascanu}. The short version is that as the number of time steps grows the size of the network grows. This means that the number of computations in the backprogation algorithm also grows. It can be shown that each of the n layers as seen in figure \ref{fig:RNN} gets its parameters updated as a partial derivative of the cost function for its prediction, in addition to the partial derivative of the next network. The exploding or vanising gradient problem is easily verified by examining 
        \begin{align}
            \frac{\partial \mathcal{E}}{\partial \theta} = \sum_{1\leq k \leq t}(\frac{\partial \mathcal{E}_t}{\partial \theta} \\
            \frac{\partial \mathcal{E}_t} = \sum_{1\leq k \leq t}(\frac{\partial \mathcal{E}_t}{\partial \bm{x_t}} \frac{\partial \bm{x_t}}{\partial \bm{x_k}}\frac{\partial \bm{x_k}}{\partial \theta}\\
            \frac{\partial \bm{x_t}}{\partial \bm{x_k}} = \prod_{t \geq i\geq k} \frac{\partial \bm{x_i}}{\partial \bm{x_{i-1}}}\label{eq:rnn_xprod}, 
        \end{align}
        which is taken from \cite{Pascanu}. As seen in equation \ref{eq:rnn_xprod} the partial becomes a product of all internal states of the timesteps in between t and k. Hence, if these values are $\geq1$ the gradient will grow, and if they are $\leq 1$ they will decrease. As the number of timesteps grows, the gradient will then either explode of vanish. A possible solution to this problem is presented in the next section.  
    
    \subsection{LSTM}
        Long Short Term Memory Networks or LSTM is an extension of the RNN seen in subsection \ref{subsubsec:RNN}. The idea behind it is to create loops where the gradient can flow for long durations, and hence solving the issue with vanishing and exploding gradients. LSTM was introduced in \cite{Hochreiter1997}, and has been found very successful in many different applications. Figure  \ref{fig:lstm} shows the building blocks of the LSTM. The LSTM consist of four internal layers, forget layer, input gate layer, two tanh layers and the output layer. All of those layers are a separate neural network layer. The previous cell state is fed in at the top of the LSTM, the previous state is kept based on the previous output $\bm h_{t-1}$ and the current input $\bm x_t$ which is fed through the forget layer. If none of the previous state is seen as relevant, the output will be a vector of zeros, if the previous state is seen as extremely important it will output a vector of ones. Next, the information in the new input is analyzed, the tanh layer creates an estimation of the current lstm state $\bm c_t$ based on $\bm h_{t-1}$ and $\bm x_t$, this value  is then scaled by the output of a sigmoid layer $\bm I_t$ which decides which of the internal states to update. $\bm c_t$ is then computed as a function of the previous state and the new state estimation. The output is calculated by sending the current cell state through a tanh layer, before a sigmoid layer decides which part of the cell state to output.  
        
        \begin{figure}
            \centering
            \includegraphics[width = \textwidth]{report/figures/techniques/lstm.png}
            \caption{A lstm network, CC BY-SA 4.0, François Deloche} %\url{https://commons.wikimedia.org/wiki/File:Long_Short-Term_Memory.svg}
            \label{fig:lstm}
        \end{figure}
        
        
        LSTM is proposed as a scheme for anomaly detection in \cite{Malhotra2016} and \cite{Malhotra}. It is shown that the scheme is working for both periodic and aperiodic timeseries. A LSTM network is a reccurent neural network, that have been successfully used for handwritten text recognition and speech recognition. The paper proposes a an encoder-decoder scheme designed for anomaly detection. A timeseries is encoded, and then decoded back to reconstruct the input series. The reconstruction error, the error between the input and the reconstructed input is then used to identify outliers. In the training phase the system is only shown normal data, hence it only learns to reconstruct the normal system behaviour. The network should then not be able to reconstruct anomalous data as good as with normal data. The paper concludes that LSTM networks can be a viable approach for anomaly detection. It also shows promise in detecting anomalies from unpredictable time-series.      
        