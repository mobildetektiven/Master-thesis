@article{PedregosaF.andVaroquauxG.andGramfortA.andMichel2011,
author = {{Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel}, V. and {and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer}, P. and and {and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos}, A. and {Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay}, E.},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@article{Agarwal2012,
abstract = {We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation {\$}\backslashmathfrak{\{}X{\}}{\$} of the sum of an approximately) low rank matrix {\$}\backslashTheta{\^{}}\backslashstar{\$} with a second matrix {\$}\backslashGamma{\^{}}\backslashstar{\$} endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including factor analysis, multi-task regression, and robust covariance estimation. We derive a general theorem that bounds the Frobenius norm error for an estimate of the pair {\$}(\backslashTheta{\^{}}\backslashstar, \backslashGamma{\^{}}\backslashstar){\$} obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results utilize a "spikiness" condition that is related to but milder than singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices {\$}\backslashTheta{\^{}}\backslashstar{\$} that can be exactly or approximately low rank, and matrices {\$}\backslashGamma{\^{}}\backslashstar{\$} that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error. The sharpness of our predictions is confirmed by numerical simulations.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J.},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {978-1-4503-0619-5},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Composite regularizers,High-dimensional inference,Nuclear norm},
number = {2},
pages = {1171--1197},
pmid = {23285570},
title = {{Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions}},
volume = {40},
year = {2012}
}
@article{Stenman2003,
abstract = {A method for detecting static friction (stiction) in$\backslash$ncontrol valves is proposed. The method is model-based$\backslash$nand is inspired by ideas from the fields of change$\backslash$ndetection and multi-model mode estimation. Opposed to$\backslash$nexisting methods only limited process knowledge is$\backslash$nneeded and it is not required that the loop has$\backslash$noscillating behaviour. The advantage of the method is$\backslash$nillustrated in both numerical simulations and$\backslash$nevaluations on real loop data. Copyright {\textcopyright} 2003 John$\backslash$nWiley {\&} Sons, Ltd.},
author = {Stenman, A. and Gustafsson, F. and Forsman, K.},
doi = {10.1002/acs.769},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stenman, Gustafsson, Forsman - 2003 - A segmentation-based method for detection of stiction in control valves(2).pdf:pdf},
issn = {0890-6327},
journal = {International Journal of Adaptive Control and Signal Processing},
keywords = {friction,multi‐model mode estimation,stiction,valve diagnosis},
month = {sep},
number = {7-9},
pages = {625--634},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{A segmentation-based method for detection of stiction in control valves}},
url = {http://doi.wiley.com/10.1002/acs.769},
volume = {17},
year = {2003}
}
@article{Dau2012,
abstract = {Theory of Mind (ToM) is the ability to read another person's mind. To apply ToM in robots, robot should read the intention from target. However, it is difficult to read target's intention directly. Robot uses the sensors to measure distance from target because distance is the feature to read target's intention. Neural network has been widely used to control the robot for generating a diverse speciation. It has been less explored in behavior-based robotics. Speciation usually relies on a distance measure that allows different from the robot to target to be compared. In this paper, we proposed novel measure to generate diverse behaviors of a robot with speciation for ToM. It includes some distance measure such as Euclidean distance, cosine distance, arctangent distance, and edit distance. It generates diverse behaviors of the robot by neural network for ToM. The proposed method has been experimented on a real e-puck robot platform. {\textcopyright} 2012 Springer-Verlag.},
author = {Dau, Hoang Anh and Ciesielski, Vic and Song, Andy},
doi = {10.1007/978-3-642-34859-4},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dau, Ciesielski, Song - 2012 - Simulated Evolution and Learning.pdf:pdf},
isbn = {978-3-642-34858-7},
issn = {03029743},
number = {December},
title = {{Simulated Evolution and Learning}},
url = {http://link.springer.com/10.1007/978-3-642-34859-4},
volume = {7673},
year = {2012}
}
@article{Choudhury2006,
abstract = {Stiction is a common problem in spring-diaphragm type valves, which are widely used in the process industry. Although there have been many attempts to understand and detect stiction in control valves, none of the current methods can simultaneously detect and quantify stiction. Conventional invasive methods such as the valve travel test can easily detect stiction, but are expensive and tedious to apply to hundreds of valves to detect stiction. Thus there is a clear need in the process industry for a non-invasive method that can not only detect but also quantify stiction so that the valves that need repair or maintenance can be identified, isolated and repaired. This work describes a model free method that can detect and quantify stiction that may be present in control valves using routine operating data obtained from the process. No additional excitation or experimentation of the plant is required. Over a dozen industrial case studies have demonstrated the wide applicability and practicality of this method as an useful diagnostic aid in control loop performance monitoring. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Choudhury, M. A A Shoukat and Shah, S. L. and Thornhill, N. F. and Shook, David S.},
doi = {10.1016/j.conengprac.2005.10.003},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury et al. - 2006 - Automatic detection and quantification of stiction in control valves.pdf:pdf},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Actuator,Control valve,Deadband,Hysteresis,Stickband,Stiction},
month = {dec},
number = {12},
pages = {1395--1412},
publisher = {Pergamon},
title = {{Automatic detection and quantification of stiction in control valves}},
url = {http://www.sciencedirect.com/science/article/pii/S0967066105002388},
volume = {14},
year = {2006}
}
@article{Sakurada2014,
abstract = {This paper proposes to use autoencoders with nonlinear di-mensionality reduction in the anomaly detection task. The authors apply dimensionality reduction by using an autoen-coder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property. The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data. This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails. Also, autoencoders can increase their accuracy by extending them to denoising au-toenconders. Moreover, autoencoders can be useful as non-linear techniques without complex computation as kernel PCA requires. Finaly, the authors examine the learned fea-tures in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.},
author = {Sakurada, Mayu and Yairi, Takehisa},
doi = {10.1145/2689746.2689747},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakurada, Yairi - 2014 - Anomaly detection using autoencoders with nonlinear dimensionality reduction.pdf:pdf},
isbn = {9781450331593},
journal = {Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis},
keywords = {I54 [Pattern recognition],au-toencoder,auto-assosiative neural network,denoising au-toencoder,dimensionality reduction,fault detection,nonlinear,novelty detection,spacecrafts},
pages = {4},
title = {{Anomaly detection using autoencoders with nonlinear dimensionality reduction}},
url = {http://delivery.acm.org/10.1145/2690000/2689747/p4-Sakurada.pdf?ip=129.241.187.53{\&}id=2689747{\&}acc=ACTIVE SERVICE{\&}key=CDADA77FFDD8BE08.5386D6A7D247483C.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=1017423379{\&}CFTOKEN=89893174{\&}{\_}{\_}acm{\_}{\_}=1513507205{\_}e86eb51f725e21a4c24},
year = {2014}
}
@misc{Dias2016,
abstract = {Since the early 1980s, wind power technology has experienced an immense growth with respect to both the turbine size and market share. As the demand for large-scale wind turbines and lor operation {\&} maintenance cost continues to raise, the interest on condition monitoring system has increased rapidly. The main components of wind turbines are the focus of all CMS since they frequently cause high repair costs and equipment downtime. However, vast quantities of their failures are caused due to a bearing failure. Therefore, bearing condition monitoring becomes crucial. This paper aims at providing a state-of-the-art review on wind turbine bearing condition monitoring techniques such as acoustic measurement, electrical effects monitoring, power quality, temperature monitoring, wear debris analysis and vibration analysis. Furthermore, this paper will present a literature review and discuss several technical, financial and operational challenges from the purchase of the CMS to the wind farm monitoring stage.},
author = {{De Azevedo}, Henrique Dias Machado and Ara{\'{u}}jo, Alex Maur{\'{i}}cio and Bouchonneau, Nad{\`{e}}ge},
booktitle = {Renewable and Sustainable Energy Reviews},
doi = {10.1016/j.rser.2015.11.032},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Azevedo, Ara{\'{u}}jo, Bouchonneau - 2016 - A review of wind turbine bearing condition monitoring State of the art and challenges.pdf:pdf},
isbn = {13640321},
issn = {18790690},
keywords = {Condition monitoring,Fault detection,Future challenges,Rolling element bearing,State-of-the-art,Wind turbines},
pages = {368--379},
title = {{A review of wind turbine bearing condition monitoring: State of the art and challenges}},
url = {https://ac.els-cdn.com/S1364032115012976/1-s2.0-S1364032115012976-main.pdf?{\_}tid=1d730dd0-e315-11e7-bb73-00000aab0f6c{\&}acdnat=1513506729{\_}c4b5bddcd08d86d2be8afe542577200e},
volume = {56},
year = {2016}
}
@article{Blum1997,
abstract = {In this survey, we review work in machine learning on$\backslash$nmethods for handling data sets containing large amounts$\backslash$nof irrelevant information. We focus on two key issues:$\backslash$nthe problem of selecting relevant features, and the$\backslash$nproblem of selecting relevant examples. We describe the$\backslash$nadvances that have been made on these topics in both$\backslash$nempirical and theoretical work in machine learning, and$\backslash$nwe present a general framework that we use to compare$\backslash$ndifferent methods. We close with some challenges for$\backslash$nfuture work in this area.},
author = {Blum, Avrim L. and Langley, Pat},
doi = {10.1016/S0004-3702(97)00063-5},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum, Langley - 1997 - Selection of relevant features and examples in machine learning.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Machine learning,Relevant examples,Relevant features},
number = {1-2},
pages = {245--271},
title = {{Selection of relevant features and examples in machine learning}},
url = {https://ac.els-cdn.com/S0004370297000635/1-s2.0-S0004370297000635-main.pdf?{\_}tid=1cb7170c-e265-11e7-ab2f-00000aab0f6b{\&}acdnat=1513431136{\_}a11bc0fd83102eb484da915fb5873767 http://linkinghub.elsevier.com/retrieve/pii/S0004370297000635},
volume = {97},
year = {1997}
}
@article{Dozat,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - 2016 - Incorporating Nesterov Momentum into Adam(2).pdf:pdf},
journal = {ICLR Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ},
year = {2016}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:pdf},
journal = {ICLR Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {http://cs229.stanford.edu/proj2015/054{\_}report.pdf},
year = {2016}
}
@inproceedings{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimen-tal results on artiicial data and theoretical re-sults in restricted settings have shown that for selecting a good classiier from a set of classi-(model selection), ten-fold cross-validation may be better than the more expensive l e a ve-one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the eeects of diierent parameters on these al-gorithms on real-world datasets. For cross-validation, we v ary the number of folds and whether the folds are stratiied or nott for boot-strap, we v ary the number of bootstrap sam-ples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratiied cross validation, even if computation power allows using more folds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kohavi, Ron},
booktitle = {Appears in the International Joint Conference on Articial Intelligence (IJCAI)},
doi = {10.1067/mod.2000.109031},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohavi - 1995 - A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
isbn = {1-55860-363-8},
issn = {10450823},
pages = {1--7},
pmid = {11029742},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
url = {http://robotics.stanford.edu/{~}ronnyk},
year = {1995}
}
@article{Joachims1998,
abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Joachims, Thorsten and Dortmund, Universitat and Joachimscsuni-dortmundde, Thorsten},
doi = {10.1109/ICEMI.2009.5274151},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims, Dortmund, Joachimscsuni-dortmundde - 1999 - Making Large-Scale SVM Learning Practical.pdf:pdf},
isbn = {9781424438631},
issn = {15279995},
journal = {Advances in Kernel Methods - Support Vector Learning},
keywords = {310},
pages = {41--56},
pmid = {18244602},
publisher = {Dortmund: SFB 475, Universit{\"{a}}t Dortmund},
title = {{Making Large-Scale SVM Learning Practical}},
url = {https://www.econstor.eu/handle/10419/77178 http://svmlight.joachims.org/{\%}5Cnhttps://eldorado.uni-dortmund.de/handle/2003/2596},
year = {1999}
}
@article{Hernandez1998,
abstract = {Abstract The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem ...$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hern{\'{a}}ndez, Mauricio A and Stolfo, Salvatore J},
doi = {10.1023/A:1009761603038},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez, Stolfo - 1998 - Real-world Data is Dirty Data Cleansing and The MergePurge Problem.pdf:pdf},
isbn = {1384-5810},
issn = {13845810 (ISSN)},
journal = {Data Mining and Knowledge Discovery},
keywords = {data cleaning,data cleansing,duplicate elimination,semantic integration},
number = {1},
pages = {9--37},
pmid = {25246403},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038{\%}5Cnpapers3://publication/doi/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@article{Abadi,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://arxiv.org/pdf/1603.04467.pdf http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{Gola2012,
abstract = {Abstract In oil and gas industries, drilling is a complex and critical operation which require constant and accurate real-time monitoring. To this aim, real-time models are required to provide an overview of the drilling operations when direct and reliable measurements are not available. Given the harsh operating environment, sensor reliability and calibration are critical issues and bad data quality is a typical problem which affects the accuracy of the model. As a result, the driller may be misled about the down-hole situation or receive conflicting claims about operating conditions. This paper presents two approaches based on the use of artificial intelligence to improve monitoring of drilling processes in terms of reduced uncertainty and increased confidence. The first exploits the aggregation of the opinion of different experts within a so-called ensemble approach; the second is based on a so-called grey-box approach which combines a physical model and artificial intelligence. The two approaches are applied to the problem of predicting the bottom-hole pressure during a managed pressure drilling operation to demonstrate the improved accuracy and robustness. 1. Introduction A major critical task when starting the exploitation of a new well is the process of drilling the borehole. Drilling is a close-loop process in which the drilling fluid is pumped into the drill pipe at a pressure enough to cause it to circulate downwardly through the drill pipe, the drill collars, the bit nozzles and upwardly through the annulus between the borehole and the drill pipe back to the surface where it goes through a reconditioning process and is finally re-circulated through the pipes. The purpose of the drilling fluid is to remove rock and sediment fragments produced by the bit during drilling, to transport them to the surface, to cool the bit and to maintain pressure balance against the pressure in the rock formation. For these reasons, it is critical to ensure a constant fluid inlet at the right pressure at all time. A sudden loss of drilling fluid would lead in fact to built-up of rock fragments which, if not detected at an early stage, might lead to much worse situations such as stuck drill pipes, bit failures, drill string twist-off and more, resulting in significant non-productive time due to dangerous fishing trips or, in the worst cases, in the loss of expensive bottom-hole assemblies, potentially in the loss of the entire well and even in blowouts with extreme consequences like large financial losses, severe environmental damages and possible loss of lives. Given the critical role played by the drilling fluid especially in maintaining the correct pressure balance, control systems are adopted to regulate the drilling fluid influx with the purpose of precisely controlling the pressure profiles throughout the well hole. This paper focuses on the so-called Managed Pressure Drilling (MPD) system. The objectives of MPD are to ascertain the well bottom-hole pressure environment limits and to accordingly maintain the annular hydraulic pressure profile within its boundaries, i.e. above the pore pressure of the reservoir or the collapse pressure of the borehole and below the fracturing pressure of the borehole eventually responding rapidly to undesired events. Failure to maintain the correct pressure can for instance result in loss of drilling fluid to the formation or unexpected reservoir influxes (especially gas) which in the worst case scenario can lead to surface blowouts. A simplified model of the MPD control system is sketched in Figure 1. For a detailed description of the MPD system refer to [1].},
author = {Gola, Giulio and Nybo, R and Sui, D and Roverso, D},
doi = {10.2118/150201-MS},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gola et al. - 2012 - Improving Management and Control of Drilling Operations with Artificial Intelligence.pdf:pdf},
isbn = {9781618399311},
journal = {SPE Intelligent Energy {\ldots}},
number = {March},
pages = {1--7},
title = {{Improving Management and Control of Drilling Operations with Artificial Intelligence}},
url = {https://www.onepetro.org/conference-paper/SPE-150201-MS},
year = {2012}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Hawkins,
abstract = {Data Warehouse (DWH) systems allow to analyze business objects relevant to an enterprise organization (e.g., orders or customers).$\backslash$n $\backslash$n Analysts are interested in the states of these business objects: A customer is either a potential customer, a first time customer,$\backslash$n a regular customer or a past customer; purchase orders may be pending or fullfilled.$\backslash$n $\backslash$n $\backslash$n $\backslash$n Business objects and their states can be distributed over many parts of the DWH, and appear in measures, dimension attributes,$\backslash$n levels, etc.$\backslash$n $\backslash$n $\backslash$n $\backslash$n Surprisingly, this knowledge – how business objects and their states are represented in the DWH – is not made explicit in$\backslash$n existing conceptual models. We identify a need to make this relationship more accessible.$\backslash$n $\backslash$n $\backslash$n $\backslash$n We introduce the UML Profile for Representing Business Object States in a DWH. It makes the relationship between the business objects and the DWH conceptually visible. The UML Profile is applied to an$\backslash$n example.},
author = {Stefanov, Veronika and List, Beate},
doi = {10.1007/978-3-540-74553-2},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stefanov, List - 2007 - Outlier Detection Using Replicator Neural Networks.pdf:pdf},
isbn = {978-3-540-74552-5},
issn = {0302-9743},
journal = {Data Warehousing and Knowledge Discovery},
number = {DECEMBER},
pages = {209--220},
title = {{Outlier Detection Using Replicator Neural Networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.3366{\&}rep=rep1{\&}type=pdf http://www.springerlink.com/content/28433722x2204702},
volume = {4654},
year = {2007}
}
@inproceedings{Collobert,
abstract = {Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce ( i ) faster SVMs where training errors are no longer support vectors, and ( ii ) much faster Transductive SVMs.},
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L{\'{e}}on},
booktitle = {International Conference on Machine Learning},
doi = {10.1145/1143844.1143870},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
isbn = {1-59593-383-2},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://delivery.acm.org/10.1145/1150000/1143870/p201-collobert.pdf?ip=129.241.187.53{\&}id=1143870{\&}acc=ACTIVE SERVICE{\&}key=CDADA77FFDD8BE08.5386D6A7D247483C.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=832816127{\&}CFTOKEN=92573539{\&}{\_}{\_}acm{\_}{\_}=1511432934{\_}9135dee02b5971768},
year = {2006}
}
@article{Baldi1989,
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. {\textcopyright} 1989.},
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1989 - Neural networks and principal component analysis Learning from examples without local minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
url = {https://ac.els-cdn.com/0893608089900142/1-s2.0-0893608089900142-main.pdf?{\_}tid=104bc7f4-d038-11e7-88d7-00000aacb35f{\&}acdnat=1511432667{\_}42d620ab61954044a9c22d5eed2dc6f1},
volume = {2},
year = {1989}
}
@article{Scholkopf2001,
abstract = {Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.},
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J. and Williamson, Robert C.},
doi = {10.1162/089976601750264965},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1443--1471},
pmid = {11440593},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Estimating the Support of a High-Dimensional Distribution}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976601750264965},
volume = {13},
year = {2001}
}
@article{Tax2004a,
abstract = {Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.},
author = {Tax, David M J and Fisher, Douglas},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Fisher - 2004 - Support Vector Data Description.pdf:pdf},
journal = {Machine Learning},
keywords = {novelty detection,one-class classification,outlier detection,support vector classifier,support vector data description},
pages = {45--66},
title = {{Support Vector Data Description}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/ML{\_}SVDD{\_}04.pdf},
volume = {54},
year = {2004}
}
@article{Ng,
author = {Ng, Andrew},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2017 - CS229 Additional Notes on Backpropagation.pdf:pdf},
title = {{CS229: Additional Notes on Backpropagation}},
url = {http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf},
year = {2017}
}
@article{Nga,
abstract = {We now begin our study of deep learning. In this set of notes, we give an overview of neural networks, discuss vectorization and discuss training neural networks with backpropagation.},
author = {Ng, Andrew},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2017 - CS229 Lecture Notes Deep Learning.pdf:pdf},
title = {{CS229 Lecture Notes Deep Learning}},
url = {http://cs229.stanford.edu/notes/cs229-notes-deep{\_}learning.pdf},
year = {2017}
}
@book{ChristopherM.Bishop,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bishop - 2013 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
url = {http://users.isr.ist.utl.pt/{~}wurmd/Livros/school/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf},
volume = {53},
year = {2013}
}
@article{Halbertwhite,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
url = {https://ac.els-cdn.com/0893608089900208/1-s2.0-0893608089900208-main.pdf?{\_}tid=2bf292d4-cae4-11e7-ab6e-00000aacb361{\&}acdnat=1510846880{\_}49ac579d7917cbe9b59e14659f1c357e},
volume = {2},
year = {1989}
}
@article{Korkov~1992,
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. {\textcopyright} 1992.},
author = {Kůrkov{\'{a}}, V{\v{e}}ra},
doi = {10.1016/0893-6080(92)90012-8},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kůrkov{\'{a}} - 1992 - Kolmogorov's theorem and multilayer neural networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Approximations of continuous functions,Estimates of number of hidden units,Feedforward neural networks,Modulus of continuity,Multilayer perceptron type networks,Sigmoidal activation function,Uniform approximation,Universal approximation capabilities},
number = {3},
pages = {501--506},
title = {{Kolmogorov's theorem and multilayer neural networks}},
url = {https://ac.els-cdn.com/0893608092900128/1-s2.0-0893608092900128-main.pdf?{\_}tid=966cf7d8-cad2-11e7-bbab-00000aab0f6c{\&}acdnat=1510839329{\_}ec0e83cebae14e704b46d386e6522adc},
volume = {5},
year = {1992}
}
@article{Kurkova1992,
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. {\textcopyright} 1992.},
author = {Kůrkov{\'{a}}, V{\v{e}}ra},
doi = {10.1016/0893-6080(92)90012-8},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kůrkov{\'{a}} - 1992 - Kolmogorov's theorem and multilayer neural networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Approximations of continuous functions,Estimates of number of hidden units,Feedforward neural networks,Modulus of continuity,Multilayer perceptron type networks,Sigmoidal activation function,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {3},
pages = {501--506},
publisher = {Pergamon},
title = {{Kolmogorov's theorem and multilayer neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/0893608092900128},
volume = {5},
year = {1992}
}
@article{Manevitz2007,
abstract = {Automated document retrieval and classification is of central importance in many contexts; our main motivating goal is the efficient classification and retrieval of "interests" on the internet when only positive information is available. In this paper, we show how a simple feed-forward neural network can be trained to filter documents under these conditions, and that this method seems to be superior to modified methods (modified to use only positive examples), such as Rocchio, Nearest Neighbor, Naive-Bayes, Distance-based Probability and One-Class SVM algorithms. A novel experimental finding is that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation ("Hadamard") of the information prior to the training of the network. {\textcopyright} 2006.},
author = {Manevitz, Larry and Yousef, Malik},
doi = {10.1016/j.neucom.2006.05.013},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manevitz, Yousef - 2007 - One-class document classification via Neural Networks.pdf:pdf},
isbn = {09252312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autoencoder,Automated document retrieval,Bottleneck neural network,Classification,Feed-forward neural networks,Machine learning,One-class classification},
number = {7-9},
pages = {1466--1481},
title = {{One-class document classification via Neural Networks}},
url = {http://cs.haifa.ac.il/{~}manevitz/Publication/One-class document classification via Neural Networks.pdf},
volume = {70},
year = {2007}
}
@book{Hastie,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {{Hastie, Trevor Tibshirani, Robert Friedman}, Jerome},
booktitle = {Springer},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Trevor Tibshirani, Robert Friedman - 2008 - The Elements of Statistical Learning Data Mining, Inference, and Prediction.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {0162-1459},
keywords = {inger series in statistics},
pmid = {21196786},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2004.s339},
volume = {2},
year = {2008}
}
@techreport{Energi21,
abstract = {Energi21 er Olje- og energidepartementets strategiorgan for forskning, utvikling og demonstrasjon innen energiomr{\aa}det. Hovedm{\aa}let med Energi21-strategiene er {\aa} gi anbe- falinger til Olje- og energidepartementet om fremtidige prioriteringer for satsingen innen utvikling av nye klima- og milj{\o}vennlige l{\o}sninger for energiomr{\aa}det},
author = {Energi21},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Energi21 - 2014 - Strategi 2014, del 2.pdf:pdf},
title = {{Strategi 2014, del 2}},
url = {https://www.forskningsradet.no/no/Publikasjon/Energi{\_}21{\_}strategi{\_}2014{\_}{\_}Del{\_}2/1253998081130?lang=no},
year = {2014}
}
@article{Choudhury2005,
abstract = {The presence of nonlinearities, e.g., stiction, and deadband in a control valve limits the control loop performance. Stiction is the most commonly found valve problem in the process industry. In spite of many attempts to understand and model the stiction phenomena, there is a lack of a proper model, which can be understood and related directly to the practical situation as observed in real valves in the process industry. This study focuses on the understanding, from real-life data, of the mechanism that causes stiction and proposes a new data-driven model of stiction, which can be directly related to real valves. It also validates the simulation results generated using the proposed model with that from a physical model of the valve. Finally, valuable insights on stiction have been obtained from the describing function analysis of the newly proposed stiction model. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
annote = {cited 250ish times Page 643 shows a figure of a valve deadbands Deadband: ‘‘In process instrumentation, it is the range through which an input signal may be varied, upon reversal of direction, without initiating an observable change in output signal''},
author = {{Shoukat Choudhury}, M. A.A. and Thornhill, N F and Shah, Sirish L},
doi = {10.1016/j.conengprac.2004.05.005},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shoukat Choudhury, Thornhill, Shah - 2005 - Modelling valve stiction.pdf:pdf},
isbn = {0967-0661},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Backlash,Coulomb friction,Deadband,Deadzone,Hysteresis,Process control,Slip jump,Stickband,Stiction,Viscous friction},
number = {5},
pages = {641--658},
title = {{Modelling valve stiction}},
url = {https://ac.els-cdn.com/S0967066104001145/1-s2.0-S0967066104001145-main.pdf?{\_}tid=c75b9a5c-bfdd-11e7-b665-00000aab0f26{\&}acdnat=1509634671{\_}d40835e745492fd86ead5ddce1a6aed9},
volume = {13},
year = {2005}
}
@inproceedings{Kingma,
abstract = {Login systems in smart devices demand multi-factor authentication for high security and at the same time, it requires simple user experience. We propose a novel application of lip-reading satisfying these requirements. We present the adequacy of lip-reading as a biometric factor by experiment. In addition, automatic lip-reader can be implemented by LSTM (Long Short Term Memory) neural network architecture with good accuracy that can translate visual utterance to password as a knowledge factor. Furthermore, our proposed method, iterative method, can improve accuracy as much as login system required. Our work achieved 93.8{\%} by single iteration from the first result (69.1{\%}).},
annote = {Adam is a stochastic first order gradient descent based method. It uses advantages from both AdaGrad and RMSprop in its impelentation and is a popular choice for optimization in machine learning. For more in depth information see $\backslash$cite{\{}adam{\}}},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lee, Daehyun and Myung, Kyungsik},
booktitle = {2017 IEEE International Conference on Consumer Electronics, ICCE 2017},
doi = {10.1109/ICCE.2017.7889386},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Myung - 2017 - Read my lips, login to the virtual world.pdf:pdf},
isbn = {9781509055449},
issn = {09252312},
pages = {434--435},
pmid = {172668},
title = {{Read my lips, login to the virtual world}},
url = {https://arxiv.org/pdf/1412.6980v8.pdf},
year = {2017}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@inproceedings{Sutskever,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dahl, Sainath, Hinton - 2013 - Improving deep neural networks for LVCSR using rectified linear units and dropout.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
url = {http://proceedings.mlr.press/v28/sutskever13.pdf},
year = {2013}
}
@article{Hernandez1998,
abstract = {Abstract The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem ...$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hern{\'{a}}ndez, Mauricio A. and Stolfo, Salvatore J.},
doi = {10.1023/A:1009761603038},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez, Stolfo - 1998 - Real-world Data is Dirty Data Cleansing and The MergePurge Problem.pdf:pdf},
isbn = {1384-5810},
issn = {13845810 (ISSN)},
journal = {Data Mining and Knowledge Discovery},
keywords = {data cleaning,data cleansing,duplicate elimination,semantic integration},
number = {1},
pages = {9--37},
pmid = {25246403},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038 http://link.springer.com/10.1023/A:1009761603038{\%}5Cnpapers3://publication/doi/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@article{Tax2004,
abstract = {Propose des SVM qui au lieu de discriminer deux classes essaye de$\backslash$ndecrire une classe. Possibilit� d'apprendre avec ou sans contre-exemples.},
author = {Tax, David M J and Duin, Robert P W},
doi = {10.1023/B:MACH.0000008084.60811.49},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Duin - 2004 - Support Vector Data Description.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Novelty detection,One-class classification,Outlier detection,Support vector classifier,Support vector data description},
number = {1},
pages = {45--66},
pmid = {20842844},
title = {{Support Vector Data Description}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AMACH.0000008084.60811.49.pdf},
volume = {54},
year = {2004}
}
@inproceedings{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
address = {New York, New York, USA},
annote = {2000+ citations bruke i argumetnasjon for F1 score, og kurver!},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - 2006 - The relationship between Precision-Recall and ROC curves(2).pdf:pdf},
isbn = {1595933832},
issn = {14710080},
pages = {233--240},
pmid = {19165215},
publisher = {ACM Press},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@inproceedings{Davis,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - 2006 - The relationship between Precision-Recall and ROC curves.pdf:pdf},
isbn = {1595933832},
issn = {14710080},
pages = {233--240},
pmid = {19165215},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://pages.cs.wisc.edu/{~}jdavis/davisgoadrichcamera2.pdf http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@article{Wolfe1959,
archivePrefix = {arXiv},
arxivId = {www.jstor.org/stable/43635235},
author = {Wolfe, Philip and Tucker, A. W.},
doi = {10.2307/1909468},
eprint = {stable/43635235},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolfe, Tucker - 1959 - The Simplex Method for Quadratic Programming.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
month = {jul},
number = {3},
pages = {382},
primaryClass = {www.jstor.org},
publisher = {The Technology Press of The Massachusetts Institute of Technology, Cambridge, Mass.},
title = {{The Simplex Method for Quadratic Programming}},
url = {http://www.jstor.org/stable/1909468?origin=crossref},
volume = {27},
year = {1959}
}
@article{BergstraJAMESBERGSTRA2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
annote = {Grid search, manual search vs random search. 
How dows grid search work? Grid search tests all possible combinations of your hyperparameters not all hyperparameters are important to tune! Therefore random search is better than grid search.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {{Bergstra JAMESBERGSTRA}, James and {Yoshua Bengio YOSHUABENGIO}, Umontrealca},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra JAMESBERGSTRA, Yoshua Bengio YOSHUABENGIO - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\_}1},
year = {2017}
}
