@article{Pan2008,
abstract = {The study aims at realizing a remote online machine condition monitoring system built up in the architecture of both the Borland C++ Builder (BCB) software-developing environment and Internet transmission communication. Various sig-nal-processing computation schemes such as time–frequency analysis and order tracking for signal analysis and pattern recognition purposes are implemented based upon the Borland C++ Builder graphical user interface. Thus machine fault diagnostic capability can be extended by using the socket application program interface as the transmission control pro-tocol/Internet protocol (TCP/IP). In the study, the effectiveness of the developed remote diagnostic system is justified by monitoring a transmission-element test rig. A complete monitoring cycle including data acquisition, signal-processing, fea-ture extraction, pattern recognition through the artificial neural networks, and online video surveillance, is demonstrated.},
author = {Pan, Min-Chun and Li, Po-Ching and Cheng, Yong-Ren},
doi = {10.1016/j.measurement.2008.01.004},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Li, Cheng - 2008 - Remote online machine condition monitoring system.pdf:pdf},
keywords = {Fault diagnosis,Machine condition monitoring,Remote monitoring,Signal-processing},
title = {{Remote online machine condition monitoring system}},
url = {https://ac.els-cdn.com/S0263224108000067/1-s2.0-S0263224108000067-main.pdf?{\_}tid=9df13660-5853-4cee-aeff-e2f713734b5e{\&}acdnat=1526288170{\_}0c78783821d3d208d2fdd3605fc66f77},
year = {2008}
}
@article{Widodo2007,
abstract = {Recently, the issue of machine condition monitoring and fault diagnosis as a part of maintenance system became global due to the potential advantages to be gained from reduced maintenance costs, improved productivity and increased machine availability. This paper presents a survey of machine condition monitoring and fault diagnosis using support vector machine (SVM). It attempts to summarize and review the recent research and developments of SVM in machine condition monitoring and diagnosis. Numerous methods have been developed based on intelligent systems such as artificial neural network, fuzzy expert system, condition-based reasoning, random forest, etc. However, the use of SVM for machine condition monitoring and fault diagnosis is still rare. SVM has excellent performance in generalization so it can produce high accuracy in classification for machine condition monitoring and diagnosis. Until 2006, the use of SVM in machine condition monitoring and fault diagnosis is tending to develop towards expertise orientation and problem-oriented domain. Finally, the ability to continually change and obtain a novel idea for machine condition monitoring and fault diagnosis using SVM will be future works.},
author = {Widodo, Achmad and Yang, Bo-Suk},
doi = {10.1016/j.ymssp.2006.12.007},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Widodo, Yang - 2007 - Mechanical Systems and Signal Processing Support vector machine in machine condition monitoring and fault diagnosi.pdf:pdf},
isbn = {0888-3270},
issn = {08883270},
journal = {Mechanical Systems and Signal Processing},
keywords = {Fault diagnosis,Machine condition monitoring,Support vector machine},
number = {6},
pages = {2560--2574},
title = {{Support vector machine in machine condition monitoring and fault diagnosis}},
url = {https://ac.els-cdn.com/S0888327007000027/1-s2.0-S0888327007000027-main.pdf?{\_}tid=2aa3df2c-ee0b-4e63-a335-2052850aeeea{\&}acdnat=1526288040{\_}dd85dd11edbdba4dad94202201df58bb http://linkinghub.elsevier.com/retrieve/pii/S0888327007000027},
volume = {21},
year = {2007}
}
@article{Molina2000,
abstract = {The use of Neural Networks (NN) is a novel approach that can help in taking decisions when integrated in a more general system, in particular with expert systems. In this paper, an architecture for the management of hydroelectric power plants is introduced. This relies on monitoring a large number of signals, representing the technical parameters of the real plant. The general architecture is composed of an Expert System and two NN modules: Acoustic Prediction (NNAP) and Predictive Maintenance (NNPM). The NNAP is based on Kohonen Learning Vector Quantization (LVQ) Networks in order to distinguish the sounds emitted by electricity-generating machine groups. The NNPM uses an ART-MAP to identify different situations from the plant state variables, in order to prevent future malfunctions. In addition, a special process to generate a complete training set has been designed for the ART-MAP module. This process has been developed to deal with the absence of data about abnormal plant situations, and is based on neural nets trained with the backpropagation algorithm.},
author = {Molina, J M and Isasi, P and Berlanga, A and Sanchis, A},
doi = {10.1016/S0952-1976(00)00009-9},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Molina et al. - Unknown - Hydroelectric power plant management relying on neural networks and expert system integration.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {ART,Expert systems,LVQ,Neural networks,Power plants,Predictive maintenance},
number = {3},
pages = {357--369},
title = {{Hydroelectric power plant management relying on neural networks and expert system integration}},
url = {https://ac.els-cdn.com/S0952197600000099/1-s2.0-S0952197600000099-main.pdf?{\_}tid=4e916db7-3e66-4e1f-9d9d-9151ab9ca503{\&}acdnat=1526287990{\_}d9e9a9d5b113ada40a1f34c61dd1e37b},
volume = {13},
year = {2000}
}
@article{Fast2008,
abstract = {Demonstration of different utilities for industrial use of an artificial neural network (ANN) model for a gas turbine has been reported in this paper. The ANN model was constructed with the multi-layer feed-forward network type and trained with operational data using back-propagation. The results showed that operational and performance parameters of the gas turbine, including identification of anti-icing mode, can be predicted with good accuracy for varying local ambient conditions. Different possible applications of this ANN model were also demonstrated. These include instantaneous gas turbine performance estimation through a graphical user interface and extrapolation beyond the range of training data. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Fast, M and Assadi, M and De, S},
doi = {10.1016/j.apenergy.2008.03.018},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fast, Assadi, De - 2008 - Development and multi-utility of an ANN model for an industrial gas turbine.pdf:pdf},
isbn = {0306-2619},
issn = {03062619},
journal = {Applied Energy},
keywords = {ANN,Gas turbine,Modelling,Simulation},
number = {1},
pages = {9--17},
title = {{Development and multi-utility of an ANN model for an industrial gas turbine}},
url = {https://ac.els-cdn.com/S030626190800072X/1-s2.0-S030626190800072X-main.pdf?{\_}tid=d40b9acd-6957-4b4b-ab93-d44f0545c438{\&}acdnat=1526287941{\_}4e87c19fcfe530fd7f116f693bcee092},
volume = {86},
year = {2009}
}
@article{Zhang2011,
abstract = {Hydropower station is one of key energy construction projects in china. The hydro-mechanical component is one of the core parts of the hydropower station. Therefore, the importance of hydro-condition monitoring becomes even more conspicuous in hydropower station. In this present study, the hardware of hydropower station hydraulic monitoring system was re-designed and integrated through researching hydro-determination theory and method of the hydropower station, using ADAM modules; At the same time, by using development tool C++ and assembly language, a 16-channel data acquisition and visualization operations hydro-monitoring system was completed by implementing the functions of acquisition, calibration, analysis, processing for the software system. Finally, the entire monitoring system was applied on Tianqiao hydropower, and the result indicated that this system might be a better method in monitoring results.},
author = {Zhang, Wumei},
doi = {10.1016/j.proeng.2011.08.150},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2011 - Development and Application of Hydropower hydro Monitoring System.pdf:pdf},
journal = {Procedia Engineering},
keywords = {ADAM modules,Hydraulic unit,data acquisition,vibration test},
pages = {807--811},
title = {{Development and Application of Hydropower hydro Monitoring System}},
url = {www.sciencedirect.com www.elsevier.com/locate/procedia},
volume = {15},
year = {2011}
}
@article{GarciaMarquez2012,
abstract = {Wind Turbines (WT) are one of the fastest growing sources of power production in the world today and there is a constant need to reduce the costs of operating and maintaining them. Condition monitoring (CM) is a tool commonly employed for the early detection of faults/failures so as to minimise downtime and maximize productivity. This paper provides a review of the state-of-the-art in the CM of wind turbines, describing the different maintenance strategies, CM techniques and methods, and highlighting in a table the various combinations of these that have been reported in the literature. Future research opportunities in fault diagnostics are identified using a qualitative fault tree analysis. {\textcopyright} 2012 .},
author = {{Garc{\'{i}}a M{\'{a}}rquez}, Fausto Pedro and Tobias, Andrew Mark and {Pinar P{\'{e}}rez}, Jes{\'{u}}s Mar{\'{i}}a and Papaelias, Mayorkinos},
doi = {10.1016/j.renene.2012.03.003},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a M{\'{a}}rquez et al. - 2012 - Condition monitoring of wind turbines Techniques and methods.pdf:pdf},
isbn = {0960-1481},
issn = {09601481},
journal = {Renewable Energy},
keywords = {Condition monitoring,Fault detection and diagnosis,Maintenance management,Wind turbines},
pages = {169--178},
title = {{Condition monitoring of wind turbines: Techniques and methods}},
url = {https://ac.els-cdn.com/S0960148112001899/1-s2.0-S0960148112001899-main.pdf?{\_}tid=5df85283-6819-4b42-b37b-33615eff4d19{\&}acdnat=1526287836{\_}7bd74fc09d2624b61bae40c7a8cab629},
volume = {46},
year = {2012}
}
@article{Selak2014,
abstract = {This paper presents a condition monitoring and fault diagnostics (CMFD) system for hydropower plants (HPP). CMFD is based on the concept of industrial product-service systems (IPS2), in which the customer, turbine supplier, and maintenance service provider are the IPS2 stakeholders. The proposed CMFD consists of signal acquisition, data transfer to the virtual diagnostics center (VDC) and fault diagnostics. A support vector machine (SVM) classifier has been used for fault diagnostics. CMFD has been implemented on an HPP with three Kaplan units. A signal acquisition system for CMFD consists of data acquisition from a unit control system and a supplementary system for high-frequency data acquisition. The implemented SVM method exhibits high training accuracy and thus enables adequate fault diagnostics. The data are analyzed in the VDC, which allows all stakeholders access to diagnostic information from anywhere at any time. Based on this information, the service providers can establish condition-based maintenance and offer operational support. Furthermore, through the VDC, cooperation between the stakeholders can be achieved; thus, better maintenance scheduling is possible, which will be reflected in higher system availability.{\textcopyright} 2014 Published by Elsevier B.V.},
author = {Selak, Luka and Butala, Peter and Sluga, Alojzij},
doi = {10.1016/j.compind.2014.02.006},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selak, Butala, Sluga - 2014 - Condition monitoring and fault diagnostics for hydropower plants.pdf:pdf},
isbn = {0166-3615},
issn = {01663615},
journal = {Computers in Industry},
keywords = {Condition monitoring,Fault diagnostics,Industrial product-service systems,Support vector machines},
number = {6},
pages = {924--936},
title = {{Condition monitoring and fault diagnostics for hydropower plants}},
url = {https://ac.els-cdn.com/S0166361514000402/1-s2.0-S0166361514000402-main.pdf?{\_}tid=8e73ce27-8ead-4ac5-adee-2ac822165479{\&}acdnat=1525945025{\_}d5a75c0ad338e97d87f85430579d1b1f},
volume = {65},
year = {2014}
}
@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\_}1},
year = {2017}
}
@misc{Statkraft2009,
abstract = {Vannkraft er en milj{\o}vennlig og fornybar energikilde. 99 prosent av all kraftproduksjon i Norge kommer fra vannkraft. P{\aa}verdensbasis utgj{\o}r vannkraften rundt en sjettedel av den totale kraftproduksjonen},
author = {Statkraft},
title = {{Vannkraft}},
url = {https://www.statkraft.no/globalassets/old-contains-the-old-folder-structure/documents/no/vannkraft-09-no{\_}tcm10-4585.pdf},
year = {2009}
}
@inproceedings{mckinney-proc-scipy-2010,
author = {McKinney, Wes},
booktitle = {Proceedings of the 9th Python in Science Conference},
editor = {van der Walt, St{\'{e}}fan and Millman, Jarrod},
pages = {51--56},
title = {{Data Structures for Statistical Computing in Python}},
year = {2010}
}
@article{Kurkova1992,
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. {\textcopyright}1992.},
author = {Kůrkov{\'{a}}, V{\v{e}}ra},
doi = {10.1016/0893-6080(92)90012-8},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Approximations of continuous functions,Estimates of number of hidden units,Feedforward neural networks,Modulus of continuity,Multilayer perceptron type networks,Sigmoidal activation function,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {3},
pages = {501--506},
publisher = {Pergamon},
title = {{Kolmogorov's theorem and multilayer neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/0893608092900128},
volume = {5},
year = {1992}
}
@article{PedregosaF.andVaroquauxG.andGramfortA.andMichel2011,
author = {{Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel}, V. and {and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer}, P. and and {and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos}, A. and {Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay}, E.},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@article{Hastie,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - Unknown - Springer Series in Statistics The Elements of Statistical Learning The Elements of Statistical.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
journal = {The Mathematical Intelligencer},
keywords = {inger series in statistics},
number = {2},
pages = {83--85},
pmid = {21196786},
title = {{The Elements of Statistical Learning}},
url = {https://web.stanford.edu/{\%}7B{~}{\%}7Dhastie/Papers/ESLII.pdf http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf{\%}255Cnhttp://www-stat.stanford.edu/{\%}7B{~}{\%}7Dtibs/book/preface.ps},
volume = {27},
year = {2001}
}
@inproceedings{Scholkopf2000,
abstract = {Suppose you are given some dataset drawn from an underlying probabil-ity distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified l/ between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a poten-tially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.},
author = {Scholkopf, Bernhard and Williamson, Robert and Smola, Alex and Shawe-Taylor, John and Platt, John},
booktitle = {Advances in neural information processing systems},
doi = {10.1.1.71.4642},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scholkopf et al. - Unknown - Support Vector Method for Novelty Detection.pdf:pdf},
isbn = {0-262-11245-0},
issn = {0885-6125},
pages = {582--588},
pmid = {20842844},
title = {{Support Vector Method for Novelty Detection}},
url = {https://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf http://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf},
year = {2000}
}
@article{Pascanu,
abstract = {There are two widely known issues with prop-erly training recurrent neural networks, the vanishing and the exploding gradient prob-lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under-standing of the underlying issues by explor-ing these problems from an analytical, a geo-metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef-fective solution. We propose a gradient norm clipping strategy to deal with exploding gra-dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - Unknown - On the difficulty of training recurrent neural networks.pdf:pdf},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://proceedings.mlr.press/v28/pascanu13.pdf}
}
@book{Courville2016,
author = {Courville, Ian Goodfellow and Yoshua Bengio and Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Manuca1996,
abstract = {In this paper we introduce a new class of methods to test, model and describe nonstationary processes. To frame these methods, we generalize the dynamical description of autonomous systems to the case of nonautonomous systems. Of particular interest are systems for which the driving force is recurrent. For these systems we describe a method to find recurrences and to improve the statistics in reconstructing the time series and, consequently, to improve the predictability. Another objective is a proper description of the nonstationarity. All these methods are applied to four examples.},
author = {Manuca, Radu and Savit, Robert},
doi = {10.1016/S0167-2789(96)00139-X},
file = {:home/asgeir/Downloads/1-s2.0-S016727899600139X-main.pdf:pdf},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {2-3},
pages = {134--161},
title = {{Stationarity and nonstationarity in time series analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S016727899600139X},
volume = {99},
year = {1996}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Y and Simard, P and Frasconi, P},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/asgeir/Downloads/00279181.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{RaduManucaRobertSavit,
abstract = {n this paper we introduce a new class of methods to test, model and describe nonstationary processes. To frame these methods, we generalize the dynamical description of autonomous systems to the case of nonautonomous systems. Of particular interest are systems for which the driving force is recurrent. For these systems we describe a method to find recurrences and to improve the statistics in reconstructing the tine series and, consequently, to improve the predictability. Another objective is a proper description of the nonstationarity. All these methods are applied to four examples.},
author = {RaduManucaRobertSavit},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/RaduManucaRobertSavit - Unknown - Stationarity and nonstationarity in time series analysis.pdf:pdf},
title = {{Stationarity and nonstationarity in time series analysis}},
url = {https://ac.els-cdn.com/S016727899600139X/1-s2.0-S016727899600139X-main.pdf?{\_}tid=5a21010e-562c-4f72-8281-997ba365e2a8{\&}acdnat=1525006313{\_}317056420fbd7f42bab41a9791457259}
}
@techreport{Gers,
abstract = {Long short-term memory (LSTM; Hochreiter {\&} Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM net-works processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's in-ternal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive " forget gate " that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustra-tive benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve contin-ual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
author = {Gers, Felix A and Schmidhuber, Urgen and Cummins, Fred},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gers, Schmidhuber, Cummins - Unknown - Learning to Forget Continual Prediction with LSTM.pdf:pdf},
title = {{Learning to Forget: Continual Prediction with LSTM}},
url = {https://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015}
}
@article{Dunning,
author = {Dunning, Ted and Friedman, Ellen},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunning, Friedman - Unknown - Practical Machine Learning A New Look at Anomaly Detection.pdf:pdf},
title = {{Practical Machine Learning: A New Look at Anomaly Detection}},
url = {http://info.mapr.com/rs/mapr/images/Practical{\_}Machine{\_}Learning{\_}Anomaly{\_}Detection.pdf?mkt{\_}tok=eyJpIjoiTW1Nd01qTTROamxtTVRjdyIsInQiOiJianZueHFmQmJqYzZoaWt1T0lNN3JjcTI1dFh3ZTVwa0pUWEFuRU9EUWtKbWlOY1wvSE5IUEFmdk5FbTI4S2Z4Tm1tUTFvaHNQa0JLYzFoMXZmb1h2RDJKaDBYdm}
}
@article{Kim2012,
abstract = {We propose a method for nonparametric density estimation that exhibits robustness to contamina-tion of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to con-verge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.},
author = {Kim, Jooseuk and Scott, Clayton D and Edu, Clayscot@umich},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Scott, Edu - 2012 - Robust Kernel Density Estimation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {M-estimation,influence function,kernel trick,outlier,reproducing kernel Hilbert space},
pages = {2529--2565},
title = {{Robust Kernel Density Estimation}},
url = {http://www.jmlr.org/papers/volume13/kim12b/kim12b.pdf},
volume = {13},
year = {2012}
}
@article{Latecki,
abstract = {Outlier detection has recently become an important prob-lem in many industrial and financial applications. In this paper, a novel unsupervised algorithm for outlier detection with a solid statistical foun-dation is proposed. First we modify a nonparametric density estimate with a variable kernel to yield a robust local density estimation. Out-liers are then detected by comparing the local density of each point to the local density of its neighbors. Our experiments performed on sev-eral simulated data sets have demonstrated that the proposed approach can outperform two widely used outlier detection algorithms (LOF and LOCI).},
author = {Latecki, Longin Jan and Lazarevic, Aleksandar and Pokrajac, Dragoljub},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Latecki, Lazarevic, Pokrajac - Unknown - Outlier Detection with Kernel Density Functions.pdf:pdf},
title = {{Outlier Detection with Kernel Density Functions}},
url = {https://cis.temple.edu/{~}latecki/Papers/mldm07.pdf}
}
@article{Guthrie,
abstract = {This paper describes work on the detection of anomalous material in text. We show several vari-ants of an automatic technique for identifying an 'unusual' segment within a document, and consider texts which are unusual because of author, genre [Biber, 1998], topic or emotional tone. We evaluate the technique using many experiments over large document collections, created to contain randomly inserted anomalous segments. In order to success-fully identify anomalies in text, we define more than 200 stylistic features to characterize writing, some of which are well-established stylistic deter-miners, but many of which are novel. Using these features with each of our methods, we examine the effect of segment size on our ability to detect anomaly, allowing segments of size 100 words, 500 words and 1000 words. We show substantial improvements over a baseline in all cases for all methods, and identify the method variant which performs consistently better than others.},
author = {Guthrie, David and Guthrie, Louise and Allison, Ben and Wilks, Yorick},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guthrie et al. - Unknown - Unsupervised Anomaly Detection.pdf:pdf},
keywords = {natural language processing},
title = {{Unsupervised Anomaly Detection}},
url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-262.pdf}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, Jj},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - LONG SHORT-TERM MEMORY.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{LONG SHORT-TERM MEMORY}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}5Cnhttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Malhotra,
abstract = {Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM net-works for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behav-ior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
author = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra et al. - Unknown - Long Short Term Memory Networks for Anomaly Detection in Time Series(4).pdf:pdf},
title = {{Long Short Term Memory Networks for Anomaly Detection in Time Series}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf}
}
@inproceedings{Juba2015,
abstract = {—Anomaly detection plays an important role in pro-tecting computer systems from unforeseen attack by automati-cally recognizing and filter atypical inputs. However, it can be difficult to balance the sensitivity of a detector – an aggressive system can filter too many benign inputs while a conservative system can fail to catch anomalies. Accordingly, it is important to rigorously test anomaly detectors to evaluate potential error rates before deployment. However, principled systems for doing so have not been studied – testing is typically ad hoc, making it difficult to reproduce results or formally compare detectors. To address this issue we present a technique and implemented system, Fortuna, for obtaining probabilistic bounds on false positive rates for anomaly detectors that process Internet data. Using a probability distribution based on PageRank and an efficient algorithm to draw samples from the distribution, Fortuna computes an estimated false positive rate and a probabilistic bound on the estimate's accuracy. By drawing test samples from a well defined distribution that correlates well with data seen in practice, Fortuna improves on ad hoc methods for estimating false positive rate, giving bounds that are reproducible, comparable across different anomaly detectors, and theoretically sound. Experimental evaluations of three anomaly detectors (SIFT, SOAP, and JSAND) show that Fortuna is efficient enough to use in practice — it can sample enough inputs to obtain tight false positive rate bounds in less than 10 hours for all three detectors. These results indicate that Fortuna can, in practice, help place anomaly detection on a stronger theoretical foundation and help practitioners better understand the behavior and consequences of the anomaly detectors that they deploy. As part of our work, we obtain a theoretical result that may be of independent interest: We give a simple analysis of the convergence rate of the random surfer process defining PageRank that guarantees the same rate as the standard, second-eigenvalue analysis, but does not rely on any assumptions about the link structure of the web.},
archivePrefix = {arXiv},
arxivId = {1612.06676},
author = {Juba, Brendan and Musco, Christopher and Long, Fan and Sidiroglou-Douskos, Stelios and Rinard, Martin},
booktitle = {Proceedings 2015 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2015.23268},
eprint = {1612.06676},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra et al. - Unknown - Long Short Term Memory Networks for Anomaly Detection in Time Series(4).pdf:pdf},
isbn = {1-891562-38-X},
issn = {16130073},
title = {{Principled Sampling for Anomaly Detection}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf http://www.internetsociety.org/doc/principled-sampling-anomaly-detection},
year = {2015}
}
@article{Malhotra2016,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
archivePrefix = {arXiv},
arxivId = {1607.00148},
author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
eprint = {1607.00148},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra, Com - Unknown - LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection(2).pdf:pdf},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {https://arxiv.org/pdf/1607.00148.pdf http://arxiv.org/abs/1607.00148},
year = {2016}
}
@article{Hautam,
author = {Hautam, Ville and Ismo, K},
file = {:home/asgeir/Downloads/01334558.pdf:pdf},
pages = {4--7},
title = {{Outlier Detection Using k-Nearest Neighbour Graph ¥ Q P ¥ ¥ R {\$} S ¨ UTWVYX ' EDGFIH ¢¥ P {\$}}}
}
@article{Malhotraa,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are of-ten external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, man-ual controls and/or unmonitored environmental conditions or load may lead to inherently un-predictable time-series. Detecting anomalies in such scenarios becomes challenging using stan-dard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to recon-struct 'normal' time-series behavior, and there-after uses reconstruction error to detect anoma-lies. We experiment with three publicly available quasi predictable time-series datasets: power de-mand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from pre-dictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
author = {Malhotra, Pankaj and Com, Tam {Shroff{\}}@tcs},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra, Com - Unknown - LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection(2).pdf:pdf},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {https://arxiv.org/pdf/1607.00148.pdf}
}
@article{Tarassenko2009,
author = {Tarassenko, Lionel and Clifton, David A and Bannister, Peter R and King, Steve and King, Dennis},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tarassenko et al. - Unknown - Chapter 35 Novelty Detection.pdf:pdf},
isbn = {9780470058220},
journal = {Encyclopaedia of Structural Health Monitoring},
pages = {653--675},
title = {{Novelty Detection}},
url = {https://pdfs.semanticscholar.org/a65d/7c7137968e291f0732b12110e485276cbe11.pdf},
year = {2009}
}
@inproceedings{Hautamaki2004,
abstract = {We present an outlier detection using indegree number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance-based method are also proposed. We compare the methods with real and synthetic datasets. The results show that the proposed method achieves reasonable results with synthetic data and outperforms compared methods with real data sets with small number of observations.},
author = {Hautam{\"{a}}ki, Ville and K{\"{a}}rkk{\"{a}}inen, Ismo and Fr{\"{a}}nti, Pasi},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1334558},
isbn = {0769521282},
issn = {10514651},
pages = {430--433},
publisher = {IEEE},
title = {{Outlier detection using k-nearest neighbour graph}},
url = {http://ieeexplore.ieee.org/document/1334558/},
volume = {3},
year = {2004}
}
@article{Hodge,
abstract = {Outlier detection has been used for centuries to detect and, where appropri-ate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
author = {Hodge, Victoria J and Austin, Jim},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodge, Austin - Unknown - A Survey of Outlier Detection Methodologies.pdf:pdf},
keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
title = {{A Survey of Outlier Detection Methodologies}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AAIRE.0000045502.10941.a9.pdf}
}
@article{Mckinney2010,
abstract = {—In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {Mckinney, Wes},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mckinney - 2010 - Data Structures for Statistical Computing in Python.pdf:pdf},
journal = {PROC. OF THE 9th PYTHON IN SCIENCE CONF},
keywords = {Index Terms—data structure,R,statistics},
title = {{Data Structures for Statistical Computing in Python}},
url = {https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf},
year = {2010}
}
@misc{Statkraft2009,
abstract = {Vannkraft er en milj{\o}vennlig og fornybar energikilde. 99 prosent av all kraftproduksjon i Norge kommer fra vannkraft. P{\aa} verdensbasis utgj{\o}r vannkraften rundt en sjettedel av den totale kraftproduksjonen},
author = {Statkraft},
title = {{Vannkraft}},
url = {https://www.statkraft.no/globalassets/old-contains-the-old-folder-structure/documents/no/vannkraft-09-no{\_}tcm10-4585.pdf},
urldate = {2018-03-20},
year = {2009}
}
@article{Paish2002,
abstract = {Hydropower, large and small, remains by far the most important of the «renewables» for electrical power production worldwide, providing 19{\%} of the planet's electricity. Small-scale hydro is in most cases «run-of-river», with no dam or water storage, and is one of the most cost-effective and environmentally benign energy technologies to be considered both for rural electrification in less developed countries and further hydro developments in Europe. The European Commission have a target to increase small hydro capacity by 4500MW (50{\%}) by the year 2010. The UK has 100MW of existing small hydro capacity (under 5MW) operating from approximately 120 sites, and at least 400MW of unexploited potential. With positive environmental policies now being backed by favourable tariffs for 'green' electricity, the industry believes that small hydro will have a strong resurgence in Europe in the next 10 years, after 20 years of decline. This paper summarises the different small hydro technologies, new innovations being developed, and the barriers to further development. {\textcopyright} 2002 Elsevier Science Ltd.},
author = {Paish, Oliver},
doi = {10.1016/S1364-0321(02)00006-0},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paish - 2002 - Small hydro power technology and current status.pdf:pdf},
isbn = {1364-0321},
issn = {13640321},
journal = {Renewable and Sustainable Energy Reviews},
keywords = {Hydropower,Micro-hydro,Mini-hydro,Small hydro,Water power},
number = {6},
pages = {537--556},
title = {{Small hydro power: Technology and current status}},
url = {www.elsevier.com/locate/rser},
volume = {6},
year = {2002}
}
@article{Schslkopf,
abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
author = {Schslkopf, Bernhard and Smola, Alexander and Mfiller, Klaus-Robert},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schslkopf, Smola, Mfiller - Unknown - Kernel Principal Component Analysis.pdf:pdf},
title = {{Kernel Principal Component Analysis}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBFb0020217.pdf}
}
@article{Belkin2001,
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1.1.19.9400},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belkin, Niyogi - Unknown - Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering(2).pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Nips},
pages = {585--591},
pmid = {25246403},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering}},
url = {http://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.9400{\&}rep=rep1{\&}type=pdf},
volume = {14},
year = {2001}
}
@article{Li,
abstract = {This paper studies one application of mutual information to symbolic sequen-ces: the mutual information function M(d). This function is compared with the more frequently used correlation function F(d). An exact relation between M(d) and F(d) is derived for binary sequences. For sequences with more than two symbols, no such general relation exists; in particular, F(d) = 0 may or may not lead to M(d)= 0. This linear, but not general, independence between symbols separated by a distance is studied for ternary sequences. Also included is the estimation of the finite-size effect on calculating mutual information. Finally, the concept of "symbolic noise" is discussed.},
author = {Li, Wentian},
doi = {10.1007/BF01025996},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - Unknown - Mutual Information Functions versus Correlation Functions.pdf:pdf},
issn = {0022-4715},
journal = {J. Stat. Phys.},
keywords = {correlation functions,linear and general dependence,mutual information function,symbolic noise},
number = {5},
pages = {823--837},
title = {{Mutual information versus correlation functions}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF01025996.pdf papers2://publication/uuid/805C6936-1BA8-4632-B6C6-5BB2CDB979C3},
volume = {60},
year = {1990}
}
@inproceedings{Khalid2014,
author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
booktitle = {2014 Science and Information Conference},
doi = {10.1109/SAI.2014.6918213},
file = {:home/asgeir/Downloads/06918213.pdf:pdf},
isbn = {978-0-9893193-1-7},
month = {aug},
pages = {372--378},
publisher = {IEEE},
title = {{A survey of feature selection and feature extraction techniques in machine learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6918213},
year = {2014}
}
@article{Chandrashekar2014,
abstract = {a b s t r a c t Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction perfor-mance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in lit-erature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods.pdf:pdf},
journal = {Computers and Electrical Engineering},
pages = {16--28},
title = {{A survey on feature selection methods}},
url = {https://ac.els-cdn.com/S0045790613003066/1-s2.0-S0045790613003066-main.pdf?{\_}tid=c336f5ca-2494-422d-9db8-22b90d47458a{\&}acdnat=1520844383{\_}5faa66e2252a7c49c3d0c76ad66df5de},
volume = {40},
year = {2014}
}
@article{Kraskov2004,
abstract = {We present two classes of improved estimators for mutual information {\$}M(X,Y){\$}, from samples of random points distributed according to some joint probability density {\$}\backslashmu(x,y){\$}. In contrast to conventional estimators based on binnings, they are based on entropy estimates from {\$}k{\$}-nearest neighbour distances. This means that they are data efficient (with {\$}k=1{\$} we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of {\$}k/N{\$} for {\$}N{\$} points. Numerically, we find that both families become {\{}$\backslash$it exact{\}} for independent distributions, i.e. the estimator {\$}\backslashhat M(X,Y){\$} vanishes (up to statistical fluctuations) if {\$}\backslashmu(x,y) = \backslashmu(x) \backslashmu(y){\$}. This holds for all tested marginal distributions and for all dimensions of {\$}x{\$} and {\$}y{\$}. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
annote = {Scikit-learn reference!},
archivePrefix = {arXiv},
arxivId = {cond-mat/0305641},
author = {Kraskov, Alexander and St{\"{o}}gbauer, Harald and Grassberger, Peter},
doi = {10.1103/PhysRevE.69.066138},
eprint = {0305641},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraskov, St{\"{o}}gbauer, Grassberger - Unknown - Estimating mutual information.pdf:pdf},
isbn = {1539-3755 (Print)},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {6},
pages = {16},
pmid = {15244698},
primaryClass = {cond-mat},
title = {{Estimating mutual information}},
url = {https://journals.aps.org/pre/pdf/10.1103/PhysRevE.69.066138},
volume = {69},
year = {2004}
}
@article{Peng2005,
abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
archivePrefix = {arXiv},
arxivId = {f},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
eprint = {f},
file = {:home/asgeir/Downloads/01453511.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy}},
volume = {27},
year = {2005}
}
@article{Peng2005a,
abstract = {Feature selection is an important problem for pattern classification systems.Westudy how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combiningmRMRand other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost.Weperform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
archivePrefix = {arXiv},
arxivId = {f},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
eprint = {f},
file = {:home/asgeir/Downloads/01453511.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
month = {aug},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy}},
url = {http://ieeexplore.ieee.org/document/1453511/},
volume = {27},
year = {2005}
}
@article{Agarwal2012,
abstract = {We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation {\$}\backslashmathfrak{\{}X{\}}{\$} of the sum of an approximately) low rank matrix {\$}\backslashTheta{\^{}}\backslashstar{\$} with a second matrix {\$}\backslashGamma{\^{}}\backslashstar{\$} endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including factor analysis, multi-task regression, and robust covariance estimation. We derive a general theorem that bounds the Frobenius norm error for an estimate of the pair {\$}(\backslashTheta{\^{}}\backslashstar, \backslashGamma{\^{}}\backslashstar){\$} obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results utilize a "spikiness" condition that is related to but milder than singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices {\$}\backslashTheta{\^{}}\backslashstar{\$} that can be exactly or approximately low rank, and matrices {\$}\backslashGamma{\^{}}\backslashstar{\$} that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error. The sharpness of our predictions is confirmed by numerical simulations.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J.},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {978-1-4503-0619-5},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Composite regularizers,High-dimensional inference,Nuclear norm},
number = {2},
pages = {1171--1197},
pmid = {23285570},
title = {{Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions}},
volume = {40},
year = {2012}
}
@misc{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
isbn = {0893-6080},
issn = {18792782},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Ding2014,
abstract = {Novelty detection is especially important for monitoring safety-critical systems in which novel conditions rarely occur and knowledge about novelty in that system is often limited or unavailable. There are a large number of studies in the area of novelty detection, but there is a lack of a comprehensive experimental evaluation of existing novelty detection methods. This paper aims to fill this void by conducting experimental evaluation of representative novelty detection methods. It presents a state-of-the-art review of novelty detection, with a focus on methods reported in the last few years. In addition, a rigorous comparative evaluation of four widely used methods, representative of different categories of novelty detectors, is carried out using 10 benchmark datasets with different scale, dimensionality and problem complexity. The experimental results demonstrate that the k-NN novelty detection method exhibits competitive overall performance to the other methods in terms of the AUC metric. {\textcopyright} 2013 Elsevier B.V.},
author = {Ding, Xuemei and Li, Yuhua and Belatreche, Ammar and Maguire, Liam P.},
doi = {10.1016/j.neucom.2013.12.002},
issn = {18728286},
journal = {Neurocomputing},
title = {{An experimental evaluation of novelty detection methods}},
volume = {135},
year = {2014}
}
@article{Zhu2014,
abstract = {Currently, the wind energy industry is swiftly changing its maintenance strategy from schedule based maintenance to predictive based maintenance. Condition monitoring systems (CMS) play an important role in the predictive maintenance cycle. As condition monitoring systems are being adopted by more and more OEM and O{\&}M service providers from the wind energy industry, it is crucial to effectively interpret the data generated by the CMS and initiate proactive processes to efficiently reduce the risk of potential component or system failure which often leads to down tower repair or gearbox replacement. The majority of CMS are designed and constructed based on vibration analysis which has been refined over the years by researchers and scientists. This paper provides detailed description and mathematical interpretation of a comprehensive selection of condition indicators for gears, bearings and shafts. Since different condition indicators are sensitive to different kind of failure modes, the application for each condition indicators were also discussed. The Time Synchronous Averaging (TSA) algorithm was applied as the signal processing method before the extraction of condition indicators for gears and shafts. Time Synchronous Resampling algorithm was applied to stabilize the shaft speed before the extraction of bearing condition indicators. Several case studies of real world wind turbine component failure detection using condition indicators were presented to demonstrate the effectiveness of certain condition indicators.},
author = {Zhu, Junda and Nostrand, Tom and Spiegel, Cody and Morton, Brogan},
isbn = {9781936263172},
journal = {Annual Conference of the Prognostics and Health Management Society},
title = {{Survey of Condition Indicators for Condition Monitoring Systems}},
volume = {5},
year = {2014}
}
@article{Omar2013,
abstract = {Intrusion detection has gain a broad attention and become a fertile field for several researches, and still being the subject of widespread interest by researchers. The intrusion detection community still confronts difficult problems even after many years of research. Reducing the large number of false alerts during the process of detecting unknown attack patterns remains unresolved problem. However, several research results recently have shown that there are potential solutions to this problem. Anomaly detection is a key issue of intrusion detection in which perturbations of normal behavior indicates a presence of intended or unintended induced attacks, faults, defects and others. This paper presents an overview of research directions for applying supervised and unsupervised methods for managing the problem of anomaly detection. The references cited will cover the major theoretical issues, guiding the researcher in interesting research directions.},
annote = {Oppsummering av mange ulike teknikar for anomaly detection, kan brukast til sitering!},
author = {Omar, Salima and Ngadi, Asri and Jebur, Hamid H},
doi = {10.5120/13715-1478},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Omar, Ngadi, Jebur - 2013 - Machine Learning Techniques for Anomaly Detection An Overview.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {Network Intrusion Detection,Supervised Machine Learning,Unsupervised Machine Learning},
number = {2},
pages = {975--8887},
title = {{Machine Learning Techniques for Anomaly Detection: An Overview}},
url = {https://pdfs.semanticscholar.org/0278/bbaf1db5df036f02393679d485260b1daeb7.pdf},
volume = {79},
year = {2013}
}
@article{He2005,
abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning sce- narios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of pre- vious unsupervised feature selection methods are wrapper techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a ﬁlter method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demon- strate the effectiveness and efﬁciency of our algorithm.},
author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
doi = {http://books.nips.cc/papers/files/nips18/NIPS2005_0149.pdf},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Cai, Niyogi - Unknown - Laplacian Score for Feature Selection.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {507--514},
title = {{Laplacian Score for Feature Selection}},
url = {http://papers.nips.cc/paper/2909-laplacian-score-for-feature-selection.pdf},
year = {2005}
}
@article{Liu2010,
abstract = {The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, commu- nications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining appli- cations. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We first briefly introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vi- brant research field of some contemporary interests, new applications, and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.},
author = {Liu, Huan (National University of Singapore) and Motoda, Hiroshi (Osaka University) and Setiono, Rudy and Zhao, Zheng},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - Feature Selection An Ever Evolving Frontier in Data Mining.pdf:pdf},
issn = {1541-1672},
journal = {Journal of Machine Learning Research: Workshop and Conference Proceedings 10: The Fourth Workshop on Feature Selection in Data Mining},
keywords = {data mining,dimension reduction,feature extraction,feature selection},
pages = {4--13},
title = {{Feature Selection : An Ever Evolving Frontier in Data Mining}},
url = {http://proceedings.mlr.press/v10/liu10b/liu10b.pdf},
volume = {10},
year = {2010}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Hall1999,
author = {Hall, Mark A},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall - 1999 - Correlation-based Feature Selection for Machine Learning.pdf:pdf},
title = {{Correlation-based Feature Selection for Machine Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.4521{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@article{Sheather1991,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
author = {Sheather, S J and Jones, M C and Jonest, M C},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sheather, Jones, Jonest - 1991 - A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation.pdf:pdf},
journal = {Source Journal of the Royal Statistical Society. Series B (Methodological) Journal of the Royal Statistical Society. Series B J. R. Statist. Soc. B},
keywords = {ADAPTIVE CHOICE,BIAS REDUCTION,FUNCTIONAL ESTIMATION,SMOOTHING,SQUARED ERROR LOSS FUNCTIONS},
number = {3},
pages = {683--690},
title = {{A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation}},
url = {http://www.jstor.org/stable/2345597 http://about.jstor.org/terms},
volume = {53},
year = {1991}
}
@article{Guyon2003a,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre-dictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}} and De, Andre@tuebingen Mpg},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {QSAR,Variable selection,bioinformatics,clustering,computational biology,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-ery,proteomics,space dimensionality reduction,statistical testing,support vector machines,text classification,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Dy2004,
abstract = {In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.},
author = {Dy, Jennifer G and Brodley, Carla E},
doi = {10.1016/j.patrec.2014.11.006},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dy, Brodley BRODLEY - 2004 - Feature Selection for Unsupervised Learning.pdf:pdf},
isbn = {978-3-642-34487-9},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature Selection for Unsupervised Learning}},
url = {http://www.jmlr.org/papers/volume5/dy04a/dy04a.pdf},
volume = {5},
year = {2004}
}
@misc{Pimentel2014,
abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as "one-class classification", in which a model is constructed to describe "normal" training data. The novelty detection approach is typically used when the quantity of available "abnormal" data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that "normality" may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade. {\textcopyright} 2014 Published by Elsevier B.V.},
author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
booktitle = {Signal Processing},
doi = {10.1016/j.sigpro.2013.12.026},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pimentel et al. - 2014 - A review of novelty detection(2).pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
keywords = {Machine learning,Novelty detection,One-class classification},
month = {jun},
pages = {215--249},
publisher = {Elsevier},
title = {{A review of novelty detection}},
url = {https://www.sciencedirect.com/science/article/pii/S016516841300515X?via{\%}3Dihub},
volume = {99},
year = {2014}
}
@article{Clifton2014,
abstract = {—Novelty detection, or one-class classification, is of particular use in the analysis of high-integrity systems, in which examples of failure are rare in comparison with the number of examples of stable behaviour, such that a conventional multi-class classification approach cannot be taken. Support Vector Machines (SVMs) are a popular means of performing novelty detection, and it is conventional practice to use a train-validate-test approach, often involving cross-validation, to train the one-class SVM, and then select appropriate values for its parameters. An alternative method, used with multi-class SVMs, is to calibrate the SVM output into conditional class probabilities. A probabilistic ap-proach offers many advantages over the conventional method, including the facility to select automatically a probabilistic novelty threshold. The contributions of this paper are (i) the development of a probabilistic calibration technique for one-class SVMs, such that on-line novelty detection may be performed in a probabilistic manner; and (ii) the demonstration of the advantages of the pro-posed method (in comparison to the conventional one-class SVM methodology) using case studies, in which one-class probabilistic SVMs are used to perform condition monitoring of a high-integrity industrial combustion plant, and in detecting deterioration in pa-tient physiological condition during patient vital-sign monitoring.},
annote = {Many relevant references?
Talking about constructing anomaly data, this is relevant for the project!

Might be able to use plant knowledge to analyze the artificial unstable data? 25,26},
author = {Clifton, Lei and Clifton, David A. and Zhang, Yang and Watkinson, Peter and Tarassenko, Lionel and Yin, Hujun},
doi = {10.1109/TR.2014.2315911},
file = {:home/asgeir/Downloads/06786486.pdf:pdf},
isbn = {0018-9529 VO  - 63},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
keywords = {Support vector machine,calibration,condition monitoring,novelty detection,one-class classification},
number = {2},
pages = {455--467},
title = {{Probabilistic novelty detection with support vector machines}},
volume = {63},
year = {2014}
}
@article{Cheng,
abstract = {Anomaly detection in multivariate time series is an important data mining task with applications to ecosystem modeling, network traffic monitoring, medical diagnosis, and other domains. This paper presents a robust algorithm for detecting anomalies in noisy multivariate time series data by employing a kernel matrix alignment method to capture the dependence relationships among variables in the time series. Anomalies are found by performing a random walk traversal on the graph induced by the aligned kernel matrix. We show that the algorithm is flexible enough to handle different types of time series anomalies including subsequence-based and local anomalies. Our framework can also be used to characterize the anomalies found in a target time series in terms of the anomalies present in other time series. We have performed extensive experiments to empirically demonstrate the effectiveness of our algorithm. A case study is also presented to illustrate the ability of the algorithm to detect ecosystem disturbances in Earth science data. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972795.36},
author = {Cheng, Haibin and Tan, Pang-Ning and Potter, Christopher and Klooster, Steven},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - Unknown - Detection and Characterization of Anomalies in Multivariate Time Series.pdf:pdf},
journal = {SDM},
keywords = {anomaly detection,graph representation,kernel alignment,kernel function,multivariate time series},
pages = {413--424},
title = {{Detection and Characterization of Anomalies in Multivariate Time Series}},
url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.36},
volume = {9},
year = {2009}
}
@inproceedings{Kozma,
abstract = {The problem of detecting weak aiionialies in temporal signals is addressed. The performance of statistical methods utilizing the evaluation of the intensity of time-dependent fluctuations is compared with the results obtained by a layered artificial neural network model. The desired accuracy of tlie approximation by the neural network at the end of the learning phase has been estimated by analyzing the statistics of the learning data. The applicatioii of the obtained results to tlie analysis of actual anomaly data from a nuclear reactor showed that neural networks can ident,ify the onset of anomalies with a reasonable success, while usual statistical methods were unable to make distinction between iiorinal and abnormal patterns.},
author = {Kozma, R and Kitamura, M and Sakuma, M and Yokoyama, Y},
booktitle = {IEEE International Conference on Neural Networks},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kozma et al. - Unknown - ANOMALY DETECTION BY NEURAL NETWORK MODELS AND STATISTICAL TIME SERIES ANALYSIS.pdf:pdf},
isbn = {078031901X},
pages = {3207--3210},
title = {{Anomaly Detection By Neural Network Models and Statistical Time Series Analysis}},
url = {http://www.memphis.edu/clion/pdf-publications/ijcnn94.pdf},
volume = {5},
year = {1994}
}
@article{Gardner2006,
abstract = {This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a " one-shot " manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1{\%} sensitivity, a mean detection latency of GARDNER, KRIEGER, VACHTSEVANOS AND LITT 1026 -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training.},
author = {Gardner, Andrew B and Krieger, Abba M and Vachtsevanos, George and Litt, Brian and {Gardner AGARDNER}, Andrew B},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2006 - One-Class Novelty Detection for Seizure Analysis from Intracranial EEG.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {epilepsy,novelty detection,one-class SVM,seizure detection,unsupervised learning},
pages = {1025--1044},
title = {{One-Class Novelty Detection for Seizure Analysis from Intracranial EEG}},
url = {http://www.jmlr.org/papers/volume7/gardner06a/gardner06a.pdf},
volume = {7},
year = {2006}
}
@article{Ma,
abstract = {Time-series novelty detection, or anomaly detection, refers to the automatic identification of novel or abnormal events embedded in normal time-series points. Although it is a challenging topic in data mining, it has been acquiring increasing attention due to its huge potential for immediate applications. In this paper, a new algorithm for time-series novelty detection based on one-class support vector machines (SVMs) is proposed. The concepts of phase and projected phase spaces are first introduced, which allows us to convert a time-series into a set of vectors in the (projected) phase spaces. Then we interpret novel events in time-series as outliers of the "normal" distribution of the converted vectors in the (projected) phase spaces. One-class SVMs are employed as the outlier detectors. In order to obtain robust detection results, a technique to combine intermediate results at different phase spaces is also proposed. Experiments on both synthetic and measured data are presented to demonstrate the promising performance of the new algorithm.},
annote = {Good shit keep this},
author = {Ma, J. and Perkins, S.},
doi = {10.1109/IJCNN.2003.1223670},
file = {:home/asgeir/Downloads/timeseries novelty detection using SVM.pdf:pdf},
isbn = {0-7803-7898-9},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
pages = {1741--1745},
pmid = {7898689},
publisher = {IEEE},
title = {{Time-series novelty detection using one-class support vector machines}},
url = {http://ieeexplore.ieee.org/document/1223670/},
volume = {3},
year = {2003}
}
@article{Chan,
abstract = {Our goal is to generate comprehensible and accurate models from multiple time series for anomaly detection. The models need to produce anomaly scores in an online manner for real-life monitoring tasks. We introduce three algorithms that work in a constructed feature space and evaluate them with a real data set from the NASA shuttle program. Our offline and online evaluations indicate that our algorithms can be more accurate than two existing algorithms.},
author = {Chan, P K and Mahoney, M V},
doi = {10.1109/ICDM.2005.101},
file = {:home/asgeir/Downloads/01565666.pdf:pdf},
isbn = {0769522785},
issn = {15504786},
journal = {Fifth IEEE International Conference on Data Mining ICDM05},
pages = {90--97},
publisher = {IEEE},
title = {{Modeling multiple time series for anomaly detection}},
url = {http://ieeexplore.ieee.org/document/1565666/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565666},
year = {2005}
}
@article{,
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Time series forecasting based on wavelet filtering.pdf:pdf},
title = {{Time series forecasting based on wavelet filtering}},
url = {https://ac.els-cdn.com/S095741741500041X/1-s2.0-S095741741500041X-main.pdf?{\_}tid=429e3e38-0fe6-11e8-894a-00000aab0f01{\&}acdnat=1518434407{\_}b266f00e681a122b5635f4499e308ffc}
}
@article{Nanihar,
author = {Nanihar, Nadiarulah and Khalid, Amir and Mustaffa, Norrizal},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nanihar, Khalid, Mustaffa - Unknown - Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Rel.pdf:pdf},
journal = {Mater. Sci. Eng},
title = {{Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Related content Experiment on the Effects of Storage Duration of Biodiesel produced from Crude Palm Oil, Waste Cooking oil and Jatropha}},
url = {http://iopscience.iop.org/article/10.1088/1757-899X/226/1/012117/pdf},
volume = {226}
}
@article{Chakraborty1992,
abstract = {This paper presents a neural network approach to multivariate time-series analysis. Real world observations of flour prices in three cities have been used as a benchmark in our experiments. Feedforward connectionist networks have been designed to model flour prices over the period from August 1972 to November 1980 for the cities of Buffalo, Minneapolis, and Kansas City. Remarkable success has been achieved in training the networks to learn the price curve for each of these cities and in making accurate price predictions. Our results show that the neural network approach is a leading contender with the statistical modeling approaches.},
author = {Chakraborty, Kanad and Mehrotra, Kishan and Mohan, Chilukuri K. and Ranka, Sanjay},
doi = {10.1016/S0893-6080(05)80092-9},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chakraborty et al. - 1992 - Forecasting the behavior of multivariate time series using neural networks.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
month = {nov},
number = {6},
pages = {961--970},
publisher = {Pergamon},
title = {{Forecasting the behavior of multivariate time series using neural networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800929},
volume = {5},
year = {1992}
}
@book{Kim2017,
author = {Kim, Nam-Ho and Joo-Jo, Choi and An, Dawn},
doi = {10.1007/978-3-319-44742-1},
file = {:home/asgeir/Downloads/PHM.pdf:pdf},
isbn = {9780470278024},
pages = {336},
title = {{Prognostics and Health Management of Electronics}},
year = {2017}
}
@article{Si2011,
abstract = {Remaining useful life (RUL) is the useful life left on an asset at a particular time of operation. Its estimation is central to condition based maintenance and prognostics and health management. RUL is typically random and unknown, and as such it must be estimated from available sources of information such as the information obtained in condition and health monitoring. The research on how to best estimate the RUL has gained popularity recently due to the rapid advances in condition and health monitoring techniques. However, due to its complicated relationship with observable health information, there is no such best approach which can be used universally to achieve the best estimate. As such this paper reviews the recent modeling developments for estimating the RUL. The review is centred on statistical data driven approaches which rely only on available past observed data and statistical models. The approaches are classified into two broad types of models, that is, models that rely on directly observed state information of the asset, and those do not. We systematically review the models and approaches reported in the literature and finally highlight future research challenges.},
author = {Si, Xiao-Sheng and Wang, Wenbin and Hu, Chang-Hua and Zhou, Dong-Hua},
doi = {10.1016/J.EJOR.2010.11.018},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Si et al. - 2011 - Remaining useful life estimation – A review on the statistical data driven approaches.pdf:pdf},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {aug},
number = {1},
pages = {1--14},
publisher = {North-Holland},
title = {{Remaining useful life estimation – A review on the statistical data driven approaches}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221710007903},
volume = {213},
year = {2011}
}
@article{Stenman2003,
abstract = {A method for detecting static friction (stiction) in$\backslash$ncontrol valves is proposed. The method is model-based$\backslash$nand is inspired by ideas from the fields of change$\backslash$ndetection and multi-model mode estimation. Opposed to$\backslash$nexisting methods only limited process knowledge is$\backslash$nneeded and it is not required that the loop has$\backslash$noscillating behaviour. The advantage of the method is$\backslash$nillustrated in both numerical simulations and$\backslash$nevaluations on real loop data. Copyright {\textcopyright} 2003 John$\backslash$nWiley {\&} Sons, Ltd.},
author = {Stenman, A. and Gustafsson, F. and Forsman, K.},
doi = {10.1002/acs.769},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stenman, Gustafsson, Forsman - 2003 - A segmentation-based method for detection of stiction in control valves(2).pdf:pdf},
issn = {0890-6327},
journal = {International Journal of Adaptive Control and Signal Processing},
keywords = {friction,multi‐model mode estimation,stiction,valve diagnosis},
month = {sep},
number = {7-9},
pages = {625--634},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{A segmentation-based method for detection of stiction in control valves}},
url = {http://doi.wiley.com/10.1002/acs.769},
volume = {17},
year = {2003}
}
@article{Dau2012,
abstract = {Theory of Mind (ToM) is the ability to read another person's mind. To apply ToM in robots, robot should read the intention from target. However, it is difficult to read target's intention directly. Robot uses the sensors to measure distance from target because distance is the feature to read target's intention. Neural network has been widely used to control the robot for generating a diverse speciation. It has been less explored in behavior-based robotics. Speciation usually relies on a distance measure that allows different from the robot to target to be compared. In this paper, we proposed novel measure to generate diverse behaviors of a robot with speciation for ToM. It includes some distance measure such as Euclidean distance, cosine distance, arctangent distance, and edit distance. It generates diverse behaviors of the robot by neural network for ToM. The proposed method has been experimented on a real e-puck robot platform. {\textcopyright} 2012 Springer-Verlag.},
author = {Dau, Hoang Anh and Ciesielski, Vic and Song, Andy},
doi = {10.1007/978-3-642-34859-4},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dau, Ciesielski, Song - 2012 - Simulated Evolution and Learning.pdf:pdf},
isbn = {978-3-642-34858-7},
issn = {03029743},
number = {December},
title = {{Simulated Evolution and Learning}},
url = {http://link.springer.com/10.1007/978-3-642-34859-4},
volume = {7673},
year = {2012}
}
@article{Choudhury2006,
abstract = {Stiction is a common problem in spring-diaphragm type valves, which are widely used in the process industry. Although there have been many attempts to understand and detect stiction in control valves, none of the current methods can simultaneously detect and quantify stiction. Conventional invasive methods such as the valve travel test can easily detect stiction, but are expensive and tedious to apply to hundreds of valves to detect stiction. Thus there is a clear need in the process industry for a non-invasive method that can not only detect but also quantify stiction so that the valves that need repair or maintenance can be identified, isolated and repaired. This work describes a model free method that can detect and quantify stiction that may be present in control valves using routine operating data obtained from the process. No additional excitation or experimentation of the plant is required. Over a dozen industrial case studies have demonstrated the wide applicability and practicality of this method as an useful diagnostic aid in control loop performance monitoring. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Choudhury, M. A A Shoukat and Shah, S. L. and Thornhill, N. F. and Shook, David S.},
doi = {10.1016/j.conengprac.2005.10.003},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury et al. - 2006 - Automatic detection and quantification of stiction in control valves.pdf:pdf},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Actuator,Control valve,Deadband,Hysteresis,Stickband,Stiction},
month = {dec},
number = {12},
pages = {1395--1412},
publisher = {Pergamon},
title = {{Automatic detection and quantification of stiction in control valves}},
url = {http://www.sciencedirect.com/science/article/pii/S0967066105002388},
volume = {14},
year = {2006}
}
@article{Sakurada2014,
abstract = {This paper proposes to use autoencoders with nonlinear di-mensionality reduction in the anomaly detection task. The authors apply dimensionality reduction by using an autoen-coder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property. The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data. This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails. Also, autoencoders can increase their accuracy by extending them to denoising au-toenconders. Moreover, autoencoders can be useful as non-linear techniques without complex computation as kernel PCA requires. Finaly, the authors examine the learned fea-tures in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.},
author = {Sakurada, Mayu and Yairi, Takehisa},
doi = {10.1145/2689746.2689747},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakurada, Yairi - Unknown - Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction.pdf:pdf},
isbn = {9781450331593},
journal = {Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis},
keywords = {I54 [Pattern recognition],au-toencoder,auto-assosiative neural network,denoising au-toencoder,dimensionality reduction,fault detection,nonlinear,novelty detection,spacecrafts},
pages = {4},
title = {{Anomaly detection using autoencoders with nonlinear dimensionality reduction}},
url = {http://delivery.acm.org/10.1145/2690000/2689747/p4-Sakurada.pdf?ip=129.241.187.53{\&}id=2689747{\&}acc=ACTIVE SERVICE{\&}key=CDADA77FFDD8BE08.5386D6A7D247483C.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=1017423379{\&}CFTOKEN=89893174{\&}{\_}{\_}acm{\_}{\_}=1513507205{\_}e86eb51f725e21a4c24},
year = {2014}
}
@misc{Dias2016,
abstract = {Since the early 1980s, wind power technology has experienced an immense growth with respect to both the turbine size and market share. As the demand for large-scale wind turbines and lor operation {\&} maintenance cost continues to raise, the interest on condition monitoring system has increased rapidly. The main components of wind turbines are the focus of all CMS since they frequently cause high repair costs and equipment downtime. However, vast quantities of their failures are caused due to a bearing failure. Therefore, bearing condition monitoring becomes crucial. This paper aims at providing a state-of-the-art review on wind turbine bearing condition monitoring techniques such as acoustic measurement, electrical effects monitoring, power quality, temperature monitoring, wear debris analysis and vibration analysis. Furthermore, this paper will present a literature review and discuss several technical, financial and operational challenges from the purchase of the CMS to the wind farm monitoring stage.},
author = {{De Azevedo}, Henrique Dias Machado and Ara{\'{u}}jo, Alex Maur{\'{i}}cio and Bouchonneau, Nad{\`{e}}ge},
booktitle = {Renewable and Sustainable Energy Reviews},
doi = {10.1016/j.rser.2015.11.032},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dias et al. - 2016 - A review of wind turbine bearing condition monitoring State of the art and challenges.pdf:pdf},
isbn = {13640321},
issn = {18790690},
keywords = {Condition monitoring,Fault detection,Future challenges,Rolling element bearing,State-of-the-art,Wind turbines},
pages = {368--379},
title = {{A review of wind turbine bearing condition monitoring: State of the art and challenges}},
url = {https://ac.els-cdn.com/S1364032115012976/1-s2.0-S1364032115012976-main.pdf?{\_}tid=1d730dd0-e315-11e7-bb73-00000aab0f6c{\&}acdnat=1513506729{\_}c4b5bddcd08d86d2be8afe542577200e},
volume = {56},
year = {2016}
}
@article{Blum1997,
abstract = {In this survey, we review work in machine learning on$\backslash$nmethods for handling data sets containing large amounts$\backslash$nof irrelevant information. We focus on two key issues:$\backslash$nthe problem of selecting relevant features, and the$\backslash$nproblem of selecting relevant examples. We describe the$\backslash$nadvances that have been made on these topics in both$\backslash$nempirical and theoretical work in machine learning, and$\backslash$nwe present a general framework that we use to compare$\backslash$ndifferent methods. We close with some challenges for$\backslash$nfuture work in this area.},
author = {Blum, Avrim L. and Langley, Pat},
doi = {10.1016/S0004-3702(97)00063-5},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bluma, Langley - 1997 - Selection of relevant features and examples in machine.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Machine learning,Relevant examples,Relevant features},
number = {1-2},
pages = {245--271},
title = {{Selection of relevant features and examples in machine learning}},
url = {https://ac.els-cdn.com/S0004370297000635/1-s2.0-S0004370297000635-main.pdf?{\_}tid=1cb7170c-e265-11e7-ab2f-00000aab0f6b{\&}acdnat=1513431136{\_}a11bc0fd83102eb484da915fb5873767 http://linkinghub.elsevier.com/retrieve/pii/S0004370297000635},
volume = {97},
year = {1997}
}
@article{Dozat,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - Unknown - Workshop track -ICLR 2016 INCORPORATING NESTEROV MOMENTUM INTO ADAM.pdf:pdf},
journal = {ICLR Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ},
year = {2016}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - Unknown - Incorporating Nesterov Momentum into Adam.pdf:pdf},
journal = {ICLR Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {http://cs229.stanford.edu/proj2015/054{\_}report.pdf},
year = {2016}
}
@inproceedings{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimen-tal results on artiicial data and theoretical re-sults in restricted settings have shown that for selecting a good classiier from a set of classi-(model selection), ten-fold cross-validation may be better than the more expensive l e a ve-one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the eeects of diierent parameters on these al-gorithms on real-world datasets. For cross-validation, we v ary the number of folds and whether the folds are stratiied or nott for boot-strap, we v ary the number of bootstrap sam-ples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratiied cross validation, even if computation power allows using more folds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kohavi, Ron},
booktitle = {Appears in the International Joint Conference on Articial Intelligence (IJCAI)},
doi = {10.1067/mod.2000.109031},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohavi - 1995 - A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
isbn = {1-55860-363-8},
issn = {10450823},
pages = {1--7},
pmid = {11029742},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
url = {http://robotics.stanford.edu/{~}ronnyk},
year = {1995}
}
@article{Joachims1998,
abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Joachims, Thorsten and Dortmund, Universitat and Joachimscsuni-dortmundde, Thorsten},
doi = {10.1109/ICEMI.2009.5274151},
eprint = {arXiv:1301.3781v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 1998 - Making large-scale SVM learning practical.pdf:pdf},
isbn = {9781424438631},
issn = {15279995},
journal = {Advances in Kernel Methods - Support Vector Learning},
keywords = {310},
pages = {41--56},
pmid = {18244602},
publisher = {Dortmund: SFB 475, Universit{\"{a}}t Dortmund},
title = {{Making Large-Scale SVM Learning Practical}},
url = {https://www.econstor.eu/handle/10419/77178 http://svmlight.joachims.org/{\%}5Cnhttps://eldorado.uni-dortmund.de/handle/2003/2596},
year = {1999}
}
@article{Hernandez1998,
abstract = {Abstract The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem ...$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hern{\'{a}}ndez, Mauricio A and Stolfo, Salvatore J},
doi = {10.1023/A:1009761603038},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez, Stolfo - 1998 - Real-world Data is Dirty Data Cleansing and The MergePurge Problem.pdf:pdf},
isbn = {1384-5810},
issn = {13845810 (ISSN)},
journal = {Data Mining and Knowledge Discovery},
keywords = {data cleaning,data cleansing,duplicate elimination,semantic integration},
number = {1},
pages = {9--37},
pmid = {25246403},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038{\%}5Cnpapers3://publication/doi/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@article{Abadi,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1603.04467},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
pmid = {16411492},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://arxiv.org/pdf/1603.04467.pdf http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{Gola2012,
abstract = {Abstract In oil and gas industries, drilling is a complex and critical operation which require constant and accurate real-time monitoring. To this aim, real-time models are required to provide an overview of the drilling operations when direct and reliable measurements are not available. Given the harsh operating environment, sensor reliability and calibration are critical issues and bad data quality is a typical problem which affects the accuracy of the model. As a result, the driller may be misled about the down-hole situation or receive conflicting claims about operating conditions. This paper presents two approaches based on the use of artificial intelligence to improve monitoring of drilling processes in terms of reduced uncertainty and increased confidence. The first exploits the aggregation of the opinion of different experts within a so-called ensemble approach; the second is based on a so-called grey-box approach which combines a physical model and artificial intelligence. The two approaches are applied to the problem of predicting the bottom-hole pressure during a managed pressure drilling operation to demonstrate the improved accuracy and robustness. 1. Introduction A major critical task when starting the exploitation of a new well is the process of drilling the borehole. Drilling is a close-loop process in which the drilling fluid is pumped into the drill pipe at a pressure enough to cause it to circulate downwardly through the drill pipe, the drill collars, the bit nozzles and upwardly through the annulus between the borehole and the drill pipe back to the surface where it goes through a reconditioning process and is finally re-circulated through the pipes. The purpose of the drilling fluid is to remove rock and sediment fragments produced by the bit during drilling, to transport them to the surface, to cool the bit and to maintain pressure balance against the pressure in the rock formation. For these reasons, it is critical to ensure a constant fluid inlet at the right pressure at all time. A sudden loss of drilling fluid would lead in fact to built-up of rock fragments which, if not detected at an early stage, might lead to much worse situations such as stuck drill pipes, bit failures, drill string twist-off and more, resulting in significant non-productive time due to dangerous fishing trips or, in the worst cases, in the loss of expensive bottom-hole assemblies, potentially in the loss of the entire well and even in blowouts with extreme consequences like large financial losses, severe environmental damages and possible loss of lives. Given the critical role played by the drilling fluid especially in maintaining the correct pressure balance, control systems are adopted to regulate the drilling fluid influx with the purpose of precisely controlling the pressure profiles throughout the well hole. This paper focuses on the so-called Managed Pressure Drilling (MPD) system. The objectives of MPD are to ascertain the well bottom-hole pressure environment limits and to accordingly maintain the annular hydraulic pressure profile within its boundaries, i.e. above the pore pressure of the reservoir or the collapse pressure of the borehole and below the fracturing pressure of the borehole eventually responding rapidly to undesired events. Failure to maintain the correct pressure can for instance result in loss of drilling fluid to the formation or unexpected reservoir influxes (especially gas) which in the worst case scenario can lead to surface blowouts. A simplified model of the MPD control system is sketched in Figure 1. For a detailed description of the MPD system refer to [1].},
author = {Gola, Giulio and Nybo, R and Sui, D and Roverso, D},
doi = {10.2118/150201-MS},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gola et al. - 2012 - Improving Management and Control of Drilling Operations with Artificial Intelligence.pdf:pdf},
isbn = {9781618399311},
journal = {SPE Intelligent Energy {\ldots}},
number = {March},
pages = {1--7},
title = {{Improving Management and Control of Drilling Operations with Artificial Intelligence}},
url = {https://www.onepetro.org/conference-paper/SPE-150201-MS},
year = {2012}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos, Pedro - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Hawkins,
abstract = {Data Warehouse (DWH) systems allow to analyze business objects relevant to an enterprise organization (e.g., orders or customers).$\backslash$n $\backslash$n Analysts are interested in the states of these business objects: A customer is either a potential customer, a first time customer,$\backslash$n a regular customer or a past customer; purchase orders may be pending or fullfilled.$\backslash$n $\backslash$n $\backslash$n $\backslash$n Business objects and their states can be distributed over many parts of the DWH, and appear in measures, dimension attributes,$\backslash$n levels, etc.$\backslash$n $\backslash$n $\backslash$n $\backslash$n Surprisingly, this knowledge – how business objects and their states are represented in the DWH – is not made explicit in$\backslash$n existing conceptual models. We identify a need to make this relationship more accessible.$\backslash$n $\backslash$n $\backslash$n $\backslash$n We introduce the UML Profile for Representing Business Object States in a DWH. It makes the relationship between the business objects and the DWH conceptually visible. The UML Profile is applied to an$\backslash$n example.},
author = {Stefanov, Veronika and List, Beate},
doi = {10.1007/978-3-540-74553-2},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawkins et al. - Unknown - Outlier Detection Using Replicator Neural Networks.pdf:pdf},
isbn = {978-3-540-74552-5},
issn = {0302-9743},
journal = {Data Warehousing and Knowledge Discovery},
number = {DECEMBER},
pages = {209--220},
title = {{Outlier Detection Using Replicator Neural Networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.3366{\&}rep=rep1{\&}type=pdf http://www.springerlink.com/content/28433722x2204702},
volume = {4654},
year = {2007}
}
@inproceedings{Collobert,
abstract = {Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce ( i ) faster SVMs where training errors are no longer support vectors, and ( ii ) much faster Transductive SVMs.},
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L{\'{e}}on},
booktitle = {International Conference on Machine Learning},
doi = {10.1145/1143844.1143870},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - Unknown - Trading Convexity for Scalability.pdf:pdf},
isbn = {1-59593-383-2},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://delivery.acm.org/10.1145/1150000/1143870/p201-collobert.pdf?ip=129.241.187.53{\&}id=1143870{\&}acc=ACTIVE SERVICE{\&}key=CDADA77FFDD8BE08.5386D6A7D247483C.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=832816127{\&}CFTOKEN=92573539{\&}{\_}{\_}acm{\_}{\_}=1511432934{\_}9135dee02b5971768},
year = {2006}
}
@article{Baldi1989,
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. {\textcopyright} 1989.},
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Hornik - 1989 - Neural Networks and Principal Component Analysis Learning from Examples Without Local Minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
url = {https://ac.els-cdn.com/0893608089900142/1-s2.0-0893608089900142-main.pdf?{\_}tid=104bc7f4-d038-11e7-88d7-00000aacb35f{\&}acdnat=1511432667{\_}42d620ab61954044a9c22d5eed2dc6f1},
volume = {2},
year = {1989}
}
@article{Scholkopf2001,
abstract = {Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.},
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J. and Williamson, Robert C.},
doi = {10.1162/089976601750264965},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1443--1471},
pmid = {11440593},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Estimating the Support of a High-Dimensional Distribution}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976601750264965},
volume = {13},
year = {2001}
}
@article{Tax2004a,
abstract = {Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.},
author = {Tax, David M J and Fisher, Douglas},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Fisher - 2004 - Support Vector Data Description(2).pdf:pdf},
journal = {Machine Learning},
keywords = {novelty detection,one-class classification,outlier detection,support vector classifier,support vector data description},
pages = {45--66},
title = {{Support Vector Data Description}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/ML{\_}SVDD{\_}04.pdf},
volume = {54},
year = {2004}
}
@article{Ng,
author = {Ng, Andrew},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2017 - CS229 Additional Notes on Backpropagation.pdf:pdf},
title = {{CS229: Additional Notes on Backpropagation}},
url = {http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf},
year = {2017}
}
@article{Nga,
abstract = {We now begin our study of deep learning. In this set of notes, we give an overview of neural networks, discuss vectorization and discuss training neural networks with backpropagation.},
author = {Ng, Andrew},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2017 - CS229 Lecture Notes Deep Learning.pdf:pdf},
title = {{CS229 Lecture Notes Deep Learning}},
url = {http://cs229.stanford.edu/notes/cs229-notes-deep{\_}learning.pdf},
year = {2017}
}
@book{ChristopherM.Bishop,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christopher M. Bishop - Unknown - Pattern recognition and machine learning.pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
url = {http://users.isr.ist.utl.pt/{~}wurmd/Livros/school/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf},
volume = {53},
year = {2013}
}
@article{Halbertwhite,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Halbertwhite - Unknown - Multilayer Feedforward Networks are Universal Approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
url = {https://ac.els-cdn.com/0893608089900208/1-s2.0-0893608089900208-main.pdf?{\_}tid=2bf292d4-cae4-11e7-ab6e-00000aacb361{\&}acdnat=1510846880{\_}49ac579d7917cbe9b59e14659f1c357e},
volume = {2},
year = {1989}
}
@article{Korkov~1992,
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. {\textcopyright} 1992.},
author = {Kůrkov{\'{a}}, V{\v{e}}ra},
doi = {10.1016/0893-6080(92)90012-8},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korkov{\~{}} - 1992 - Kolmogorov's Theorem and Multilayer Neural Networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Approximations of continuous functions,Estimates of number of hidden units,Feedforward neural networks,Modulus of continuity,Multilayer perceptron type networks,Sigmoidal activation function,Uniform approximation,Universal approximation capabilities},
number = {3},
pages = {501--506},
title = {{Kolmogorov's theorem and multilayer neural networks}},
url = {https://ac.els-cdn.com/0893608092900128/1-s2.0-0893608092900128-main.pdf?{\_}tid=966cf7d8-cad2-11e7-bbab-00000aab0f6c{\&}acdnat=1510839329{\_}ec0e83cebae14e704b46d386e6522adc},
volume = {5},
year = {1992}
}
@article{Kurkova1992,
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation. {\textcopyright} 1992.},
author = {Kůrkov{\'{a}}, V{\v{e}}ra},
doi = {10.1016/0893-6080(92)90012-8},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korkov{\~{}} - 1992 - Kolmogorov's Theorem and Multilayer Neural Networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Approximations of continuous functions,Estimates of number of hidden units,Feedforward neural networks,Modulus of continuity,Multilayer perceptron type networks,Sigmoidal activation function,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {3},
pages = {501--506},
publisher = {Pergamon},
title = {{Kolmogorov's theorem and multilayer neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/0893608092900128},
volume = {5},
year = {1992}
}
@article{Manevitz2007,
abstract = {Automated document retrieval and classification is of central importance in many contexts; our main motivating goal is the efficient classification and retrieval of "interests" on the internet when only positive information is available. In this paper, we show how a simple feed-forward neural network can be trained to filter documents under these conditions, and that this method seems to be superior to modified methods (modified to use only positive examples), such as Rocchio, Nearest Neighbor, Naive-Bayes, Distance-based Probability and One-Class SVM algorithms. A novel experimental finding is that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation ("Hadamard") of the information prior to the training of the network. {\textcopyright} 2006.},
author = {Manevitz, Larry and Yousef, Malik},
doi = {10.1016/j.neucom.2006.05.013},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manevitz, Yousef - 2007 - One-class document classification via Neural Networks.pdf:pdf},
isbn = {09252312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autoencoder,Automated document retrieval,Bottleneck neural network,Classification,Feed-forward neural networks,Machine learning,One-class classification},
number = {7-9},
pages = {1466--1481},
title = {{One-class document classification via Neural Networks}},
url = {http://cs.haifa.ac.il/{~}manevitz/Publication/One-class document classification via Neural Networks.pdf},
volume = {70},
year = {2007}
}
@book{Hastie,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {{Hastie, Trevor Tibshirani, Robert Friedman}, Jerome},
booktitle = {Springer},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - Unknown - Springer Series in Statistics The Elements of Statistical Learning The Elements of Statistical.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {0162-1459},
keywords = {inger series in statistics},
pmid = {21196786},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2004.s339},
volume = {2},
year = {2008}
}
@techreport{Energi21,
abstract = {Energi21 er Olje- og energidepartementets strategiorgan for forskning, utvikling og demonstrasjon innen energiomr{\aa}det. Hovedm{\aa}let med Energi21-strategiene er {\aa} gi anbe- falinger til Olje- og energidepartementet om fremtidige prioriteringer for satsingen innen utvikling av nye klima- og milj{\o}vennlige l{\o}sninger for energiomr{\aa}det},
author = {Energi21},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Energi21 - 2014 - Strategi 2014, del 2.pdf:pdf},
title = {{Strategi 2014, del 2}},
url = {https://www.forskningsradet.no/no/Publikasjon/Energi{\_}21{\_}strategi{\_}2014{\_}{\_}Del{\_}2/1253998081130?lang=no},
year = {2014}
}
@article{Choudhury2005,
abstract = {The presence of nonlinearities, e.g., stiction, and deadband in a control valve limits the control loop performance. Stiction is the most commonly found valve problem in the process industry. In spite of many attempts to understand and model the stiction phenomena, there is a lack of a proper model, which can be understood and related directly to the practical situation as observed in real valves in the process industry. This study focuses on the understanding, from real-life data, of the mechanism that causes stiction and proposes a new data-driven model of stiction, which can be directly related to real valves. It also validates the simulation results generated using the proposed model with that from a physical model of the valve. Finally, valuable insights on stiction have been obtained from the describing function analysis of the newly proposed stiction model. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
annote = {cited 250ish times Page 643 shows a figure of a valve deadbands Deadband: ‘‘In process instrumentation, it is the range through which an input signal may be varied, upon reversal of direction, without initiating an observable change in output signal''},
author = {{Shoukat Choudhury}, M. A.A. and Thornhill, N F and Shah, Sirish L},
doi = {10.1016/j.conengprac.2004.05.005},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury, Thornhill, Shah - 2005 - Modelling valve stiction.pdf:pdf},
isbn = {0967-0661},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Backlash,Coulomb friction,Deadband,Deadzone,Hysteresis,Process control,Slip jump,Stickband,Stiction,Viscous friction},
number = {5},
pages = {641--658},
title = {{Modelling valve stiction}},
url = {https://ac.els-cdn.com/S0967066104001145/1-s2.0-S0967066104001145-main.pdf?{\_}tid=c75b9a5c-bfdd-11e7-b665-00000aab0f26{\&}acdnat=1509634671{\_}d40835e745492fd86ead5ddce1a6aed9},
volume = {13},
year = {2005}
}
@inproceedings{Kingma,
abstract = {Login systems in smart devices demand multi-factor authentication for high security and at the same time, it requires simple user experience. We propose a novel application of lip-reading satisfying these requirements. We present the adequacy of lip-reading as a biometric factor by experiment. In addition, automatic lip-reader can be implemented by LSTM (Long Short Term Memory) neural network architecture with good accuracy that can translate visual utterance to password as a knowledge factor. Furthermore, our proposed method, iterative method, can improve accuracy as much as login system required. Our work achieved 93.8{\%} by single iteration from the first result (69.1{\%}).},
annote = {Adam is a stochastic first order gradient descent based method. It uses advantages from both AdaGrad and RMSprop in its impelentation and is a popular choice for optimization in machine learning. For more in depth information see $\backslash$cite{\{}adam{\}}},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lee, Daehyun and Myung, Kyungsik},
booktitle = {2017 IEEE International Conference on Consumer Electronics, ICCE 2017},
doi = {10.1109/ICCE.2017.7889386},
eprint = {1412.6980},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
isbn = {9781509055449},
issn = {09252312},
pages = {434--435},
pmid = {172668},
title = {{Read my lips, login to the virtual world}},
url = {https://arxiv.org/pdf/1412.6980v8.pdf},
year = {2017}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@inproceedings{Sutskever,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever et al. - Unknown - On the importance of initialization and momentum in deep learning.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
number = {2010},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
url = {http://proceedings.mlr.press/v28/sutskever13.pdf},
year = {2013}
}
@article{Hernandez1998,
abstract = {Abstract The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem ...$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hern{\'{a}}ndez, Mauricio A. and Stolfo, Salvatore J.},
doi = {10.1023/A:1009761603038},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez, Stolfo - 1998 - Real-world Data is Dirty Data Cleansing and The MergePurge Problem.pdf:pdf},
isbn = {1384-5810},
issn = {13845810 (ISSN)},
journal = {Data Mining and Knowledge Discovery},
keywords = {data cleaning,data cleansing,duplicate elimination,semantic integration},
number = {1},
pages = {9--37},
pmid = {25246403},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038 http://link.springer.com/10.1023/A:1009761603038{\%}5Cnpapers3://publication/doi/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@article{Tax2004,
abstract = {Propose des SVM qui au lieu de discriminer deux classes essaye de$\backslash$ndecrire une classe. Possibilit� d'apprendre avec ou sans contre-exemples.},
author = {Tax, David M J and Duin, Robert P W},
doi = {10.1023/B:MACH.0000008084.60811.49},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Fisher - 2004 - Support Vector Data Description.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Novelty detection,One-class classification,Outlier detection,Support vector classifier,Support vector data description},
number = {1},
pages = {45--66},
pmid = {20842844},
title = {{Support Vector Data Description}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AMACH.0000008084.60811.49.pdf},
volume = {54},
year = {2004}
}
@inproceedings{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
address = {New York, New York, USA},
annote = {2000+ citations bruke i argumetnasjon for F1 score, og kurver!},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - 2006 - The relationship between Precision-Recall and ROC curves.pdf:pdf},
isbn = {1595933832},
issn = {14710080},
pages = {233--240},
pmid = {19165215},
publisher = {ACM Press},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@inproceedings{Davis,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
archivePrefix = {arXiv},
arxivId = {1609.07195},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143874},
eprint = {1609.07195},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - Unknown - The Relationship Between Precision-Recall and ROC Curves.pdf:pdf},
isbn = {1595933832},
issn = {14710080},
pages = {233--240},
pmid = {19165215},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://pages.cs.wisc.edu/{~}jdavis/davisgoadrichcamera2.pdf http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@article{Wolfe1959,
archivePrefix = {arXiv},
arxivId = {www.jstor.org/stable/43635235},
author = {Wolfe, Philip and Tucker, A. W.},
doi = {10.2307/1909468},
eprint = {stable/43635235},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolfe, Tucker - 1959 - The Simplex Method for Quadratic Programming.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
month = {jul},
number = {3},
pages = {382},
primaryClass = {www.jstor.org},
publisher = {The Technology Press of The Massachusetts Institute of Technology, Cambridge, Mass.},
title = {{The Simplex Method for Quadratic Programming}},
url = {http://www.jstor.org/stable/1909468?origin=crossref},
volume = {27},
year = {1959}
}
@article{BergstraJAMESBERGSTRA2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
annote = {Grid search, manual search vs random search. 
How dows grid search work? Grid search tests all possible combinations of your hyperparameters not all hyperparameters are important to tune! Therefore random search is better than grid search.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {{Bergstra JAMESBERGSTRA}, James and {Yoshua Bengio YOSHUABENGIO}, Umontrealca},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra JAMESBERGSTRA, Yoshua Bengio YOSHUABENGIO - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\_}1},
year = {2017}
}
