@article{Pan2008,
abstract = {The study aims at realizing a remote online machine condition monitoring system built up in the architecture of both the Borland C++ Builder (BCB) software-developing environment and Internet transmission communication. Various sig-nal-processing computation schemes such as time–frequency analysis and order tracking for signal analysis and pattern recognition purposes are implemented based upon the Borland C++ Builder graphical user interface. Thus machine fault diagnostic capability can be extended by using the socket application program interface as the transmission control pro-tocol/Internet protocol (TCP/IP). In the study, the effectiveness of the developed remote diagnostic system is justified by monitoring a transmission-element test rig. A complete monitoring cycle including data acquisition, signal-processing, fea-ture extraction, pattern recognition through the artificial neural networks, and online video surveillance, is demonstrated.},
author = {Pan, Min-Chun and Li, Po-Ching and Cheng, Yong-Ren},
doi = {10.1016/j.measurement.2008.01.004},
file = {::},
keywords = {Fault diagnosis,Machine condition monitoring,Remote monitoring,Signal-processing},
title = {{Remote online machine condition monitoring system}},
url = {https://ac.els-cdn.com/S0263224108000067/1-s2.0-S0263224108000067-main.pdf?{\_}tid=9df13660-5853-4cee-aeff-e2f713734b5e{\&}acdnat=1526288170{\_}0c78783821d3d208d2fdd3605fc66f77},
year = {2008}
}
@article{Widodo2007,
abstract = {Recently, the issue of machine condition monitoring and fault diagnosis as a part of maintenance system became global due to the potential advantages to be gained from reduced maintenance costs, improved productivity and increased machine availability. This paper presents a survey of machine condition monitoring and fault diagnosis using support vector machine (SVM). It attempts to summarize and review the recent research and developments of SVM in machine condition monitoring and diagnosis. Numerous methods have been developed based on intelligent systems such as artificial neural network, fuzzy expert system, condition-based reasoning, random forest, etc. However, the use of SVM for machine condition monitoring and fault diagnosis is still rare. SVM has excellent performance in generalization so it can produce high accuracy in classification for machine condition monitoring and diagnosis. Until 2006, the use of SVM in machine condition monitoring and fault diagnosis is tending to develop towards expertise orientation and problem-oriented domain. Finally, the ability to continually change and obtain a novel idea for machine condition monitoring and fault diagnosis using SVM will be future works.},
author = {Widodo, Achmad and Yang, Bo-Suk},
doi = {10.1016/j.ymssp.2006.12.007},
file = {::},
journal = {Mechanical Systems and Signal Processing},
keywords = {Fault diagnosis,Machine condition monitoring,Support vector machine},
pages = {2560--2574},
title = {{Mechanical Systems and Signal Processing Support vector machine in machine condition monitoring and fault diagnosis}},
url = {https://ac.els-cdn.com/S0888327007000027/1-s2.0-S0888327007000027-main.pdf?{\_}tid=2aa3df2c-ee0b-4e63-a335-2052850aeeea{\&}acdnat=1526288040{\_}dd85dd11edbdba4dad94202201df58bb},
volume = {21},
year = {2007}
}
@article{Molina,
abstract = {The use of Neural Networks (NN) is a novel approach that can help in taking decisions when integrated in a more general system, in particular with expert systems. In this paper, an architecture for the management of hydroelectric power plants is introduced. This relies on monitoring a large number of signals, representing the technical parameters of the real plant. The general architecture is composed of an Expert System and two NN modules: Acoustic Prediction (NNAP) and Predictive Maintenance (NNPM). The NNAP is based on Kohonen Learning Vector Quantization (LVQ) Networks in order to distinguish the sounds emitted by electricity-generating machine groups. The NNPM uses an ART-MAP to identify dierent situations from the plant state variables, in order to prevent future malfunctions. In addition, a special process to generate a complete training set has been designed for the ART-MAP module. This process has been developed to deal with the absence of data about abnormal plant situations, and is based on neural nets trained with the backpropagation algorithm. 7},
author = {Molina, J M and Isasi, P and Berlanga, A and Sanchis, A},
file = {::},
keywords = {ART,Expert systems,LVQ,Neural networks,Power plants,Predictive maintenance},
title = {{Hydroelectric power plant management relying on neural networks and expert system integration}},
url = {https://ac.els-cdn.com/S0952197600000099/1-s2.0-S0952197600000099-main.pdf?{\_}tid=4e916db7-3e66-4e1f-9d9d-9151ab9ca503{\&}acdnat=1526287990{\_}d9e9a9d5b113ada40a1f34c61dd1e37b}
}
@article{Fast2008,
abstract = {a b s t r a c t Demonstration of different utilities for industrial use of an artificial neural network (ANN) model for a gas turbine has been reported in this paper. The ANN model was constructed with the multi-layer feed-for-ward network type and trained with operational data using back-propagation. The results showed that operational and performance parameters of the gas turbine, including identification of anti-icing mode, can be predicted with good accuracy for varying local ambient conditions. Different possible applications of this ANN model were also demonstrated. These include instantaneous gas turbine performance esti-mation through a graphical user interface and extrapolation beyond the range of training data.},
author = {Fast, M and Assadi, M and De, S},
doi = {10.1016/j.apenergy.2008.03.018},
file = {::},
keywords = {ANN,Gas turbine,Modelling,Simulation},
title = {{Development and multi-utility of an ANN model for an industrial gas turbine}},
url = {https://ac.els-cdn.com/S030626190800072X/1-s2.0-S030626190800072X-main.pdf?{\_}tid=d40b9acd-6957-4b4b-ab93-d44f0545c438{\&}acdnat=1526287941{\_}4e87c19fcfe530fd7f116f693bcee092},
year = {2008}
}
@article{Zhang2011,
abstract = {Hydropower station is one of key energy construction projects in china. The hydro-mechanical component is one of the core parts of the hydropower station. Therefore, the importance of hydro-condition monitoring becomes even more conspicuous in hydropower station. In this present study, the hardware of hydropower station hydraulic monitoring system was re-designed and integrated through researching hydro-determination theory and method of the hydropower station, using ADAM modules; At the same time, by using development tool C++ and assembly language, a 16-channel data acquisition and visualization operations hydro-monitoring system was completed by implementing the functions of acquisition, calibration, analysis, processing for the software system. Finally, the entire monitoring system was applied on Tianqiao hydropower, and the result indicated that this system might be a better method in monitoring results.},
author = {Zhang, Wumei},
doi = {10.1016/j.proeng.2011.08.150},
file = {::},
journal = {Procedia Engineering},
keywords = {ADAM modules,Hydraulic unit,data acquisition,vibration test},
pages = {807--811},
title = {{Development and Application of Hydropower hydro Monitoring System}},
url = {www.sciencedirect.com www.elsevier.com/locate/procedia},
volume = {15},
year = {2011}
}
@article{GarciaMarquez2012,
abstract = {a b s t r a c t Wind Turbines (WT) are one of the fastest growing sources of power production in the world today and there is a constant need to reduce the costs of operating and maintaining them. Condition monitoring (CM) is a tool commonly employed for the early detection of faults/failures so as to minimise downtime and maximize productivity. This paper provides a review of the state-of-the-art in the CM of wind turbines, describing the different maintenance strategies, CM techniques and methods, and highlighting in a table the various combinations of these that have been reported in the literature. Future research opportunities in fault diagnostics are identified using a qualitative fault tree analysis.},
author = {{Garc{\'{i}}a M{\'{a}}rquez}, Fausto Pedro and Tobias, Andrew Mark and Mar{\'{i}}a, Jes{\'{u}}s and P{\'{e}}rez, Pinar and Papaelias, Mayorkinos},
doi = {10.1016/j.renene.2012.03.003},
file = {::},
journal = {Renewable Energy},
keywords = {Condition monitoring,Fault detection and diagnosis,Maintenance management,Wind turbines},
pages = {169--178},
title = {{Condition monitoring of wind turbines: Techniques and methods}},
url = {https://ac.els-cdn.com/S0960148112001899/1-s2.0-S0960148112001899-main.pdf?{\_}tid=5df85283-6819-4b42-b37b-33615eff4d19{\&}acdnat=1526287836{\_}7bd74fc09d2624b61bae40c7a8cab629},
volume = {46},
year = {2012}
}
@article{PedregosaF.andVaroquauxG.andGramfortA.andMichel2011,
author = {{Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel}, V. and {and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer}, P. and and {and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos}, A. and {Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay}, E.},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@inproceedings{Scholkopf2000,
abstract = {Suppose you are given some dataset drawn from an underlying probabil-ity distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified l/ between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a poten-tially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.},
author = {Scholkopf, Bernhard and Williamson, Robert and Smola, Alex and Shawe-Taylor, John and Platt, John},
booktitle = {Advances in neural information processing systems},
doi = {10.1.1.71.4642},
file = {::},
isbn = {0-262-11245-0},
issn = {0885-6125},
pages = {582--588},
pmid = {20842844},
title = {{Support Vector Method for Novelty Detection}},
url = {https://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf http://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf},
year = {2000}
}
@article{Pascanu,
abstract = {There are two widely known issues with prop-erly training recurrent neural networks, the vanishing and the exploding gradient prob-lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under-standing of the underlying issues by explor-ing these problems from an analytical, a geo-metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef-fective solution. We propose a gradient norm clipping strategy to deal with exploding gra-dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
file = {::},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://proceedings.mlr.press/v28/pascanu13.pdf}
}
@article{Manuca1996,
abstract = {In this paper we introduce a new class of methods to test, model and describe nonstationary processes. To frame these methods, we generalize the dynamical description of autonomous systems to the case of nonautonomous systems. Of particular interest are systems for which the driving force is recurrent. For these systems we describe a method to find recurrences and to improve the statistics in reconstructing the time series and, consequently, to improve the predictability. Another objective is a proper description of the nonstationarity. All these methods are applied to four examples.},
author = {Manuca, Radu and Savit, Robert},
doi = {10.1016/S0167-2789(96)00139-X},
file = {::},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {2-3},
pages = {134--161},
title = {{Stationarity and nonstationarity in time series analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S016727899600139X},
volume = {99},
year = {1996}
}
@techreport{Gers,
abstract = {Long short-term memory (LSTM; Hochreiter {\&} Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM net-works processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's in-ternal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive " forget gate " that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustra-tive benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve contin-ual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
author = {Gers, Felix A and Schmidhuber, Urgen and Cummins, Fred},
file = {::},
title = {{Learning to Forget: Continual Prediction with LSTM}},
url = {https://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015}
}
@article{Dunning,
author = {Dunning, Ted and Friedman, Ellen},
file = {::},
title = {{Practical Machine Learning: A New Look at Anomaly Detection}},
url = {http://info.mapr.com/rs/mapr/images/Practical{\_}Machine{\_}Learning{\_}Anomaly{\_}Detection.pdf?mkt{\_}tok=eyJpIjoiTW1Nd01qTTROamxtTVRjdyIsInQiOiJianZueHFmQmJqYzZoaWt1T0lNN3JjcTI1dFh3ZTVwa0pUWEFuRU9EUWtKbWlOY1wvSE5IUEFmdk5FbTI4S2Z4Tm1tUTFvaHNQa0JLYzFoMXZmb1h2RDJKaDBYdm}
}
@article{Kim2012,
abstract = {We propose a method for nonparametric density estimation that exhibits robustness to contamina-tion of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to con-verge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.},
author = {Kim, Jooseuk and Scott, Clayton D and Edu, Clayscot@umich},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {M-estimation,influence function,kernel trick,outlier,reproducing kernel Hilbert space},
pages = {2529--2565},
title = {{Robust Kernel Density Estimation}},
url = {http://www.jmlr.org/papers/volume13/kim12b/kim12b.pdf},
volume = {13},
year = {2012}
}
@article{Latecki,
abstract = {Outlier detection has recently become an important prob-lem in many industrial and financial applications. In this paper, a novel unsupervised algorithm for outlier detection with a solid statistical foun-dation is proposed. First we modify a nonparametric density estimate with a variable kernel to yield a robust local density estimation. Out-liers are then detected by comparing the local density of each point to the local density of its neighbors. Our experiments performed on sev-eral simulated data sets have demonstrated that the proposed approach can outperform two widely used outlier detection algorithms (LOF and LOCI).},
author = {Latecki, Longin Jan and Lazarevic, Aleksandar and Pokrajac, Dragoljub},
file = {::},
title = {{Outlier Detection with Kernel Density Functions}},
url = {https://cis.temple.edu/{~}latecki/Papers/mldm07.pdf}
}
@article{Guthrie,
abstract = {This paper describes work on the detection of anomalous material in text. We show several vari-ants of an automatic technique for identifying an 'unusual' segment within a document, and consider texts which are unusual because of author, genre [Biber, 1998], topic or emotional tone. We evaluate the technique using many experiments over large document collections, created to contain randomly inserted anomalous segments. In order to success-fully identify anomalies in text, we define more than 200 stylistic features to characterize writing, some of which are well-established stylistic deter-miners, but many of which are novel. Using these features with each of our methods, we examine the effect of segment size on our ability to detect anomaly, allowing segments of size 100 words, 500 words and 1000 words. We show substantial improvements over a baseline in all cases for all methods, and identify the method variant which performs consistently better than others.},
author = {Guthrie, David and Guthrie, Louise and Allison, Ben and Wilks, Yorick},
file = {::},
keywords = {natural language processing},
title = {{Unsupervised Anomaly Detection}},
url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-262.pdf}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, Jj},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {::},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{LONG SHORT-TERM MEMORY}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}5Cnhttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Malhotra,
abstract = {Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM net-works for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behav-ior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
author = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
file = {::},
title = {{Long Short Term Memory Networks for Anomaly Detection in Time Series}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf}
}
@article{Malhotra2016,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
archivePrefix = {arXiv},
arxivId = {1607.00148},
author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
eprint = {1607.00148},
file = {::},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {https://arxiv.org/pdf/1607.00148.pdf http://arxiv.org/abs/1607.00148},
year = {2016}
}
@article{Hautam,
author = {Hautam, Ville and Ismo, K},
file = {::},
pages = {4--7},
title = {{Outlier Detection Using k-Nearest Neighbour Graph ¥ Q P ¥ ¥ R {\$} S ¨ UTWVYX ' EDGFIH ¢¥ P {\$}}}
}
@article{Tarassenko2009,
author = {Tarassenko, Lionel and Clifton, David A and Bannister, Peter R and King, Steve and King, Dennis},
file = {::},
isbn = {9780470058220},
journal = {Encyclopaedia of Structural Health Monitoring},
pages = {653--675},
title = {{Novelty Detection}},
url = {https://pdfs.semanticscholar.org/a65d/7c7137968e291f0732b12110e485276cbe11.pdf},
year = {2009}
}
@inproceedings{Hautamaki2004,
abstract = {We present an outlier detection using indegree number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance-based method are also proposed. We compare the methods with real and synthetic datasets. The results show that the proposed method achieves reasonable results with synthetic data and outperforms compared methods with real data sets with small number of observations.},
author = {Hautam{\"{a}}ki, Ville and K{\"{a}}rkk{\"{a}}inen, Ismo and Fr{\"{a}}nti, Pasi},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1334558},
isbn = {0769521282},
issn = {10514651},
pages = {430--433},
publisher = {IEEE},
title = {{Outlier detection using k-nearest neighbour graph}},
url = {http://ieeexplore.ieee.org/document/1334558/},
volume = {3},
year = {2004}
}
@article{Hodge,
abstract = {Outlier detection has been used for centuries to detect and, where appropri-ate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
author = {Hodge, Victoria J and Austin, Jim},
file = {::},
keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
title = {{A Survey of Outlier Detection Methodologies}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AAIRE.0000045502.10941.a9.pdf}
}
@article{Mckinney2010,
abstract = {—In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {Mckinney, Wes},
file = {::},
journal = {PROC. OF THE 9th PYTHON IN SCIENCE CONF},
keywords = {Index Terms—data structure,R,statistics},
title = {{Data Structures for Statistical Computing in Python}},
url = {https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf},
year = {2010}
}
@misc{Statkraft2009,
abstract = {Vannkraft er en milj{\o}vennlig og fornybar energikilde. 99 prosent av all kraftproduksjon i Norge kommer fra vannkraft. P{\aa} verdensbasis utgj{\o}r vannkraften rundt en sjettedel av den totale kraftproduksjonen},
author = {Statkraft},
title = {{Vannkraft}},
url = {https://www.statkraft.no/globalassets/old-contains-the-old-folder-structure/documents/no/vannkraft-09-no{\_}tcm10-4585.pdf},
urldate = {2018-03-20},
year = {2009}
}
@article{Paish2002,
abstract = {Hydropower, large and small, remains by far the most important of the «renewables» for electrical power production worldwide, providing 19{\%} of the planet's electricity. Small-scale hydro is in most cases «run-of-river», with no dam or water storage, and is one of the most cost-effective and environmentally benign energy technologies to be considered both for rural electrification in less developed countries and further hydro developments in Europe. The European Commission have a target to increase small hydro capacity by 4500MW (50{\%}) by the year 2010. The UK has 100MW of existing small hydro capacity (under 5MW) operating from approximately 120 sites, and at least 400MW of unexploited potential. With positive environmental policies now being backed by favourable tariffs for 'green' electricity, the industry believes that small hydro will have a strong resurgence in Europe in the next 10 years, after 20 years of decline. This paper summarises the different small hydro technologies, new innovations being developed, and the barriers to further development. {\textcopyright} 2002 Elsevier Science Ltd.},
author = {Paish, Oliver},
doi = {10.1016/S1364-0321(02)00006-0},
file = {::},
isbn = {1364-0321},
issn = {13640321},
journal = {Renewable and Sustainable Energy Reviews},
keywords = {Hydropower,Micro-hydro,Mini-hydro,Small hydro,Water power},
number = {6},
pages = {537--556},
title = {{Small hydro power: Technology and current status}},
url = {www.elsevier.com/locate/rser},
volume = {6},
year = {2002}
}
@article{Schslkopf,
abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
author = {Schslkopf, Bernhard and Smola, Alexander and Mfiller, Klaus-Robert},
file = {::},
title = {{Kernel Principal Component Analysis}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBFb0020217.pdf}
}
@article{Belkin2001,
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1.1.19.9400},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1049-5258},
issn = {10495258},
journal = {Nips},
pages = {585--591},
pmid = {25246403},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering}},
url = {http://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.9400{\&}rep=rep1{\&}type=pdf},
volume = {14},
year = {2001}
}
@article{Li,
abstract = {This paper studies one application of mutual information to symbolic sequen-ces: the mutual information function M(d). This function is compared with the more frequently used correlation function F(d). An exact relation between M(d) and F(d) is derived for binary sequences. For sequences with more than two symbols, no such general relation exists; in particular, F(d) = 0 may or may not lead to M(d)= 0. This linear, but not general, independence between symbols separated by a distance is studied for ternary sequences. Also included is the estimation of the finite-size effect on calculating mutual information. Finally, the concept of "symbolic noise" is discussed.},
author = {Li, Wentian},
doi = {10.1007/BF01025996},
file = {::},
issn = {0022-4715},
journal = {J. Stat. Phys.},
keywords = {correlation functions,linear and general dependence,mutual information function,symbolic noise},
number = {5},
pages = {823--837},
title = {{Mutual information versus correlation functions}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF01025996.pdf papers2://publication/uuid/805C6936-1BA8-4632-B6C6-5BB2CDB979C3},
volume = {60},
year = {1990}
}
@inproceedings{Khalid2014,
author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
booktitle = {2014 Science and Information Conference},
doi = {10.1109/SAI.2014.6918213},
file = {::},
isbn = {978-0-9893193-1-7},
month = {aug},
pages = {372--378},
publisher = {IEEE},
title = {{A survey of feature selection and feature extraction techniques in machine learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6918213},
year = {2014}
}
@article{Chandrashekar2014,
abstract = {a b s t r a c t Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction perfor-mance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in lit-erature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {::},
journal = {Computers and Electrical Engineering},
pages = {16--28},
title = {{A survey on feature selection methods}},
url = {https://ac.els-cdn.com/S0045790613003066/1-s2.0-S0045790613003066-main.pdf?{\_}tid=c336f5ca-2494-422d-9db8-22b90d47458a{\&}acdnat=1520844383{\_}5faa66e2252a7c49c3d0c76ad66df5de},
volume = {40},
year = {2014}
}
@article{Kraskov2004,
abstract = {We present two classes of improved estimators for mutual information {\$}M(X,Y){\$}, from samples of random points distributed according to some joint probability density {\$}\backslashmu(x,y){\$}. In contrast to conventional estimators based on binnings, they are based on entropy estimates from {\$}k{\$}-nearest neighbour distances. This means that they are data efficient (with {\$}k=1{\$} we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of {\$}k/N{\$} for {\$}N{\$} points. Numerically, we find that both families become {\{}$\backslash$it exact{\}} for independent distributions, i.e. the estimator {\$}\backslashhat M(X,Y){\$} vanishes (up to statistical fluctuations) if {\$}\backslashmu(x,y) = \backslashmu(x) \backslashmu(y){\$}. This holds for all tested marginal distributions and for all dimensions of {\$}x{\$} and {\$}y{\$}. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
annote = {Scikit-learn reference!},
archivePrefix = {arXiv},
arxivId = {cond-mat/0305641},
author = {Kraskov, Alexander and St{\"{o}}gbauer, Harald and Grassberger, Peter},
doi = {10.1103/PhysRevE.69.066138},
eprint = {0305641},
file = {::},
isbn = {1539-3755 (Print)},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {6},
pages = {16},
pmid = {15244698},
primaryClass = {cond-mat},
title = {{Estimating mutual information}},
url = {https://journals.aps.org/pre/pdf/10.1103/PhysRevE.69.066138},
volume = {69},
year = {2004}
}
@article{Peng2005,
abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
archivePrefix = {arXiv},
arxivId = {f},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
eprint = {f},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Long, Ding - 2005 - Feature selection based on mutual information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy}},
volume = {27},
year = {2005}
}
@article{Peng2005a,
abstract = {Feature selection is an important problem for pattern classification systems.Westudy how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combiningmRMRand other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost.Weperform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
archivePrefix = {arXiv},
arxivId = {f},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
eprint = {f},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Long, Ding - 2005 - Feature selection based on mutual information Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
month = {aug},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy}},
url = {http://ieeexplore.ieee.org/document/1453511/},
volume = {27},
year = {2005}
}
@misc{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
isbn = {0893-6080},
issn = {18792782},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Ding2014,
abstract = {Novelty detection is especially important for monitoring safety-critical systems in which novel conditions rarely occur and knowledge about novelty in that system is often limited or unavailable. There are a large number of studies in the area of novelty detection, but there is a lack of a comprehensive experimental evaluation of existing novelty detection methods. This paper aims to fill this void by conducting experimental evaluation of representative novelty detection methods. It presents a state-of-the-art review of novelty detection, with a focus on methods reported in the last few years. In addition, a rigorous comparative evaluation of four widely used methods, representative of different categories of novelty detectors, is carried out using 10 benchmark datasets with different scale, dimensionality and problem complexity. The experimental results demonstrate that the k-NN novelty detection method exhibits competitive overall performance to the other methods in terms of the AUC metric. {\textcopyright} 2013 Elsevier B.V.},
author = {Ding, Xuemei and Li, Yuhua and Belatreche, Ammar and Maguire, Liam P.},
doi = {10.1016/j.neucom.2013.12.002},
issn = {18728286},
journal = {Neurocomputing},
title = {{An experimental evaluation of novelty detection methods}},
volume = {135},
year = {2014}
}
@article{Omar2013,
abstract = {Intrusion detection has gain a broad attention and become a fertile field for several researches, and still being the subject of widespread interest by researchers. The intrusion detection community still confronts difficult problems even after many years of research. Reducing the large number of false alerts during the process of detecting unknown attack patterns remains unresolved problem. However, several research results recently have shown that there are potential solutions to this problem. Anomaly detection is a key issue of intrusion detection in which perturbations of normal behavior indicates a presence of intended or unintended induced attacks, faults, defects and others. This paper presents an overview of research directions for applying supervised and unsupervised methods for managing the problem of anomaly detection. The references cited will cover the major theoretical issues, guiding the researcher in interesting research directions.},
annote = {Oppsummering av mange ulike teknikar for anomaly detection, kan brukast til sitering!},
author = {Omar, Salima and Ngadi, Asri and Jebur, Hamid H},
doi = {10.5120/13715-1478},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Omar, Ngadi, Jebur - 2013 - Machine Learning Techniques for Anomaly Detection An Overview.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {Network Intrusion Detection,Supervised Machine Learning,Unsupervised Machine Learning},
number = {2},
pages = {975--8887},
title = {{Machine Learning Techniques for Anomaly Detection: An Overview}},
url = {https://pdfs.semanticscholar.org/0278/bbaf1db5df036f02393679d485260b1daeb7.pdf},
volume = {79},
year = {2013}
}
@article{He2005,
abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning sce- narios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of pre- vious unsupervised feature selection methods are wrapper techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a ﬁlter method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demon- strate the effectiveness and efﬁciency of our algorithm.},
author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
doi = {http://books.nips.cc/papers/files/nips18/NIPS2005_0149.pdf},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Cai, Niyogi - 2005 - Laplacian Score for Feature Selection.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {507--514},
title = {{Laplacian Score for Feature Selection}},
url = {http://papers.nips.cc/paper/2909-laplacian-score-for-feature-selection.pdf},
year = {2005}
}
@article{Liu2010,
abstract = {The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, commu- nications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining appli- cations. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We first briefly introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vi- brant research field of some contemporary interests, new applications, and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.},
author = {Liu, Huan (National University of Singapore) and Motoda, Hiroshi (Osaka University) and Setiono, Rudy and Zhao, Zheng},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2010 - Feature Selection An Ever Evolving Frontier in Data Mining.pdf:pdf},
issn = {1541-1672},
journal = {Journal of Machine Learning Research: Workshop and Conference Proceedings 10: The Fourth Workshop on Feature Selection in Data Mining},
keywords = {data mining,dimension reduction,feature extraction,feature selection},
pages = {4--13},
title = {{Feature Selection : An Ever Evolving Frontier in Data Mining}},
url = {http://proceedings.mlr.press/v10/liu10b/liu10b.pdf},
volume = {10},
year = {2010}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Hall1999,
author = {Hall, Mark A},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall - 1999 - Correlation-based Feature Selection for Machine Learning.pdf:pdf},
title = {{Correlation-based Feature Selection for Machine Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.4521{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@article{Sheather1991,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
author = {Sheather, S J and Jones, M C and Jonest, M C},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sheather, Jones, Jonest - 1991 - A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation.pdf:pdf},
journal = {Source Journal of the Royal Statistical Society. Series B (Methodological) Journal of the Royal Statistical Society. Series B J. R. Statist. Soc. B},
keywords = {ADAPTIVE CHOICE,BIAS REDUCTION,FUNCTIONAL ESTIMATION,SMOOTHING,SQUARED ERROR LOSS FUNCTIONS},
number = {3},
pages = {683--690},
title = {{A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation}},
url = {http://www.jstor.org/stable/2345597 http://about.jstor.org/terms},
volume = {53},
year = {1991}
}
@article{Guyon2003a,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre-dictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}} and De, Andre@tuebingen Mpg},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {QSAR,Variable selection,bioinformatics,clustering,computational biology,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-ery,proteomics,space dimensionality reduction,statistical testing,support vector machines,text classification,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Dy2004,
abstract = {In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.},
author = {Dy, Jennifer G and Brodley, Carla E},
doi = {10.1016/j.patrec.2014.11.006},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dy, Brodley - 2004 - Feature Selection for Unsupervised Learning.pdf:pdf},
isbn = {978-3-642-34487-9},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature Selection for Unsupervised Learning}},
url = {http://www.jmlr.org/papers/volume5/dy04a/dy04a.pdf},
volume = {5},
year = {2004}
}
@misc{Pimentel2014,
abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as "one-class classification", in which a model is constructed to describe "normal" training data. The novelty detection approach is typically used when the quantity of available "abnormal" data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that "normality" may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade. {\textcopyright} 2014 Published by Elsevier B.V.},
author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
booktitle = {Signal Processing},
doi = {10.1016/j.sigpro.2013.12.026},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pimentel et al. - 2014 - A review of novelty detection.pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
keywords = {Machine learning,Novelty detection,One-class classification},
month = {jun},
pages = {215--249},
publisher = {Elsevier},
title = {{A review of novelty detection}},
url = {https://www.sciencedirect.com/science/article/pii/S016516841300515X?via{\%}3Dihub},
volume = {99},
year = {2014}
}
@article{Clifton2014,
abstract = {—Novelty detection, or one-class classification, is of particular use in the analysis of high-integrity systems, in which examples of failure are rare in comparison with the number of examples of stable behaviour, such that a conventional multi-class classification approach cannot be taken. Support Vector Machines (SVMs) are a popular means of performing novelty detection, and it is conventional practice to use a train-validate-test approach, often involving cross-validation, to train the one-class SVM, and then select appropriate values for its parameters. An alternative method, used with multi-class SVMs, is to calibrate the SVM output into conditional class probabilities. A probabilistic ap-proach offers many advantages over the conventional method, including the facility to select automatically a probabilistic novelty threshold. The contributions of this paper are (i) the development of a probabilistic calibration technique for one-class SVMs, such that on-line novelty detection may be performed in a probabilistic manner; and (ii) the demonstration of the advantages of the pro-posed method (in comparison to the conventional one-class SVM methodology) using case studies, in which one-class probabilistic SVMs are used to perform condition monitoring of a high-integrity industrial combustion plant, and in detecting deterioration in pa-tient physiological condition during patient vital-sign monitoring.},
annote = {Many relevant references?
Talking about constructing anomaly data, this is relevant for the project!

Might be able to use plant knowledge to analyze the artificial unstable data? 25,26},
author = {Clifton, Lei and Clifton, David A. and Zhang, Yang and Watkinson, Peter and Tarassenko, Lionel and Yin, Hujun},
doi = {10.1109/TR.2014.2315911},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clifton et al. - 2014 - Probabilistic Novelty Detection With Support Vector Machines.pdf:pdf},
isbn = {0018-9529 VO  - 63},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
keywords = {Support vector machine,calibration,condition monitoring,novelty detection,one-class classification},
number = {2},
pages = {455--467},
title = {{Probabilistic novelty detection with support vector machines}},
volume = {63},
year = {2014}
}
@article{Cheng,
abstract = {Anomaly detection in multivariate time series is an important data mining task with applications to ecosystem modeling, network traffic monitoring, medical diagnosis, and other domains. This paper presents a robust algorithm for detecting anomalies in noisy multivariate time series data by employing a kernel matrix alignment method to capture the dependence relationships among variables in the time series. Anomalies are found by performing a random walk traversal on the graph induced by the aligned kernel matrix. We show that the algorithm is flexible enough to handle different types of time series anomalies including subsequence-based and local anomalies. Our framework can also be used to characterize the anomalies found in a target time series in terms of the anomalies present in other time series. We have performed extensive experiments to empirically demonstrate the effectiveness of our algorithm. A case study is also presented to illustrate the ability of the algorithm to detect ecosystem disturbances in Earth science data. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972795.36},
author = {Cheng, Haibin and Tan, Pang-Ning and Potter, Christopher and Klooster, Steven},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2009 - Detection and Characterization of Anomalies in Multivariate Time Series.pdf:pdf},
journal = {SDM},
keywords = {anomaly detection,graph representation,kernel alignment,kernel function,multivariate time series},
pages = {413--424},
title = {{Detection and Characterization of Anomalies in Multivariate Time Series}},
url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.36},
volume = {9},
year = {2009}
}
@inproceedings{Kozma,
abstract = {The problem of detecting weak aiionialies in temporal signals is addressed. The performance of statistical methods utilizing the evaluation of the intensity of time-dependent fluctuations is compared with the results obtained by a layered artificial neural network model. The desired accuracy of tlie approximation by the neural network at the end of the learning phase has been estimated by analyzing the statistics of the learning data. The applicatioii of the obtained results to tlie analysis of actual anomaly data from a nuclear reactor showed that neural networks can ident,ify the onset of anomalies with a reasonable success, while usual statistical methods were unable to make distinction between iiorinal and abnormal patterns.},
author = {Kozma, R and Kitamura, M and Sakuma, M and Yokoyama, Y},
booktitle = {IEEE International Conference on Neural Networks},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kozma et al. - 1994 - Anomaly Detection By Neural Network Models and Statistical Time Series Analysis.pdf:pdf},
isbn = {078031901X},
pages = {3207--3210},
title = {{Anomaly Detection By Neural Network Models and Statistical Time Series Analysis}},
url = {http://www.memphis.edu/clion/pdf-publications/ijcnn94.pdf},
volume = {5},
year = {1994}
}
@article{Gardner2006,
abstract = {This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a " one-shot " manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1{\%} sensitivity, a mean detection latency of GARDNER, KRIEGER, VACHTSEVANOS AND LITT 1026 -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training.},
author = {Gardner, Andrew B and Krieger, Abba M and Vachtsevanos, George and Litt, Brian and {Gardner AGARDNER}, Andrew B},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2006 - One-Class Novelty Detection for Seizure Analysis from Intracranial EEG.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {epilepsy,novelty detection,one-class SVM,seizure detection,unsupervised learning},
pages = {1025--1044},
title = {{One-Class Novelty Detection for Seizure Analysis from Intracranial EEG}},
url = {http://www.jmlr.org/papers/volume7/gardner06a/gardner06a.pdf},
volume = {7},
year = {2006}
}
@article{Ma,
abstract = {Time-series novelty detection, or anomaly detection, refers to the automatic identification of novel or abnormal events embedded in normal time-series points. Although it is a challenging topic in data mining, it has been acquiring increasing attention due to its huge potential for immediate applications. In this paper, a new algorithm for time-series novelty detection based on one-class support vector machines (SVMs) is proposed. The concepts of phase and projected phase spaces are first introduced, which allows us to convert a time-series into a set of vectors in the (projected) phase spaces. Then we interpret novel events in time-series as outliers of the "normal" distribution of the converted vectors in the (projected) phase spaces. One-class SVMs are employed as the outlier detectors. In order to obtain robust detection results, a technique to combine intermediate results at different phase spaces is also proposed. Experiments on both synthetic and measured data are presented to demonstrate the promising performance of the new algorithm.},
annote = {Good shit keep this},
author = {Ma, J. and Perkins, S.},
doi = {10.1109/IJCNN.2003.1223670},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma, Perkins - 2003 - Time-series novelty detection using one-class support vector machines.pdf:pdf},
isbn = {0-7803-7898-9},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
pages = {1741--1745},
pmid = {7898689},
publisher = {IEEE},
title = {{Time-series novelty detection using one-class support vector machines}},
url = {http://ieeexplore.ieee.org/document/1223670/},
volume = {3},
year = {2003}
}
@article{Chan,
abstract = {Our goal is to generate comprehensible and accurate models from multiple time series for anomaly detection. The models need to produce anomaly scores in an online manner for real-life monitoring tasks. We introduce three algorithms that work in a constructed feature space and evaluate them with a real data set from the NASA shuttle program. Our offline and online evaluations indicate that our algorithms can be more accurate than two existing algorithms.},
author = {Chan, P K and Mahoney, M V},
doi = {10.1109/ICDM.2005.101},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Mahoney - 2005 - Modeling multiple time series for anomaly detection.pdf:pdf},
isbn = {0769522785},
issn = {15504786},
journal = {Fifth IEEE International Conference on Data Mining ICDM05},
pages = {90--97},
publisher = {IEEE},
title = {{Modeling multiple time series for anomaly detection}},
url = {http://ieeexplore.ieee.org/document/1565666/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565666},
year = {2005}
}
@article{,
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Time series forecasting based on wavelet filtering.pdf:pdf},
title = {{Time series forecasting based on wavelet filtering}},
url = {https://ac.els-cdn.com/S095741741500041X/1-s2.0-S095741741500041X-main.pdf?{\_}tid=429e3e38-0fe6-11e8-894a-00000aab0f01{\&}acdnat=1518434407{\_}b266f00e681a122b5635f4499e308ffc}
}
@article{Nanihar,
author = {Nanihar, Nadiarulah and Khalid, Amir and Mustaffa, Norrizal},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nanihar, Khalid, Mustaffa - Unknown - Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Rel.pdf:pdf},
journal = {Mater. Sci. Eng},
title = {{Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Related content Experiment on the Effects of Storage Duration of Biodiesel produced from Crude Palm Oil, Waste Cooking oil and Jatropha}},
url = {http://iopscience.iop.org/article/10.1088/1757-899X/226/1/012117/pdf},
volume = {226}
}
@book{Kim2017,
author = {Kim, Nam-Ho and Joo-Jo, Choi and An, Dawn},
doi = {10.1007/978-3-319-44742-1},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Joo-Jo, An - 2017 - Prognostics and Health Management of Electronics.pdf:pdf},
isbn = {9780470278024},
pages = {336},
title = {{Prognostics and Health Management of Electronics}},
year = {2017}
}
@article{Si2011,
abstract = {Remaining useful life (RUL) is the useful life left on an asset at a particular time of operation. Its estimation is central to condition based maintenance and prognostics and health management. RUL is typically random and unknown, and as such it must be estimated from available sources of information such as the information obtained in condition and health monitoring. The research on how to best estimate the RUL has gained popularity recently due to the rapid advances in condition and health monitoring techniques. However, due to its complicated relationship with observable health information, there is no such best approach which can be used universally to achieve the best estimate. As such this paper reviews the recent modeling developments for estimating the RUL. The review is centred on statistical data driven approaches which rely only on available past observed data and statistical models. The approaches are classified into two broad types of models, that is, models that rely on directly observed state information of the asset, and those do not. We systematically review the models and approaches reported in the literature and finally highlight future research challenges.},
author = {Si, Xiao-Sheng and Wang, Wenbin and Hu, Chang-Hua and Zhou, Dong-Hua},
doi = {10.1016/J.EJOR.2010.11.018},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Si et al. - 2011 - Remaining useful life estimation – A review on the statistical data driven approaches.pdf:pdf},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {aug},
number = {1},
pages = {1--14},
publisher = {North-Holland},
title = {{Remaining useful life estimation – A review on the statistical data driven approaches}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221710007903},
volume = {213},
year = {2011}
}
@book{Hastie,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {{Hastie, Trevor Tibshirani, Robert Friedman}, Jerome},
booktitle = {Springer},
doi = {10.1198/jasa.2004.s339},
eprint = {1010.3003},
file = {:C$\backslash$:/Users/asgeiroa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Trevor Tibshirani, Robert Friedman - 2008 - The Elements of Statistical Learning Data Mining, Inference, and Prediction.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {0162-1459},
keywords = {inger series in statistics},
pmid = {21196786},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2004.s339},
volume = {2},
year = {2008}
}
