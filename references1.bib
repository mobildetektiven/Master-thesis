@inproceedings{Juba2015,
abstract = {—Anomaly detection plays an important role in pro-tecting computer systems from unforeseen attack by automati-cally recognizing and filter atypical inputs. However, it can be difficult to balance the sensitivity of a detector – an aggressive system can filter too many benign inputs while a conservative system can fail to catch anomalies. Accordingly, it is important to rigorously test anomaly detectors to evaluate potential error rates before deployment. However, principled systems for doing so have not been studied – testing is typically ad hoc, making it difficult to reproduce results or formally compare detectors. To address this issue we present a technique and implemented system, Fortuna, for obtaining probabilistic bounds on false positive rates for anomaly detectors that process Internet data. Using a probability distribution based on PageRank and an efficient algorithm to draw samples from the distribution, Fortuna computes an estimated false positive rate and a probabilistic bound on the estimate's accuracy. By drawing test samples from a well defined distribution that correlates well with data seen in practice, Fortuna improves on ad hoc methods for estimating false positive rate, giving bounds that are reproducible, comparable across different anomaly detectors, and theoretically sound. Experimental evaluations of three anomaly detectors (SIFT, SOAP, and JSAND) show that Fortuna is efficient enough to use in practice — it can sample enough inputs to obtain tight false positive rate bounds in less than 10 hours for all three detectors. These results indicate that Fortuna can, in practice, help place anomaly detection on a stronger theoretical foundation and help practitioners better understand the behavior and consequences of the anomaly detectors that they deploy. As part of our work, we obtain a theoretical result that may be of independent interest: We give a simple analysis of the convergence rate of the random surfer process defining PageRank that guarantees the same rate as the standard, second-eigenvalue analysis, but does not rely on any assumptions about the link structure of the web.},
archivePrefix = {arXiv},
arxivId = {1612.06676},
author = {Juba, Brendan and Musco, Christopher and Long, Fan and Sidiroglou-Douskos, Stelios and Rinard, Martin},
booktitle = {Proceedings 2015 Network and Distributed System Security Symposium},
doi = {10.14722/ndss.2015.23268},
eprint = {1612.06676},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra et al. - Unknown - Long Short Term Memory Networks for Anomaly Detection in Time Series(3).pdf:pdf},
isbn = {1-891562-38-X},
issn = {16130073},
title = {{Principled Sampling for Anomaly Detection}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf http://www.internetsociety.org/doc/principled-sampling-anomaly-detection},
year = {2015}
}
@article{Malhotra2016,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
archivePrefix = {arXiv},
arxivId = {1607.00148},
author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
eprint = {1607.00148},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra, Com - Unknown - LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection(2).pdf:pdf},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {https://arxiv.org/pdf/1607.00148.pdf http://arxiv.org/abs/1607.00148},
year = {2016}
}
@article{Hautam,
author = {Hautam, Ville and Ismo, K},
file = {:home/asgeir/Downloads/01334558.pdf:pdf},
pages = {4--7},
title = {{Outlier Detection Using k-Nearest Neighbour Graph ¥ Q P ¥ ¥ R {\$} S ¨ UTWVYX ' EDGFIH ¢¥ P {\$}}}
}
@article{Tarassenko2009,
author = {Tarassenko, Lionel and Clifton, David A and Bannister, Peter R and King, Steve and King, Dennis},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tarassenko et al. - Unknown - Chapter 35 Novelty Detection.pdf:pdf},
isbn = {9780470058220},
journal = {Encyclopaedia of Structural Health Monitoring},
pages = {653--675},
title = {{Novelty Detection}},
url = {https://pdfs.semanticscholar.org/a65d/7c7137968e291f0732b12110e485276cbe11.pdf},
year = {2009}
}
@inproceedings{Hautamaki2004,
abstract = {We present an outlier detection using indegree number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance-based method are also proposed. We compare the methods with real and synthetic datasets. The results show that the proposed method achieves reasonable results with synthetic data and outperforms compared methods with real data sets with small number of observations.},
author = {Hautam{\"{a}}ki, Ville and K{\"{a}}rkk{\"{a}}inen, Ismo and Fr{\"{a}}nti, Pasi},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1334558},
isbn = {0769521282},
issn = {10514651},
pages = {430--433},
publisher = {IEEE},
title = {{Outlier detection using k-nearest neighbour graph}},
url = {http://ieeexplore.ieee.org/document/1334558/},
volume = {3},
year = {2004}
}
@article{Hodge,
abstract = {Outlier detection has been used for centuries to detect and, where appropri-ate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
author = {Hodge, Victoria J and Austin, Jim},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodge, Austin - Unknown - A Survey of Outlier Detection Methodologies.pdf:pdf},
keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
title = {{A Survey of Outlier Detection Methodologies}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AAIRE.0000045502.10941.a9.pdf}
}
@article{Mckinney2010,
abstract = {—In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {Mckinney, Wes},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mckinney - 2010 - Data Structures for Statistical Computing in Python.pdf:pdf},
journal = {PROC. OF THE 9th PYTHON IN SCIENCE CONF},
keywords = {Index Terms—data structure,R,statistics},
title = {{Data Structures for Statistical Computing in Python}},
url = {https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf},
year = {2010}
}
@misc{Statkraft2009,
abstract = {Vannkraft er en milj{\o}vennlig og fornybar energikilde. 99 prosent av all kraftproduksjon i Norge kommer fra vannkraft. P{\aa} verdensbasis utgj{\o}r vannkraften rundt en sjettedel av den totale kraftproduksjonen},
author = {Statkraft},
title = {{Vannkraft}},
url = {https://www.statkraft.no/globalassets/old-contains-the-old-folder-structure/documents/no/vannkraft-09-no{\_}tcm10-4585.pdf},
urldate = {2018-03-20},
year = {2009}
}
@article{Paish2002,
abstract = {Hydropower, large and small, remains by far the most important of the «renewables» for electrical power production worldwide, providing 19{\%} of the planet's electricity. Small-scale hydro is in most cases «run-of-river», with no dam or water storage, and is one of the most cost-effective and environmentally benign energy technologies to be considered both for rural electrification in less developed countries and further hydro developments in Europe. The European Commission have a target to increase small hydro capacity by 4500MW (50{\%}) by the year 2010. The UK has 100MW of existing small hydro capacity (under 5MW) operating from approximately 120 sites, and at least 400MW of unexploited potential. With positive environmental policies now being backed by favourable tariffs for 'green' electricity, the industry believes that small hydro will have a strong resurgence in Europe in the next 10 years, after 20 years of decline. This paper summarises the different small hydro technologies, new innovations being developed, and the barriers to further development. {\textcopyright} 2002 Elsevier Science Ltd.},
author = {Paish, Oliver},
doi = {10.1016/S1364-0321(02)00006-0},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paish - 2002 - Small hydro power technology and current status.pdf:pdf},
isbn = {1364-0321},
issn = {13640321},
journal = {Renewable and Sustainable Energy Reviews},
keywords = {Hydropower,Micro-hydro,Mini-hydro,Small hydro,Water power},
number = {6},
pages = {537--556},
title = {{Small hydro power: Technology and current status}},
url = {www.elsevier.com/locate/rser},
volume = {6},
year = {2002}
}
@article{Schslkopf,
abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
author = {Schslkopf, Bernhard and Smola, Alexander and Mfiller, Klaus-Robert},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schslkopf, Smola, Mfiller - Unknown - Kernel Principal Component Analysis.pdf:pdf},
title = {{Kernel Principal Component Analysis}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBFb0020217.pdf}
}
@article{Belkin2001,
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1.1.19.9400},
eprint = {arXiv:1011.1669v3},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belkin, Niyogi - Unknown - Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering(2).pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Nips},
pages = {585--591},
pmid = {25246403},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering}},
url = {http://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.9400{\&}rep=rep1{\&}type=pdf},
volume = {14},
year = {2001}
}
@article{Li,
abstract = {This paper studies one application of mutual information to symbolic sequen-ces: the mutual information function M(d). This function is compared with the more frequently used correlation function F(d). An exact relation between M(d) and F(d) is derived for binary sequences. For sequences with more than two symbols, no such general relation exists; in particular, F(d) = 0 may or may not lead to M(d)= 0. This linear, but not general, independence between symbols separated by a distance is studied for ternary sequences. Also included is the estimation of the finite-size effect on calculating mutual information. Finally, the concept of "symbolic noise" is discussed.},
author = {Li, Wentian},
doi = {10.1007/BF01025996},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - Unknown - Mutual Information Functions versus Correlation Functions.pdf:pdf},
issn = {0022-4715},
journal = {J. Stat. Phys.},
keywords = {correlation functions,linear and general dependence,mutual information function,symbolic noise},
number = {5},
pages = {823--837},
title = {{Mutual information versus correlation functions}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF01025996.pdf papers2://publication/uuid/805C6936-1BA8-4632-B6C6-5BB2CDB979C3},
volume = {60},
year = {1990}
}
@inproceedings{Khalid2014,
author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
booktitle = {2014 Science and Information Conference},
doi = {10.1109/SAI.2014.6918213},
file = {:home/asgeir/Downloads/06918213.pdf:pdf},
isbn = {978-0-9893193-1-7},
month = {aug},
pages = {372--378},
publisher = {IEEE},
title = {{A survey of feature selection and feature extraction techniques in machine learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6918213},
year = {2014}
}
@article{Chandrashekar2014,
abstract = {a b s t r a c t Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction perfor-mance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in lit-erature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrashekar, Sahin - 2014 - A survey on feature selection methods.pdf:pdf},
journal = {Computers and Electrical Engineering},
pages = {16--28},
title = {{A survey on feature selection methods}},
url = {https://ac.els-cdn.com/S0045790613003066/1-s2.0-S0045790613003066-main.pdf?{\_}tid=c336f5ca-2494-422d-9db8-22b90d47458a{\&}acdnat=1520844383{\_}5faa66e2252a7c49c3d0c76ad66df5de},
volume = {40},
year = {2014}
}
@article{Kraskov2004,
abstract = {We present two classes of improved estimators for mutual information {\$}M(X,Y){\$}, from samples of random points distributed according to some joint probability density {\$}\backslashmu(x,y){\$}. In contrast to conventional estimators based on binnings, they are based on entropy estimates from {\$}k{\$}-nearest neighbour distances. This means that they are data efficient (with {\$}k=1{\$} we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of {\$}k/N{\$} for {\$}N{\$} points. Numerically, we find that both families become {\{}$\backslash$it exact{\}} for independent distributions, i.e. the estimator {\$}\backslashhat M(X,Y){\$} vanishes (up to statistical fluctuations) if {\$}\backslashmu(x,y) = \backslashmu(x) \backslashmu(y){\$}. This holds for all tested marginal distributions and for all dimensions of {\$}x{\$} and {\$}y{\$}. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
annote = {Scikit-learn reference!},
archivePrefix = {arXiv},
arxivId = {cond-mat/0305641},
author = {Kraskov, Alexander and St{\"{o}}gbauer, Harald and Grassberger, Peter},
doi = {10.1103/PhysRevE.69.066138},
eprint = {0305641},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraskov, St{\"{o}}gbauer, Grassberger - Unknown - Estimating mutual information.pdf:pdf},
isbn = {1539-3755 (Print)},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {6},
pages = {16},
pmid = {15244698},
primaryClass = {cond-mat},
title = {{Estimating mutual information}},
url = {https://journals.aps.org/pre/pdf/10.1103/PhysRevE.69.066138},
volume = {69},
year = {2004}
}
@article{Peng2005,
abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
archivePrefix = {arXiv},
arxivId = {f},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
eprint = {f},
file = {:home/asgeir/Downloads/01453511.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy}},
volume = {27},
year = {2005}
}
@misc{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
isbn = {0893-6080},
issn = {18792782},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Ding2014,
abstract = {Novelty detection is especially important for monitoring safety-critical systems in which novel conditions rarely occur and knowledge about novelty in that system is often limited or unavailable. There are a large number of studies in the area of novelty detection, but there is a lack of a comprehensive experimental evaluation of existing novelty detection methods. This paper aims to fill this void by conducting experimental evaluation of representative novelty detection methods. It presents a state-of-the-art review of novelty detection, with a focus on methods reported in the last few years. In addition, a rigorous comparative evaluation of four widely used methods, representative of different categories of novelty detectors, is carried out using 10 benchmark datasets with different scale, dimensionality and problem complexity. The experimental results demonstrate that the k-NN novelty detection method exhibits competitive overall performance to the other methods in terms of the AUC metric. {\textcopyright} 2013 Elsevier B.V.},
author = {Ding, Xuemei and Li, Yuhua and Belatreche, Ammar and Maguire, Liam P.},
doi = {10.1016/j.neucom.2013.12.002},
issn = {18728286},
journal = {Neurocomputing},
title = {{An experimental evaluation of novelty detection methods}},
volume = {135},
year = {2014}
}
@article{Omar2013,
abstract = {Intrusion detection has gain a broad attention and become a fertile field for several researches, and still being the subject of widespread interest by researchers. The intrusion detection community still confronts difficult problems even after many years of research. Reducing the large number of false alerts during the process of detecting unknown attack patterns remains unresolved problem. However, several research results recently have shown that there are potential solutions to this problem. Anomaly detection is a key issue of intrusion detection in which perturbations of normal behavior indicates a presence of intended or unintended induced attacks, faults, defects and others. This paper presents an overview of research directions for applying supervised and unsupervised methods for managing the problem of anomaly detection. The references cited will cover the major theoretical issues, guiding the researcher in interesting research directions.},
annote = {Oppsummering av mange ulike teknikar for anomaly detection, kan brukast til sitering!},
author = {Omar, Salima and Ngadi, Asri and Jebur, Hamid H},
doi = {10.5120/13715-1478},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Omar, Ngadi, Jebur - 2013 - Machine Learning Techniques for Anomaly Detection An Overview.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {Network Intrusion Detection,Supervised Machine Learning,Unsupervised Machine Learning},
number = {2},
pages = {975--8887},
title = {{Machine Learning Techniques for Anomaly Detection: An Overview}},
url = {https://pdfs.semanticscholar.org/0278/bbaf1db5df036f02393679d485260b1daeb7.pdf},
volume = {79},
year = {2013}
}
@article{He2005,
abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning sce- narios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of pre- vious unsupervised feature selection methods are wrapper techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a ﬁlter method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classiﬁcation problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demon- strate the effectiveness and efﬁciency of our algorithm.},
author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
doi = {http://books.nips.cc/papers/files/nips18/NIPS2005_0149.pdf},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Cai, Niyogi - Unknown - Laplacian Score for Feature Selection.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {507--514},
title = {{Laplacian Score for Feature Selection}},
url = {http://papers.nips.cc/paper/2909-laplacian-score-for-feature-selection.pdf},
year = {2005}
}
@article{Liu2010,
abstract = {The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, commu- nications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining appli- cations. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We first briefly introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vi- brant research field of some contemporary interests, new applications, and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.},
author = {Liu, Huan (National University of Singapore) and Motoda, Hiroshi (Osaka University) and Setiono, Rudy and Zhao, Zheng},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - Feature Selection An Ever Evolving Frontier in Data Mining.pdf:pdf},
issn = {1541-1672},
journal = {Journal of Machine Learning Research: Workshop and Conference Proceedings 10: The Fourth Workshop on Feature Selection in Data Mining},
keywords = {data mining,dimension reduction,feature extraction,feature selection},
pages = {4--13},
title = {{Feature Selection : An Ever Evolving Frontier in Data Mining}},
url = {http://proceedings.mlr.press/v10/liu10b/liu10b.pdf},
volume = {10},
year = {2010}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Hall1999,
author = {Hall, Mark A},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall - 1999 - Correlation-based Feature Selection for Machine Learning.pdf:pdf},
title = {{Correlation-based Feature Selection for Machine Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.4521{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@article{Sheather1991,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
author = {Sheather, S J and Jones, M C and Jonest, M C},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sheather, Jones, Jonest - 1991 - A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation.pdf:pdf},
journal = {Source Journal of the Royal Statistical Society. Series B (Methodological) Journal of the Royal Statistical Society. Series B J. R. Statist. Soc. B},
keywords = {ADAPTIVE CHOICE,BIAS REDUCTION,FUNCTIONAL ESTIMATION,SMOOTHING,SQUARED ERROR LOSS FUNCTIONS},
number = {3},
pages = {683--690},
title = {{A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation}},
url = {http://www.jstor.org/stable/2345597 http://about.jstor.org/terms},
volume = {53},
year = {1991}
}
@article{Guyon2003a,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre-dictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}} and De, Andre@tuebingen Mpg},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff, De - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {QSAR,Variable selection,bioinformatics,clustering,computational biology,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-ery,proteomics,space dimensionality reduction,statistical testing,support vector machines,text classification,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
url = {http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf},
volume = {3},
year = {2003}
}
@article{Dy2004,
abstract = {In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.},
author = {Dy, Jennifer G and Brodley, Carla E},
doi = {10.1016/j.patrec.2014.11.006},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dy, Brodley BRODLEY - 2004 - Feature Selection for Unsupervised Learning.pdf:pdf},
isbn = {978-3-642-34487-9},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature Selection for Unsupervised Learning}},
url = {http://www.jmlr.org/papers/volume5/dy04a/dy04a.pdf},
volume = {5},
year = {2004}
}
@misc{Pimentel2014,
abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as "one-class classification", in which a model is constructed to describe "normal" training data. The novelty detection approach is typically used when the quantity of available "abnormal" data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that "normality" may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade. {\textcopyright} 2014 Published by Elsevier B.V.},
author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
booktitle = {Signal Processing},
doi = {10.1016/j.sigpro.2013.12.026},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pimentel et al. - 2014 - A review of novelty detection(2).pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
keywords = {Machine learning,Novelty detection,One-class classification},
month = {jun},
pages = {215--249},
publisher = {Elsevier},
title = {{A review of novelty detection}},
url = {https://www.sciencedirect.com/science/article/pii/S016516841300515X?via{\%}3Dihub},
volume = {99},
year = {2014}
}
@article{Clifton2014,
abstract = {—Novelty detection, or one-class classification, is of particular use in the analysis of high-integrity systems, in which examples of failure are rare in comparison with the number of examples of stable behaviour, such that a conventional multi-class classification approach cannot be taken. Support Vector Machines (SVMs) are a popular means of performing novelty detection, and it is conventional practice to use a train-validate-test approach, often involving cross-validation, to train the one-class SVM, and then select appropriate values for its parameters. An alternative method, used with multi-class SVMs, is to calibrate the SVM output into conditional class probabilities. A probabilistic ap-proach offers many advantages over the conventional method, including the facility to select automatically a probabilistic novelty threshold. The contributions of this paper are (i) the development of a probabilistic calibration technique for one-class SVMs, such that on-line novelty detection may be performed in a probabilistic manner; and (ii) the demonstration of the advantages of the pro-posed method (in comparison to the conventional one-class SVM methodology) using case studies, in which one-class probabilistic SVMs are used to perform condition monitoring of a high-integrity industrial combustion plant, and in detecting deterioration in pa-tient physiological condition during patient vital-sign monitoring.},
annote = {Many relevant references?
Talking about constructing anomaly data, this is relevant for the project!

Might be able to use plant knowledge to analyze the artificial unstable data? 25,26},
author = {Clifton, Lei and Clifton, David A. and Zhang, Yang and Watkinson, Peter and Tarassenko, Lionel and Yin, Hujun},
doi = {10.1109/TR.2014.2315911},
file = {:home/asgeir/Downloads/06786486.pdf:pdf},
isbn = {0018-9529 VO  - 63},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
keywords = {Support vector machine,calibration,condition monitoring,novelty detection,one-class classification},
number = {2},
pages = {455--467},
title = {{Probabilistic novelty detection with support vector machines}},
volume = {63},
year = {2014}
}
@article{Cheng,
abstract = {Anomaly detection in multivariate time series is an important data mining task with applications to ecosystem modeling, network traffic monitoring, medical diagnosis, and other domains. This paper presents a robust algorithm for detecting anomalies in noisy multivariate time series data by employing a kernel matrix alignment method to capture the dependence relationships among variables in the time series. Anomalies are found by performing a random walk traversal on the graph induced by the aligned kernel matrix. We show that the algorithm is flexible enough to handle different types of time series anomalies including subsequence-based and local anomalies. Our framework can also be used to characterize the anomalies found in a target time series in terms of the anomalies present in other time series. We have performed extensive experiments to empirically demonstrate the effectiveness of our algorithm. A case study is also presented to illustrate the ability of the algorithm to detect ecosystem disturbances in Earth science data. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972795.36},
author = {Cheng, Haibin and Tan, Pang-Ning and Potter, Christopher and Klooster, Steven},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - Unknown - Detection and Characterization of Anomalies in Multivariate Time Series.pdf:pdf},
journal = {SDM},
keywords = {anomaly detection,graph representation,kernel alignment,kernel function,multivariate time series},
pages = {413--424},
title = {{Detection and Characterization of Anomalies in Multivariate Time Series}},
url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972795.36},
volume = {9},
year = {2009}
}
@inproceedings{Kozma,
abstract = {The problem of detecting weak aiionialies in temporal signals is addressed. The performance of statistical methods utilizing the evaluation of the intensity of time-dependent fluctuations is compared with the results obtained by a layered artificial neural network model. The desired accuracy of tlie approximation by the neural network at the end of the learning phase has been estimated by analyzing the statistics of the learning data. The applicatioii of the obtained results to tlie analysis of actual anomaly data from a nuclear reactor showed that neural networks can ident,ify the onset of anomalies with a reasonable success, while usual statistical methods were unable to make distinction between iiorinal and abnormal patterns.},
author = {Kozma, R and Kitamura, M and Sakuma, M and Yokoyama, Y},
booktitle = {IEEE International Conference on Neural Networks},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kozma et al. - Unknown - ANOMALY DETECTION BY NEURAL NETWORK MODELS AND STATISTICAL TIME SERIES ANALYSIS.pdf:pdf},
isbn = {078031901X},
pages = {3207--3210},
title = {{Anomaly Detection By Neural Network Models and Statistical Time Series Analysis}},
url = {http://www.memphis.edu/clion/pdf-publications/ijcnn94.pdf},
volume = {5},
year = {1994}
}
@article{Gardner2006,
abstract = {This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a " one-shot " manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1{\%} sensitivity, a mean detection latency of GARDNER, KRIEGER, VACHTSEVANOS AND LITT 1026 -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training.},
author = {Gardner, Andrew B and Krieger, Abba M and Vachtsevanos, George and Litt, Brian and {Gardner AGARDNER}, Andrew B},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2006 - One-Class Novelty Detection for Seizure Analysis from Intracranial EEG.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {epilepsy,novelty detection,one-class SVM,seizure detection,unsupervised learning},
pages = {1025--1044},
title = {{One-Class Novelty Detection for Seizure Analysis from Intracranial EEG}},
url = {http://www.jmlr.org/papers/volume7/gardner06a/gardner06a.pdf},
volume = {7},
year = {2006}
}
@article{Ma,
abstract = {Time-series novelty detection, or anomaly detection, refers to the automatic identification of novel or abnormal events embedded in normal time-series points. Although it is a challenging topic in data mining, it has been acquiring increasing attention due to its huge potential for immediate applications. In this paper, a new algorithm for time-series novelty detection based on one-class support vector machines (SVMs) is proposed. The concepts of phase and projected phase spaces are first introduced, which allows us to convert a time-series into a set of vectors in the (projected) phase spaces. Then we interpret novel events in time-series as outliers of the "normal" distribution of the converted vectors in the (projected) phase spaces. One-class SVMs are employed as the outlier detectors. In order to obtain robust detection results, a technique to combine intermediate results at different phase spaces is also proposed. Experiments on both synthetic and measured data are presented to demonstrate the promising performance of the new algorithm.},
annote = {Good shit keep this},
author = {Ma, J. and Perkins, S.},
doi = {10.1109/IJCNN.2003.1223670},
file = {:home/asgeir/Downloads/timeseries novelty detection using SVM.pdf:pdf},
isbn = {0-7803-7898-9},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
pages = {1741--1745},
pmid = {7898689},
publisher = {IEEE},
title = {{Time-series novelty detection using one-class support vector machines}},
url = {http://ieeexplore.ieee.org/document/1223670/},
volume = {3},
year = {2003}
}
@article{Chan,
abstract = {Our goal is to generate comprehensible and accurate models from multiple time series for anomaly detection. The models need to produce anomaly scores in an online manner for real-life monitoring tasks. We introduce three algorithms that work in a constructed feature space and evaluate them with a real data set from the NASA shuttle program. Our offline and online evaluations indicate that our algorithms can be more accurate than two existing algorithms.},
author = {Chan, P K and Mahoney, M V},
doi = {10.1109/ICDM.2005.101},
file = {:home/asgeir/Downloads/01565666.pdf:pdf},
isbn = {0769522785},
issn = {15504786},
journal = {Fifth IEEE International Conference on Data Mining ICDM05},
pages = {90--97},
publisher = {IEEE},
title = {{Modeling multiple time series for anomaly detection}},
url = {http://ieeexplore.ieee.org/document/1565666/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565666},
year = {2005}
}
@article{,
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Time series forecasting based on wavelet filtering.pdf:pdf},
title = {{Time series forecasting based on wavelet filtering}},
url = {https://ac.els-cdn.com/S095741741500041X/1-s2.0-S095741741500041X-main.pdf?{\_}tid=429e3e38-0fe6-11e8-894a-00000aab0f01{\&}acdnat=1518434407{\_}b266f00e681a122b5635f4499e308ffc}
}
@article{Nanihar,
author = {Nanihar, Nadiarulah and Khalid, Amir and Mustaffa, Norrizal},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nanihar, Khalid, Mustaffa - Unknown - Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Rel.pdf:pdf},
journal = {Mater. Sci. Eng},
title = {{Multivariate Time Series Forecasting of Crude Palm Oil Price Using Machine Learning Techniques Related content Experiment on the Effects of Storage Duration of Biodiesel produced from Crude Palm Oil, Waste Cooking oil and Jatropha}},
url = {http://iopscience.iop.org/article/10.1088/1757-899X/226/1/012117/pdf},
volume = {226}
}
@book{Kim2017,
author = {Kim, Nam-Ho and Joo-Jo, Choi and An, Dawn},
doi = {10.1007/978-3-319-44742-1},
file = {:home/asgeir/Downloads/PHM.pdf:pdf},
isbn = {9780470278024},
pages = {336},
title = {{Prognostics and Health Management of Electronics}},
year = {2017}
}
@article{Si2011,
abstract = {Remaining useful life (RUL) is the useful life left on an asset at a particular time of operation. Its estimation is central to condition based maintenance and prognostics and health management. RUL is typically random and unknown, and as such it must be estimated from available sources of information such as the information obtained in condition and health monitoring. The research on how to best estimate the RUL has gained popularity recently due to the rapid advances in condition and health monitoring techniques. However, due to its complicated relationship with observable health information, there is no such best approach which can be used universally to achieve the best estimate. As such this paper reviews the recent modeling developments for estimating the RUL. The review is centred on statistical data driven approaches which rely only on available past observed data and statistical models. The approaches are classified into two broad types of models, that is, models that rely on directly observed state information of the asset, and those do not. We systematically review the models and approaches reported in the literature and finally highlight future research challenges.},
author = {Si, Xiao-Sheng and Wang, Wenbin and Hu, Chang-Hua and Zhou, Dong-Hua},
doi = {10.1016/J.EJOR.2010.11.018},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Si et al. - 2011 - Remaining useful life estimation – A review on the statistical data driven approaches.pdf:pdf},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {aug},
number = {1},
pages = {1--14},
publisher = {North-Holland},
title = {{Remaining useful life estimation – A review on the statistical data driven approaches}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221710007903},
volume = {213},
year = {2011}
}
